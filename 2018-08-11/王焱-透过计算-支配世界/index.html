<!DOCTYPE html>
<html>
  <head>
  <title>王焱：透过计算，支配世界 – 觀點 – 從草根到大師 git.io/JJCxS</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="
  
              

        
 
    本文作者王焱  
     
     AI  的讨论网上很多，以前也看过一些，并没有特别关注过，这次开会前看了看资料，我觉得不管人工智能如何发展，至少到目前，经典的社会理论框架里还能容纳这些问题。  
     
     1917  年马克斯•韦伯在慕尼黑大学有一个演讲《学术作为一种志业》，当时是德国在参加一次大战，国内又有人鼓动革命，社会动荡，乌云笼罩，大学生期望他能够指导一下，面对乱世应当采取什么样的政治立场，结果韦伯也没回答学生的问题，他讲的是在当下学术怎么样能成为一种志业。他在讲演里边已经概括了他眼中的现代社会，第一就是，我们的时代是个“透过计算，支配世界”的时代，这意味着这是个工具理性特别发达的时代，而其他的行动类型正在逐渐萎缩。  
     
    人工智能说到底也就是一个“算法的革命”。无论哪种人工智能，现在计算上没有超出以往设想的那种能力，特别像超级计算机。据最新公布的数字，美国名为“  Summit  ”的超级计算机的浮点运算速度可达每秒  20  亿亿次。听说中国的超级计算机，经常是在那儿闲置的，因为这个计算机造出来了，但是提不出计算的方案来，什么问题是值得你进行超级计算的？每秒  20  亿亿次。我们的创新能力差，尽管超级计算的能力有了，但是你设计不出要计算的东西来，这是中国的一个特别情况。但是按韦伯的说法，透过计算支配世界的理性是一种工具理性，它在手段上、功能效率上突飞猛进，但是价值与目的方面的理性考虑越来越萎缩，这就出现了一种极大的不平衡。  
     
    
    何光沪：
    还有一个题目可以计算，  14  亿人怎么想。  
     
    
    王焱：
    光沪这个问题恐怕不是计算机能够计算出来的。工具理性的发达，其他的行动类型逐渐萎缩，按韦伯说的，社会行动的理想类型，有传统型的、情感型的、价值合理型的、工具合理型的。人工智能不管怎么发达，比如说  AlphaGo  给它输进既定的围棋游戏规则以后，你可以计算在这样的规则下，怎么样能通过算法制胜。电脑发明以前，都是一代人一代人自己发明自己的棋局，人寿几何？速度很慢。现在用电脑可能每秒钟几亿亿次，可能把以往你穷极人类的想象也做不出的那些棋局发明出来，所以棋手柯洁后来说，现在人得向电脑学习。他下棋的对手是电脑，电脑的棋局我们都想不到，甚至理解不了。但是围棋的游戏规则还是人给机器输入的，怎么叫赢，怎么叫输，电脑是在这个既定规则下演算。所以人工智能的突飞猛进，还是在工具理性的框架内，在既定规则既定的目的之下的发展。相比之下，价值理性就不行了，这样高的运算速度，目的何在？在社会政治领域内，韦伯嘲笑过那些只讲价值正确、目的正确的人，最终只是“没有结果的亢奋”。但是只讲工具效率，不去思考目的的类型蔓延世界，也许结果更可怕。  
     
    韦伯这一派的社会理论，严格区分自然科学的方法和人文社会科学的方法是不一样的，为什么呢？因为自然科学的对象并没有反思能力，也没有价值观。你看一颗恒星在那儿，不会说自己的价值观变了，它就挪了一个位置。但是人文社会研究就不同了，它们研究的对象是人，人有价值观，有反思能力，他不但要在手段的意义上，追求功能与效率，还要价值的实现，还要追求目的和意义。所以自然科学的方法不适用于人文社会科学。人工智能也一样，我们把它看作一种拟人化的智能，但是有些东西它自己发明不出来。还要靠人类为它输入。下棋的规则是人制定的，不是机器制定的。机器人能做什么，不能做什么，也是由人指定的。即使人工智能自己有创新能力，那也是在既定的规则范围之内的创新。人工智能的发展，可以说是古代兵家思维的畸形发展，在给定的目的之下取胜就行。不可能自己发明一个规则或确定一个目的，自己能发明一个规则或目的，你就不是人工智能了，就是人类了。  
     
    韦伯演讲正好已经过去了  100  年，人类社会的百年，他的预测并没有过时，这首先就是透过计算支配世界的能力突飞猛进，所以他定义我们的时代是一个工具理性化的时代。人类的价值理性却没有什么进展。但是人类的奇特之处就在于有各式各样的价值观，各式各样的情感，人类的行动必然依据一定的价值观进行。现在的无人驾驶汽车就遇到这种伦理难题，你在公路上行驶，左边有一个小孩，右边有一辆车、里面有一车人，你要避让就出危险，你是选救这个孩子呢、还是选择救那一车人。你要是像功利主义者那样思考，可能就会牺牲那个孩子吧，另外一边有一车人呢。但是无论人怎么选择，还是有一套想法的，有一种理据。但是你把这个交给无人驾驶汽车，它就没法选择了，所以我估计无人驾驶汽车也遇到瓶颈了，要是人还好办，要是车给它设定牺牲那孩子，就难办了。包括无人机之类的，像阿富汗发生的这种情况，你设定的程序唯一目的就是取胜，不是人道主义，不是雨果说的，要用一个孩子的无辜牺牲，哪怕给我至尊的帝王之位我也不换，但是无人机不可能这么想，它就是取胜，没有什么人道主义这类的考虑。  
     
    人类社会拥有各式各样的价值观，现代人类社会已经进化到那种程度了，自从洛克的《宗教宽容》一书以来，我们一般人对于各式各样的价值观大都能够兼容，认为各种价值观都有其合理的方面  ,  但是我们每个人还是有自己的价值观，对于人工智能来说，价值观你怎么给它输入呢？这就是一个难题了。  
     
    现实中的人我们知道，某人被人定义为“脑残”，但是我们不会因为说他脑残，就要消灭他，他的脑残价值观也许也有他合理的地方，在这种意义上，我们也可将人工智能定义为“脑残”，因为他只有工具理性而没有价值理性，没有情感。  80  年代读韦伯的时候，觉得他太悲观了，工具理性的发达，也有若干正面意义，至少它导致了人的福利的提高。但是工具理性的片面发展，造成的结果是，在我们的前面，“不是鲜花灿烂的春天，而是严寒肃杀的冬夜”。在韦伯看来，这主要是由于工具理性的过度发达和价值理性相对的萎缩，加上官僚科层组织的发达，这两点使得人类的自由受到了越来越严重的桎梏。有一本美国学者米茨曼研究韦伯的书，题目就叫做《铁笼》，说人类未来发展的趋势，就是人类日益被自己打造的铁笼所囚禁，不管人工智能如何发展，在前面等待着我们的就是那个铁笼。  
     
    现在包括人工智能，无论是掌握在大资本家手里，还是专制者手里，确实给你打造好了这样的铁笼，只不过以往的帝王都梦想不到今天控制社会能够达到这种程度。这在美国好莱坞的很多电影里都有反映，一个独裁者背后有一个超级计算机的大球，可以即时监控全世界，你是谁、你正在哪，你在干什么。过去，秦始皇时代是腹诽心谤者要杀头，但是他并没有一个监控的手段知道你怎么就腹诽心谤了。现在就行了，能监控你的一举一动，你在想什么做什么，都能知道，都能监控。你在哪儿消费了，你上午手机支付，买了一包方便面，下午到天则发表了一通人工智能给我们带来的风险的演讲。现在的人确实面临着很多风险，包括大数据，掌握在什么人手里。一个信息产业的老板能看见每个人在做什么，我们就看不到。这就是所谓的垄断信息的资本家，像中国这样的社会，大数据掌握在少数个人手里，确实存在一定的大风险。  
     
    
    何光沪：
    现在的云服务。  
     
    
    王焱：
    但是人之为人，不是人类设计的电脑，对于民众而言，他可能今天这么想、明天那么想，你很难给他定义成一个僵死的不变的东西，固定的东西。莎士比亚写的《尤利乌斯•凯撒》，里边的民众今天说拥护杀死凯撒的人，因为他要独裁要破坏罗马的共和体制；明天又说杀死凯撒的人才是真正的阴谋家。此亦一是非，彼亦一是非。西方大学讲政治哲学经常拿这个当作教材，用来说明民众不可靠。但是实际上人可能就是这样的。  
     
    人工智能虽然发达，我觉得这个问题还没有解决，就是人工智能还没有自我意识，它虽有智力，但是仅仅是一种计算手段效率的智力，没有权衡目的价值的智力。如果以目的  -  手段来定向，对于正常人类来说，可以说  AI  还是个脑残。  
     
    人的价值观从哪儿来？能否输入给电脑，那种价值观应当优先？  
     
    有些电影里我们看到的，做这个机器人的工程师给机器人输入一条绝对律令：绝对不允许违抗制造你的主人，机器人还可以分析主人的音频、分析出你是不是真的主人说的话，你说的他不能违抗，绝对不能。否则他的智力超常，把你杀死了怎么办。绝对律令是造机器的人设定的。但是这个主人也不见得就是好人，可能是专制者、要当法西斯，发展到后面，很可能谁掌握的机器人多谁就是老大，就像以前说，一个奴隶主有多少奴隶，他有一千个奴隶，厉害！他有五百个，就差一些。他有五百个能够控制一个地区，有一千个机器人就能控制世界。  
     
    人工智能的价值观没有解决，现在还是设计它、制造它的人能给它输入一些规则和戒律，按现在这种发展，实际上还是工具理性的发展，机器人执行一个命令，命令是工具理性的，就是说你可在最短的时间内计算出最优的解决办法。但是机器人并没有能力选择，认为依据我的价值观实现这个目的是不道德的，所以我就恕难从命，不行动，至少现在的人工智能还判断不了。  
     
    在人类的控制之下的人工智能，应当说是眼耳鼻舌身的发达，这是人的器官的一个延长吧，行动能力效率的增加，但现在已经面临那种危险了，就是刚才说的，他可以利用他的这种工具化的延长、效率的提高，实现不道德的的目的。以人类现有的法律，第一你也跟不上，第二，有的国家根本不考虑这个，只要能维持这个权力干什么都行，人工智能干什么都行，用来实行监控，实现各种目的，你看什么书、上什么网、跟谁聊天，我都给你总结下来、记录下来。  
     
    
    何光沪：
    现在已经做到了，东德可以做到。  
     
    
    王焱：
    就是一个工具理性，失去了价值理性的制衡，实际上还是韦伯说的，我们需要的不是单纯的工具理性的发达，我们需要的是目的理性的权衡，你的理性能综合的均衡考虑你的目的是不是符合道德的、有价值的，然后再来考虑你的手段的效率优化问题，如果你不考虑目的，只考虑不断增加手段的功能效率，确实有很多危险。如果落到比如说心理阴暗的个人手里，确实会给人类带来危险。以前看福柯写的“人的消灭”，觉得有点危言耸听了，现在看人类从有文明到现在大概也就六千年，现在快到了自己消灭自己的时候了，包括人工智能或者生物医学工程。前两天刚看到德国一对夫妻做实验，把两个受精卵交给医院，医院给了他们一笔奖金，他们俩就玩儿去了，那个受精卵不断的发育，最后  6  个月那个医院就说行了，不需要  10  个月就可以出来了，然后在这个过程中，根据他们俩的审美要求，鼻子要高一点，最后就成了一男一女，就是这对夫妻的孩子。  
     
    
    何光沪：
    人工子宫。  
     
    
    王焱：
    对，生物医学工程也挺可怕，这种以前被认为是上帝的密码的东西，现在掌握在人的手里，你可以把基因剪辑来、剪辑去的，现在不但植物，人自身也要用这种基因工程，不知会造出什么怪物来，你说有生命、还是没生命，或者对他可以应用人类的道德、还是不可以应用，都是没研究过的新问题，生物医学工程可能比人工智能危险还更大一些。实际上，人类自己就要杀死自己了，各种离奇古怪的办法。实际上这是一个相当恐怖的时代，需要开放性的讨论，需要各式各样的办法，不见得是那种民主的办法，但是要有方法来防止这种后果吧。这些问题，没有答案。  
     
    我就是想到这些人工智能的价值观谁给他输入，我们是给机器人输入奴隶的道德呢，还是主人的道德呢？这是尼采的说法。如果是文革时期，我们会尽量输入什么三个坚持、四个无限的价值观，如果满街都是这种机器人那也挺恐怖的。这个价值观的输入谁来掌握？比例多少？你认为哪种价值观是最合理的？我觉得现在总的趋势还是在韦伯百年前预言的框架里，确实还没有更好的解决办法，这个是需要研究者要花费力气来研究的。  
     
    另外一个后果是，人工智能的突飞猛进，使得以前人类自身拥有的一些东西大大贬值了，人具有知识、情感、意志、美学观点，像莎士比亚当年讴歌的，人是多么伟大，多么高贵，不就是因为你有这些东西吗，人工智能如果能模仿这个，那就不知会走到哪里去了！无论如何，那些缺乏创造性和创意性思维的人，将会逐渐变成贬值。  
     
     
     [  
    王焱
    政治学、社会学家，天则经济研究所特约研究员；《公共论丛》主编；曾就职于中国社会科学院政治学研究所，原《读书》、《社会学家茶座》等著名学术期刊主编。本文为作者  2018-7-11  在「人工智能与道德风险」云豹沙龙的演讲，，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
     
  

" />
    <meta property="og:description" content="
  
              

        
 
    本文作者王焱  
     
     AI  的讨论网上很多，以前也看过一些，并没有特别关注过，这次开会前看了看资料，我觉得不管人工智能如何发展，至少到目前，经典的社会理论框架里还能容纳这些问题。  
     
     1917  年马克斯•韦伯在慕尼黑大学有一个演讲《学术作为一种志业》，当时是德国在参加一次大战，国内又有人鼓动革命，社会动荡，乌云笼罩，大学生期望他能够指导一下，面对乱世应当采取什么样的政治立场，结果韦伯也没回答学生的问题，他讲的是在当下学术怎么样能成为一种志业。他在讲演里边已经概括了他眼中的现代社会，第一就是，我们的时代是个“透过计算，支配世界”的时代，这意味着这是个工具理性特别发达的时代，而其他的行动类型正在逐渐萎缩。  
     
    人工智能说到底也就是一个“算法的革命”。无论哪种人工智能，现在计算上没有超出以往设想的那种能力，特别像超级计算机。据最新公布的数字，美国名为“  Summit  ”的超级计算机的浮点运算速度可达每秒  20  亿亿次。听说中国的超级计算机，经常是在那儿闲置的，因为这个计算机造出来了，但是提不出计算的方案来，什么问题是值得你进行超级计算的？每秒  20  亿亿次。我们的创新能力差，尽管超级计算的能力有了，但是你设计不出要计算的东西来，这是中国的一个特别情况。但是按韦伯的说法，透过计算支配世界的理性是一种工具理性，它在手段上、功能效率上突飞猛进，但是价值与目的方面的理性考虑越来越萎缩，这就出现了一种极大的不平衡。  
     
    
    何光沪：
    还有一个题目可以计算，  14  亿人怎么想。  
     
    
    王焱：
    光沪这个问题恐怕不是计算机能够计算出来的。工具理性的发达，其他的行动类型逐渐萎缩，按韦伯说的，社会行动的理想类型，有传统型的、情感型的、价值合理型的、工具合理型的。人工智能不管怎么发达，比如说  AlphaGo  给它输进既定的围棋游戏规则以后，你可以计算在这样的规则下，怎么样能通过算法制胜。电脑发明以前，都是一代人一代人自己发明自己的棋局，人寿几何？速度很慢。现在用电脑可能每秒钟几亿亿次，可能把以往你穷极人类的想象也做不出的那些棋局发明出来，所以棋手柯洁后来说，现在人得向电脑学习。他下棋的对手是电脑，电脑的棋局我们都想不到，甚至理解不了。但是围棋的游戏规则还是人给机器输入的，怎么叫赢，怎么叫输，电脑是在这个既定规则下演算。所以人工智能的突飞猛进，还是在工具理性的框架内，在既定规则既定的目的之下的发展。相比之下，价值理性就不行了，这样高的运算速度，目的何在？在社会政治领域内，韦伯嘲笑过那些只讲价值正确、目的正确的人，最终只是“没有结果的亢奋”。但是只讲工具效率，不去思考目的的类型蔓延世界，也许结果更可怕。  
     
    韦伯这一派的社会理论，严格区分自然科学的方法和人文社会科学的方法是不一样的，为什么呢？因为自然科学的对象并没有反思能力，也没有价值观。你看一颗恒星在那儿，不会说自己的价值观变了，它就挪了一个位置。但是人文社会研究就不同了，它们研究的对象是人，人有价值观，有反思能力，他不但要在手段的意义上，追求功能与效率，还要价值的实现，还要追求目的和意义。所以自然科学的方法不适用于人文社会科学。人工智能也一样，我们把它看作一种拟人化的智能，但是有些东西它自己发明不出来。还要靠人类为它输入。下棋的规则是人制定的，不是机器制定的。机器人能做什么，不能做什么，也是由人指定的。即使人工智能自己有创新能力，那也是在既定的规则范围之内的创新。人工智能的发展，可以说是古代兵家思维的畸形发展，在给定的目的之下取胜就行。不可能自己发明一个规则或确定一个目的，自己能发明一个规则或目的，你就不是人工智能了，就是人类了。  
     
    韦伯演讲正好已经过去了  100  年，人类社会的百年，他的预测并没有过时，这首先就是透过计算支配世界的能力突飞猛进，所以他定义我们的时代是一个工具理性化的时代。人类的价值理性却没有什么进展。但是人类的奇特之处就在于有各式各样的价值观，各式各样的情感，人类的行动必然依据一定的价值观进行。现在的无人驾驶汽车就遇到这种伦理难题，你在公路上行驶，左边有一个小孩，右边有一辆车、里面有一车人，你要避让就出危险，你是选救这个孩子呢、还是选择救那一车人。你要是像功利主义者那样思考，可能就会牺牲那个孩子吧，另外一边有一车人呢。但是无论人怎么选择，还是有一套想法的，有一种理据。但是你把这个交给无人驾驶汽车，它就没法选择了，所以我估计无人驾驶汽车也遇到瓶颈了，要是人还好办，要是车给它设定牺牲那孩子，就难办了。包括无人机之类的，像阿富汗发生的这种情况，你设定的程序唯一目的就是取胜，不是人道主义，不是雨果说的，要用一个孩子的无辜牺牲，哪怕给我至尊的帝王之位我也不换，但是无人机不可能这么想，它就是取胜，没有什么人道主义这类的考虑。  
     
    人类社会拥有各式各样的价值观，现代人类社会已经进化到那种程度了，自从洛克的《宗教宽容》一书以来，我们一般人对于各式各样的价值观大都能够兼容，认为各种价值观都有其合理的方面  ,  但是我们每个人还是有自己的价值观，对于人工智能来说，价值观你怎么给它输入呢？这就是一个难题了。  
     
    现实中的人我们知道，某人被人定义为“脑残”，但是我们不会因为说他脑残，就要消灭他，他的脑残价值观也许也有他合理的地方，在这种意义上，我们也可将人工智能定义为“脑残”，因为他只有工具理性而没有价值理性，没有情感。  80  年代读韦伯的时候，觉得他太悲观了，工具理性的发达，也有若干正面意义，至少它导致了人的福利的提高。但是工具理性的片面发展，造成的结果是，在我们的前面，“不是鲜花灿烂的春天，而是严寒肃杀的冬夜”。在韦伯看来，这主要是由于工具理性的过度发达和价值理性相对的萎缩，加上官僚科层组织的发达，这两点使得人类的自由受到了越来越严重的桎梏。有一本美国学者米茨曼研究韦伯的书，题目就叫做《铁笼》，说人类未来发展的趋势，就是人类日益被自己打造的铁笼所囚禁，不管人工智能如何发展，在前面等待着我们的就是那个铁笼。  
     
    现在包括人工智能，无论是掌握在大资本家手里，还是专制者手里，确实给你打造好了这样的铁笼，只不过以往的帝王都梦想不到今天控制社会能够达到这种程度。这在美国好莱坞的很多电影里都有反映，一个独裁者背后有一个超级计算机的大球，可以即时监控全世界，你是谁、你正在哪，你在干什么。过去，秦始皇时代是腹诽心谤者要杀头，但是他并没有一个监控的手段知道你怎么就腹诽心谤了。现在就行了，能监控你的一举一动，你在想什么做什么，都能知道，都能监控。你在哪儿消费了，你上午手机支付，买了一包方便面，下午到天则发表了一通人工智能给我们带来的风险的演讲。现在的人确实面临着很多风险，包括大数据，掌握在什么人手里。一个信息产业的老板能看见每个人在做什么，我们就看不到。这就是所谓的垄断信息的资本家，像中国这样的社会，大数据掌握在少数个人手里，确实存在一定的大风险。  
     
    
    何光沪：
    现在的云服务。  
     
    
    王焱：
    但是人之为人，不是人类设计的电脑，对于民众而言，他可能今天这么想、明天那么想，你很难给他定义成一个僵死的不变的东西，固定的东西。莎士比亚写的《尤利乌斯•凯撒》，里边的民众今天说拥护杀死凯撒的人，因为他要独裁要破坏罗马的共和体制；明天又说杀死凯撒的人才是真正的阴谋家。此亦一是非，彼亦一是非。西方大学讲政治哲学经常拿这个当作教材，用来说明民众不可靠。但是实际上人可能就是这样的。  
     
    人工智能虽然发达，我觉得这个问题还没有解决，就是人工智能还没有自我意识，它虽有智力，但是仅仅是一种计算手段效率的智力，没有权衡目的价值的智力。如果以目的  -  手段来定向，对于正常人类来说，可以说  AI  还是个脑残。  
     
    人的价值观从哪儿来？能否输入给电脑，那种价值观应当优先？  
     
    有些电影里我们看到的，做这个机器人的工程师给机器人输入一条绝对律令：绝对不允许违抗制造你的主人，机器人还可以分析主人的音频、分析出你是不是真的主人说的话，你说的他不能违抗，绝对不能。否则他的智力超常，把你杀死了怎么办。绝对律令是造机器的人设定的。但是这个主人也不见得就是好人，可能是专制者、要当法西斯，发展到后面，很可能谁掌握的机器人多谁就是老大，就像以前说，一个奴隶主有多少奴隶，他有一千个奴隶，厉害！他有五百个，就差一些。他有五百个能够控制一个地区，有一千个机器人就能控制世界。  
     
    人工智能的价值观没有解决，现在还是设计它、制造它的人能给它输入一些规则和戒律，按现在这种发展，实际上还是工具理性的发展，机器人执行一个命令，命令是工具理性的，就是说你可在最短的时间内计算出最优的解决办法。但是机器人并没有能力选择，认为依据我的价值观实现这个目的是不道德的，所以我就恕难从命，不行动，至少现在的人工智能还判断不了。  
     
    在人类的控制之下的人工智能，应当说是眼耳鼻舌身的发达，这是人的器官的一个延长吧，行动能力效率的增加，但现在已经面临那种危险了，就是刚才说的，他可以利用他的这种工具化的延长、效率的提高，实现不道德的的目的。以人类现有的法律，第一你也跟不上，第二，有的国家根本不考虑这个，只要能维持这个权力干什么都行，人工智能干什么都行，用来实行监控，实现各种目的，你看什么书、上什么网、跟谁聊天，我都给你总结下来、记录下来。  
     
    
    何光沪：
    现在已经做到了，东德可以做到。  
     
    
    王焱：
    就是一个工具理性，失去了价值理性的制衡，实际上还是韦伯说的，我们需要的不是单纯的工具理性的发达，我们需要的是目的理性的权衡，你的理性能综合的均衡考虑你的目的是不是符合道德的、有价值的，然后再来考虑你的手段的效率优化问题，如果你不考虑目的，只考虑不断增加手段的功能效率，确实有很多危险。如果落到比如说心理阴暗的个人手里，确实会给人类带来危险。以前看福柯写的“人的消灭”，觉得有点危言耸听了，现在看人类从有文明到现在大概也就六千年，现在快到了自己消灭自己的时候了，包括人工智能或者生物医学工程。前两天刚看到德国一对夫妻做实验，把两个受精卵交给医院，医院给了他们一笔奖金，他们俩就玩儿去了，那个受精卵不断的发育，最后  6  个月那个医院就说行了，不需要  10  个月就可以出来了，然后在这个过程中，根据他们俩的审美要求，鼻子要高一点，最后就成了一男一女，就是这对夫妻的孩子。  
     
    
    何光沪：
    人工子宫。  
     
    
    王焱：
    对，生物医学工程也挺可怕，这种以前被认为是上帝的密码的东西，现在掌握在人的手里，你可以把基因剪辑来、剪辑去的，现在不但植物，人自身也要用这种基因工程，不知会造出什么怪物来，你说有生命、还是没生命，或者对他可以应用人类的道德、还是不可以应用，都是没研究过的新问题，生物医学工程可能比人工智能危险还更大一些。实际上，人类自己就要杀死自己了，各种离奇古怪的办法。实际上这是一个相当恐怖的时代，需要开放性的讨论，需要各式各样的办法，不见得是那种民主的办法，但是要有方法来防止这种后果吧。这些问题，没有答案。  
     
    我就是想到这些人工智能的价值观谁给他输入，我们是给机器人输入奴隶的道德呢，还是主人的道德呢？这是尼采的说法。如果是文革时期，我们会尽量输入什么三个坚持、四个无限的价值观，如果满街都是这种机器人那也挺恐怖的。这个价值观的输入谁来掌握？比例多少？你认为哪种价值观是最合理的？我觉得现在总的趋势还是在韦伯百年前预言的框架里，确实还没有更好的解决办法，这个是需要研究者要花费力气来研究的。  
     
    另外一个后果是，人工智能的突飞猛进，使得以前人类自身拥有的一些东西大大贬值了，人具有知识、情感、意志、美学观点，像莎士比亚当年讴歌的，人是多么伟大，多么高贵，不就是因为你有这些东西吗，人工智能如果能模仿这个，那就不知会走到哪里去了！无论如何，那些缺乏创造性和创意性思维的人，将会逐渐变成贬值。  
     
     
     [  
    王焱
    政治学、社会学家，天则经济研究所特约研究员；《公共论丛》主编；曾就职于中国社会科学院政治学研究所，原《读书》、《社会学家茶座》等著名学术期刊主编。本文为作者  2018-7-11  在「人工智能与道德风险」云豹沙龙的演讲，，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
     
  

" />
    
    <meta name="author" content="觀點" />

    
    <meta property="og:title" content="王焱：透过计算，支配世界" />
    <meta property="twitter:title" content="王焱：透过计算，支配世界" />
    

  <link rel="stylesheet" type="text/css" href="/opinion/style.css" />
  <link rel="alternate" type="application/rss+xml" title="觀點 - 從草根到大師 git.io/JJCxS" href="/opinion/feed.xml" />

  <!-- Social Share Kit CSS -->
  <link rel="stylesheet" href="/opinion/assets/css/social-share-kit.css" type="text/css">
  <link rel="stylesheet" href="/opinion/assets/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="/opinion/assets/css/bootstrap.min.css" type="text/css">
  <script type="text/javascript" src="/opinion/assets/js/jquery-3.5.1.js"></script>
  <script type="text/javascript" src="/opinion/assets/js/page.js"></script>

</head>

  <body>
    <div class="wrapper-masthead">
  <div class="container">
    <header class="masthead clearfix">
      

      <div class="site-info">
        <h1 class="site-name" style="display: inline-block;"><a href="/opinion/">觀點</a></h1>
        <i class="site-description" style="font-size: 12px;">從草根到大師 git.io/JJCxS</i>
      </div>

      <nav>
        <span id="search-container" >
          <a href="/opinion/tools"><i class="fa fa-bookmark twitter" title="百宝箱"></i></a>
        <a><i class="fa fa-search" title="限前100結果"></i></a><input type="text" id="search-input" placeholder="標題 作者 來源 日期 (17489)"
          style="margin: 10px 0px 0px 0px; height: 30px;width: auto" title="本站最正確的打開方式">
        </span>
        
        
        <a href="/opinion/categories" style="color: Tomato;"><i class="fa fa-tags" title="分类"></i></a>
        
        
        
        <a href="https://be4.herokuapp.com/" style="color: #003366;"><i class="fa fa-comments" title="论坛"></i></a>
        
        
        
        <a href="/opinion/about"><i class="fa fa-info-circle" title="关于"></i></a>
        
        
        <a title="电脑热键：&larr;上一篇(页), &rarr;下一篇(页), ins同来源新一篇，del同来源旧一篇" onclick="toggle_visibility('help')"><i class="fa fa-question-circle"></i></a>
        <a id="fa-home" href="https://nodebe4.github.io" title="BE4服务列表" onclick="//toggle_visibility('site-list')"><i class="fa fa-home" aria-hidden="true"></i></a>
      </nav>

    </header>
    <div id="site-list" class="tags" style="display: block;text-align: right;border-bottom: 1px solid lightGray;"><noscript><span style="background-color: #e8e8e8;color: #d10000;font-size: 14px;">开启浏览器JavaScript以获取搜索功能和更好的浏览体验</span></noscript></div>
    <p id="help" style="font-size: 14px;display: none;text-align: right;"><span style="color:green;">电脑热键：&larr;上一篇(页), &rarr;下一篇(页), ins同来源新一篇, del同来源旧一篇</span>; <span style="color:orange">对应触屏FAB：上下右左</span>; 轉Markdown<a href="https://euangoddard.github.io/clipboard2markdown/"><i class="fa fa-file-text-o"></i></a></p>
  </div>
</div>

<script type="text/javascript" >
  function toggle_visibility(id){
    var help = document.getElementById(id)
    if (help.style.display=='none'){
      help.style.display='block';
    }else{
      help.style.display='none';
    }
  }

  const url = "https://nodebe4.github.io/sitelist.json"

  document.addEventListener("DOMContentLoaded", function(event){
    // var homebtn = document.getElementById("fa-home")
    // homebtn.removeAttribute("href")
    var content = document.getElementById("site-list");
    content.innerHTML = ''
    var ul = document.createElement("ul")
    ul.classList.add("label")
    content.appendChild(ul)
    var cnt = 0

    $.getJSON(url, function(allsites) {

      allsites.map(item =>{
        var li = document.createElement('li')
        li.classList.add("tag")
        li.id = 'site-' + cnt
        ul.appendChild(li)
        var a0 = document.createElement('a')
        li.appendChild(a0)
        a0.href = item.url[0]
        var span = document.createElement('span')
        a0.appendChild(span)
        span.innerText = item['name']
        // span.style.backgroundColor = item['background-color']
        // span.style.color='#E4CBC3'
        span.style.color = item['background-color']
        span.style['font-size'] = '14px'
        cnt += 1
        // test_alive(li.id, a0.href)
      })
    })
  })

function test_alive(id, url){
  var divstatus = document.getElementById(id)
  const base = 'https://textance.herokuapp.com/title/'
  var fullurl = base + url
  $.ajax({
      url: fullurl,
      complete: function(data) {
        if (data.responseText.includes('502')){
          // divstatus.style.color='#FBB7B7'
          // divstatus.style.color='gray'
          // divstatus.title = "服务器无响应"
          divstatus.parentNode.removeChild(divstatus)
        }else{
          // divstatus.style.color='#B6FAC8'
          divstatus.title = data.responseText
        }
      }
  });
  return divstatus
}
</script>



    <!-- Left & centered positioning -->

<div class="ssk-sticky ssk-right ssk-center ssk-sticky-hide-xs ssk-group ssk-round">
  
    <a href="https://be4news.pythonanywhere.com/archivenow/ia/http%3A%2F%2Funirule.cloud%2Findex.php%3Fc%3Darticle%26id%3D4664" class="ssk ssk-link" title="存到互联网档案馆" target="_blank"></a>
    <a href="https://www.facebook.com/sharer.php?u=http://unirule.cloud/index.php?c=article&id=4664" class="ssk ssk-facebook"></a>
    <a href="https://twitter.com/intent/tweet?url=http://unirule.cloud/index.php?c=article&id=4664&text=王焱：透过计算，支配世界&hashtags=觀點" class="ssk ssk-twitter"></a>
    <a href="https://reddit.com/submit?url=http://unirule.cloud/index.php?c=article&id=4664&title=王焱：透过计算，支配世界" class="ssk ssk-reddit"></a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://unirule.cloud/index.php?c=article&id=4664&title=王焱：透过计算，支配世界" class="ssk ssk-linkedin"></a>
    <a href="mailto:{email_address}?subject=王焱：透过计算，支配世界&body=
  
              

        
 
    本文作者王焱  
     
     AI  的讨论网上很多，以前也看过一些，并没有特别关注过，这次开会前看了看资料，我觉得不管人工智能如何发展，至少到目前，经典的社会理论框架里还能容纳这些问题。  
     
     1917  年马克斯•韦伯在慕尼黑大学有一个演讲《学术作为一种志业》，当时是德国在参加一次大战，国内又有人鼓动革命，社会动荡，乌云笼罩，大学生期望他能够指导一下，面对乱世应当采取什么样的政治立场，结果韦伯也没回答学生的问题，他讲的是在当下学术怎么样能成为一种志业。他在讲演里边已经概括了他眼中的现代社会，第一就是，我们的时代是个“透过计算，支配世界”的时代，这意味着这是个工具理性特别发达的时代，而其他的行动类型正在逐渐萎缩。  
     
    人工智能说到底也就是一个“算法的革命”。无论哪种人工智能，现在计算上没有超出以往设想的那种能力，特别像超级计算机。据最新公布的数字，美国名为“  Summit  ”的超级计算机的浮点运算速度可达每秒  20  亿亿次。听说中国的超级计算机，经常是在那儿闲置的，因为这个计算机造出来了，但是提不出计算的方案来，什么问题是值得你进行超级计算的？每秒  20  亿亿次。我们的创新能力差，尽管超级计算的能力有了，但是你设计不出要计算的东西来，这是中国的一个特别情况。但是按韦伯的说法，透过计算支配世界的理性是一种工具理性，它在手段上、功能效率上突飞猛进，但是价值与目的方面的理性考虑越来越萎缩，这就出现了一种极大的不平衡。  
     
    
    何光沪：
    还有一个题目可以计算，  14  亿人怎么想。  
     
    
    王焱：
    光沪这个问题恐怕不是计算机能够计算出来的。工具理性的发达，其他的行动类型逐渐萎缩，按韦伯说的，社会行动的理想类型，有传统型的、情感型的、价值合理型的、工具合理型的。人工智能不管怎么发达，比如说  AlphaGo  给它输进既定的围棋游戏规则以后，你可以计算在这样的规则下，怎么样能通过算法制胜。电脑发明以前，都是一代人一代人自己发明自己的棋局，人寿几何？速度很慢。现在用电脑可能每秒钟几亿亿次，可能把以往你穷极人类的想象也做不出的那些棋局发明出来，所以棋手柯洁后来说，现在人得向电脑学习。他下棋的对手是电脑，电脑的棋局我们都想不到，甚至理解不了。但是围棋的游戏规则还是人给机器输入的，怎么叫赢，怎么叫输，电脑是在这个既定规则下演算。所以人工智能的突飞猛进，还是在工具理性的框架内，在既定规则既定的目的之下的发展。相比之下，价值理性就不行了，这样高的运算速度，目的何在？在社会政治领域内，韦伯嘲笑过那些只讲价值正确、目的正确的人，最终只是“没有结果的亢奋”。但是只讲工具效率，不去思考目的的类型蔓延世界，也许结果更可怕。  
     
    韦伯这一派的社会理论，严格区分自然科学的方法和人文社会科学的方法是不一样的，为什么呢？因为自然科学的对象并没有反思能力，也没有价值观。你看一颗恒星在那儿，不会说自己的价值观变了，它就挪了一个位置。但是人文社会研究就不同了，它们研究的对象是人，人有价值观，有反思能力，他不但要在手段的意义上，追求功能与效率，还要价值的实现，还要追求目的和意义。所以自然科学的方法不适用于人文社会科学。人工智能也一样，我们把它看作一种拟人化的智能，但是有些东西它自己发明不出来。还要靠人类为它输入。下棋的规则是人制定的，不是机器制定的。机器人能做什么，不能做什么，也是由人指定的。即使人工智能自己有创新能力，那也是在既定的规则范围之内的创新。人工智能的发展，可以说是古代兵家思维的畸形发展，在给定的目的之下取胜就行。不可能自己发明一个规则或确定一个目的，自己能发明一个规则或目的，你就不是人工智能了，就是人类了。  
     
    韦伯演讲正好已经过去了  100  年，人类社会的百年，他的预测并没有过时，这首先就是透过计算支配世界的能力突飞猛进，所以他定义我们的时代是一个工具理性化的时代。人类的价值理性却没有什么进展。但是人类的奇特之处就在于有各式各样的价值观，各式各样的情感，人类的行动必然依据一定的价值观进行。现在的无人驾驶汽车就遇到这种伦理难题，你在公路上行驶，左边有一个小孩，右边有一辆车、里面有一车人，你要避让就出危险，你是选救这个孩子呢、还是选择救那一车人。你要是像功利主义者那样思考，可能就会牺牲那个孩子吧，另外一边有一车人呢。但是无论人怎么选择，还是有一套想法的，有一种理据。但是你把这个交给无人驾驶汽车，它就没法选择了，所以我估计无人驾驶汽车也遇到瓶颈了，要是人还好办，要是车给它设定牺牲那孩子，就难办了。包括无人机之类的，像阿富汗发生的这种情况，你设定的程序唯一目的就是取胜，不是人道主义，不是雨果说的，要用一个孩子的无辜牺牲，哪怕给我至尊的帝王之位我也不换，但是无人机不可能这么想，它就是取胜，没有什么人道主义这类的考虑。  
     
    人类社会拥有各式各样的价值观，现代人类社会已经进化到那种程度了，自从洛克的《宗教宽容》一书以来，我们一般人对于各式各样的价值观大都能够兼容，认为各种价值观都有其合理的方面  ,  但是我们每个人还是有自己的价值观，对于人工智能来说，价值观你怎么给它输入呢？这就是一个难题了。  
     
    现实中的人我们知道，某人被人定义为“脑残”，但是我们不会因为说他脑残，就要消灭他，他的脑残价值观也许也有他合理的地方，在这种意义上，我们也可将人工智能定义为“脑残”，因为他只有工具理性而没有价值理性，没有情感。  80  年代读韦伯的时候，觉得他太悲观了，工具理性的发达，也有若干正面意义，至少它导致了人的福利的提高。但是工具理性的片面发展，造成的结果是，在我们的前面，“不是鲜花灿烂的春天，而是严寒肃杀的冬夜”。在韦伯看来，这主要是由于工具理性的过度发达和价值理性相对的萎缩，加上官僚科层组织的发达，这两点使得人类的自由受到了越来越严重的桎梏。有一本美国学者米茨曼研究韦伯的书，题目就叫做《铁笼》，说人类未来发展的趋势，就是人类日益被自己打造的铁笼所囚禁，不管人工智能如何发展，在前面等待着我们的就是那个铁笼。  
     
    现在包括人工智能，无论是掌握在大资本家手里，还是专制者手里，确实给你打造好了这样的铁笼，只不过以往的帝王都梦想不到今天控制社会能够达到这种程度。这在美国好莱坞的很多电影里都有反映，一个独裁者背后有一个超级计算机的大球，可以即时监控全世界，你是谁、你正在哪，你在干什么。过去，秦始皇时代是腹诽心谤者要杀头，但是他并没有一个监控的手段知道你怎么就腹诽心谤了。现在就行了，能监控你的一举一动，你在想什么做什么，都能知道，都能监控。你在哪儿消费了，你上午手机支付，买了一包方便面，下午到天则发表了一通人工智能给我们带来的风险的演讲。现在的人确实面临着很多风险，包括大数据，掌握在什么人手里。一个信息产业的老板能看见每个人在做什么，我们就看不到。这就是所谓的垄断信息的资本家，像中国这样的社会，大数据掌握在少数个人手里，确实存在一定的大风险。  
     
    
    何光沪：
    现在的云服务。  
     
    
    王焱：
    但是人之为人，不是人类设计的电脑，对于民众而言，他可能今天这么想、明天那么想，你很难给他定义成一个僵死的不变的东西，固定的东西。莎士比亚写的《尤利乌斯•凯撒》，里边的民众今天说拥护杀死凯撒的人，因为他要独裁要破坏罗马的共和体制；明天又说杀死凯撒的人才是真正的阴谋家。此亦一是非，彼亦一是非。西方大学讲政治哲学经常拿这个当作教材，用来说明民众不可靠。但是实际上人可能就是这样的。  
     
    人工智能虽然发达，我觉得这个问题还没有解决，就是人工智能还没有自我意识，它虽有智力，但是仅仅是一种计算手段效率的智力，没有权衡目的价值的智力。如果以目的  -  手段来定向，对于正常人类来说，可以说  AI  还是个脑残。  
     
    人的价值观从哪儿来？能否输入给电脑，那种价值观应当优先？  
     
    有些电影里我们看到的，做这个机器人的工程师给机器人输入一条绝对律令：绝对不允许违抗制造你的主人，机器人还可以分析主人的音频、分析出你是不是真的主人说的话，你说的他不能违抗，绝对不能。否则他的智力超常，把你杀死了怎么办。绝对律令是造机器的人设定的。但是这个主人也不见得就是好人，可能是专制者、要当法西斯，发展到后面，很可能谁掌握的机器人多谁就是老大，就像以前说，一个奴隶主有多少奴隶，他有一千个奴隶，厉害！他有五百个，就差一些。他有五百个能够控制一个地区，有一千个机器人就能控制世界。  
     
    人工智能的价值观没有解决，现在还是设计它、制造它的人能给它输入一些规则和戒律，按现在这种发展，实际上还是工具理性的发展，机器人执行一个命令，命令是工具理性的，就是说你可在最短的时间内计算出最优的解决办法。但是机器人并没有能力选择，认为依据我的价值观实现这个目的是不道德的，所以我就恕难从命，不行动，至少现在的人工智能还判断不了。  
     
    在人类的控制之下的人工智能，应当说是眼耳鼻舌身的发达，这是人的器官的一个延长吧，行动能力效率的增加，但现在已经面临那种危险了，就是刚才说的，他可以利用他的这种工具化的延长、效率的提高，实现不道德的的目的。以人类现有的法律，第一你也跟不上，第二，有的国家根本不考虑这个，只要能维持这个权力干什么都行，人工智能干什么都行，用来实行监控，实现各种目的，你看什么书、上什么网、跟谁聊天，我都给你总结下来、记录下来。  
     
    
    何光沪：
    现在已经做到了，东德可以做到。  
     
    
    王焱：
    就是一个工具理性，失去了价值理性的制衡，实际上还是韦伯说的，我们需要的不是单纯的工具理性的发达，我们需要的是目的理性的权衡，你的理性能综合的均衡考虑你的目的是不是符合道德的、有价值的，然后再来考虑你的手段的效率优化问题，如果你不考虑目的，只考虑不断增加手段的功能效率，确实有很多危险。如果落到比如说心理阴暗的个人手里，确实会给人类带来危险。以前看福柯写的“人的消灭”，觉得有点危言耸听了，现在看人类从有文明到现在大概也就六千年，现在快到了自己消灭自己的时候了，包括人工智能或者生物医学工程。前两天刚看到德国一对夫妻做实验，把两个受精卵交给医院，医院给了他们一笔奖金，他们俩就玩儿去了，那个受精卵不断的发育，最后  6  个月那个医院就说行了，不需要  10  个月就可以出来了，然后在这个过程中，根据他们俩的审美要求，鼻子要高一点，最后就成了一男一女，就是这对夫妻的孩子。  
     
    
    何光沪：
    人工子宫。  
     
    
    王焱：
    对，生物医学工程也挺可怕，这种以前被认为是上帝的密码的东西，现在掌握在人的手里，你可以把基因剪辑来、剪辑去的，现在不但植物，人自身也要用这种基因工程，不知会造出什么怪物来，你说有生命、还是没生命，或者对他可以应用人类的道德、还是不可以应用，都是没研究过的新问题，生物医学工程可能比人工智能危险还更大一些。实际上，人类自己就要杀死自己了，各种离奇古怪的办法。实际上这是一个相当恐怖的时代，需要开放性的讨论，需要各式各样的办法，不见得是那种民主的办法，但是要有方法来防止这种后果吧。这些问题，没有答案。  
     
    我就是想到这些人工智能的价值观谁给他输入，我们是给机器人输入奴隶的道德呢，还是主人的道德呢？这是尼采的说法。如果是文革时期，我们会尽量输入什么三个坚持、四个无限的价值观，如果满街都是这种机器人那也挺恐怖的。这个价值观的输入谁来掌握？比例多少？你认为哪种价值观是最合理的？我觉得现在总的趋势还是在韦伯百年前预言的框架里，确实还没有更好的解决办法，这个是需要研究者要花费力气来研究的。  
     
    另外一个后果是，人工智能的突飞猛进，使得以前人类自身拥有的一些东西大大贬值了，人具有知识、情感、意志、美学观点，像莎士比亚当年讴歌的，人是多么伟大，多么高贵，不就是因为你有这些东西吗，人工智能如果能模仿这个，那就不知会走到哪里去了！无论如何，那些缺乏创造性和创意性思维的人，将会逐渐变成贬值。  
     
     
     [  
    王焱
    政治学、社会学家，天则经济研究所特约研究员；《公共论丛》主编；曾就职于中国社会科学院政治学研究所，原《读书》、《社会学家茶座》等著名学术期刊主编。本文为作者  2018-7-11  在「人工智能与道德风险」云豹沙龙的演讲，，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
     
  

" class="ssk ssk-email"></a>
    <a href="http://pinterest.com/pin/create/link/?url=http://unirule.cloud/index.php?c=article&id=4664" class="ssk ssk-pinterest"></a>
    <a href="https://www.tumblr.com/widgets/share/tool?canonicalUrl=http://unirule.cloud/index.php?c=article&id=4664&title=王焱：透过计算，支配世界&caption=
  
              

        
 
    本文作者王焱  
     
     AI  的讨论网上很多，以前也看过一些，并没有特别关注过，这次开会前看了看资料，我觉得不管人工智能如何发展，至少到目前，经典的社会理论框架里还能容纳这些问题。  
     
     1917  年马克斯•韦伯在慕尼黑大学有一个演讲《学术作为一种志业》，当时是德国在参加一次大战，国内又有人鼓动革命，社会动荡，乌云笼罩，大学生期望他能够指导一下，面对乱世应当采取什么样的政治立场，结果韦伯也没回答学生的问题，他讲的是在当下学术怎么样能成为一种志业。他在讲演里边已经概括了他眼中的现代社会，第一就是，我们的时代是个“透过计算，支配世界”的时代，这意味着这是个工具理性特别发达的时代，而其他的行动类型正在逐渐萎缩。  
     
    人工智能说到底也就是一个“算法的革命”。无论哪种人工智能，现在计算上没有超出以往设想的那种能力，特别像超级计算机。据最新公布的数字，美国名为“  Summit  ”的超级计算机的浮点运算速度可达每秒  20  亿亿次。听说中国的超级计算机，经常是在那儿闲置的，因为这个计算机造出来了，但是提不出计算的方案来，什么问题是值得你进行超级计算的？每秒  20  亿亿次。我们的创新能力差，尽管超级计算的能力有了，但是你设计不出要计算的东西来，这是中国的一个特别情况。但是按韦伯的说法，透过计算支配世界的理性是一种工具理性，它在手段上、功能效率上突飞猛进，但是价值与目的方面的理性考虑越来越萎缩，这就出现了一种极大的不平衡。  
     
    
    何光沪：
    还有一个题目可以计算，  14  亿人怎么想。  
     
    
    王焱：
    光沪这个问题恐怕不是计算机能够计算出来的。工具理性的发达，其他的行动类型逐渐萎缩，按韦伯说的，社会行动的理想类型，有传统型的、情感型的、价值合理型的、工具合理型的。人工智能不管怎么发达，比如说  AlphaGo  给它输进既定的围棋游戏规则以后，你可以计算在这样的规则下，怎么样能通过算法制胜。电脑发明以前，都是一代人一代人自己发明自己的棋局，人寿几何？速度很慢。现在用电脑可能每秒钟几亿亿次，可能把以往你穷极人类的想象也做不出的那些棋局发明出来，所以棋手柯洁后来说，现在人得向电脑学习。他下棋的对手是电脑，电脑的棋局我们都想不到，甚至理解不了。但是围棋的游戏规则还是人给机器输入的，怎么叫赢，怎么叫输，电脑是在这个既定规则下演算。所以人工智能的突飞猛进，还是在工具理性的框架内，在既定规则既定的目的之下的发展。相比之下，价值理性就不行了，这样高的运算速度，目的何在？在社会政治领域内，韦伯嘲笑过那些只讲价值正确、目的正确的人，最终只是“没有结果的亢奋”。但是只讲工具效率，不去思考目的的类型蔓延世界，也许结果更可怕。  
     
    韦伯这一派的社会理论，严格区分自然科学的方法和人文社会科学的方法是不一样的，为什么呢？因为自然科学的对象并没有反思能力，也没有价值观。你看一颗恒星在那儿，不会说自己的价值观变了，它就挪了一个位置。但是人文社会研究就不同了，它们研究的对象是人，人有价值观，有反思能力，他不但要在手段的意义上，追求功能与效率，还要价值的实现，还要追求目的和意义。所以自然科学的方法不适用于人文社会科学。人工智能也一样，我们把它看作一种拟人化的智能，但是有些东西它自己发明不出来。还要靠人类为它输入。下棋的规则是人制定的，不是机器制定的。机器人能做什么，不能做什么，也是由人指定的。即使人工智能自己有创新能力，那也是在既定的规则范围之内的创新。人工智能的发展，可以说是古代兵家思维的畸形发展，在给定的目的之下取胜就行。不可能自己发明一个规则或确定一个目的，自己能发明一个规则或目的，你就不是人工智能了，就是人类了。  
     
    韦伯演讲正好已经过去了  100  年，人类社会的百年，他的预测并没有过时，这首先就是透过计算支配世界的能力突飞猛进，所以他定义我们的时代是一个工具理性化的时代。人类的价值理性却没有什么进展。但是人类的奇特之处就在于有各式各样的价值观，各式各样的情感，人类的行动必然依据一定的价值观进行。现在的无人驾驶汽车就遇到这种伦理难题，你在公路上行驶，左边有一个小孩，右边有一辆车、里面有一车人，你要避让就出危险，你是选救这个孩子呢、还是选择救那一车人。你要是像功利主义者那样思考，可能就会牺牲那个孩子吧，另外一边有一车人呢。但是无论人怎么选择，还是有一套想法的，有一种理据。但是你把这个交给无人驾驶汽车，它就没法选择了，所以我估计无人驾驶汽车也遇到瓶颈了，要是人还好办，要是车给它设定牺牲那孩子，就难办了。包括无人机之类的，像阿富汗发生的这种情况，你设定的程序唯一目的就是取胜，不是人道主义，不是雨果说的，要用一个孩子的无辜牺牲，哪怕给我至尊的帝王之位我也不换，但是无人机不可能这么想，它就是取胜，没有什么人道主义这类的考虑。  
     
    人类社会拥有各式各样的价值观，现代人类社会已经进化到那种程度了，自从洛克的《宗教宽容》一书以来，我们一般人对于各式各样的价值观大都能够兼容，认为各种价值观都有其合理的方面  ,  但是我们每个人还是有自己的价值观，对于人工智能来说，价值观你怎么给它输入呢？这就是一个难题了。  
     
    现实中的人我们知道，某人被人定义为“脑残”，但是我们不会因为说他脑残，就要消灭他，他的脑残价值观也许也有他合理的地方，在这种意义上，我们也可将人工智能定义为“脑残”，因为他只有工具理性而没有价值理性，没有情感。  80  年代读韦伯的时候，觉得他太悲观了，工具理性的发达，也有若干正面意义，至少它导致了人的福利的提高。但是工具理性的片面发展，造成的结果是，在我们的前面，“不是鲜花灿烂的春天，而是严寒肃杀的冬夜”。在韦伯看来，这主要是由于工具理性的过度发达和价值理性相对的萎缩，加上官僚科层组织的发达，这两点使得人类的自由受到了越来越严重的桎梏。有一本美国学者米茨曼研究韦伯的书，题目就叫做《铁笼》，说人类未来发展的趋势，就是人类日益被自己打造的铁笼所囚禁，不管人工智能如何发展，在前面等待着我们的就是那个铁笼。  
     
    现在包括人工智能，无论是掌握在大资本家手里，还是专制者手里，确实给你打造好了这样的铁笼，只不过以往的帝王都梦想不到今天控制社会能够达到这种程度。这在美国好莱坞的很多电影里都有反映，一个独裁者背后有一个超级计算机的大球，可以即时监控全世界，你是谁、你正在哪，你在干什么。过去，秦始皇时代是腹诽心谤者要杀头，但是他并没有一个监控的手段知道你怎么就腹诽心谤了。现在就行了，能监控你的一举一动，你在想什么做什么，都能知道，都能监控。你在哪儿消费了，你上午手机支付，买了一包方便面，下午到天则发表了一通人工智能给我们带来的风险的演讲。现在的人确实面临着很多风险，包括大数据，掌握在什么人手里。一个信息产业的老板能看见每个人在做什么，我们就看不到。这就是所谓的垄断信息的资本家，像中国这样的社会，大数据掌握在少数个人手里，确实存在一定的大风险。  
     
    
    何光沪：
    现在的云服务。  
     
    
    王焱：
    但是人之为人，不是人类设计的电脑，对于民众而言，他可能今天这么想、明天那么想，你很难给他定义成一个僵死的不变的东西，固定的东西。莎士比亚写的《尤利乌斯•凯撒》，里边的民众今天说拥护杀死凯撒的人，因为他要独裁要破坏罗马的共和体制；明天又说杀死凯撒的人才是真正的阴谋家。此亦一是非，彼亦一是非。西方大学讲政治哲学经常拿这个当作教材，用来说明民众不可靠。但是实际上人可能就是这样的。  
     
    人工智能虽然发达，我觉得这个问题还没有解决，就是人工智能还没有自我意识，它虽有智力，但是仅仅是一种计算手段效率的智力，没有权衡目的价值的智力。如果以目的  -  手段来定向，对于正常人类来说，可以说  AI  还是个脑残。  
     
    人的价值观从哪儿来？能否输入给电脑，那种价值观应当优先？  
     
    有些电影里我们看到的，做这个机器人的工程师给机器人输入一条绝对律令：绝对不允许违抗制造你的主人，机器人还可以分析主人的音频、分析出你是不是真的主人说的话，你说的他不能违抗，绝对不能。否则他的智力超常，把你杀死了怎么办。绝对律令是造机器的人设定的。但是这个主人也不见得就是好人，可能是专制者、要当法西斯，发展到后面，很可能谁掌握的机器人多谁就是老大，就像以前说，一个奴隶主有多少奴隶，他有一千个奴隶，厉害！他有五百个，就差一些。他有五百个能够控制一个地区，有一千个机器人就能控制世界。  
     
    人工智能的价值观没有解决，现在还是设计它、制造它的人能给它输入一些规则和戒律，按现在这种发展，实际上还是工具理性的发展，机器人执行一个命令，命令是工具理性的，就是说你可在最短的时间内计算出最优的解决办法。但是机器人并没有能力选择，认为依据我的价值观实现这个目的是不道德的，所以我就恕难从命，不行动，至少现在的人工智能还判断不了。  
     
    在人类的控制之下的人工智能，应当说是眼耳鼻舌身的发达，这是人的器官的一个延长吧，行动能力效率的增加，但现在已经面临那种危险了，就是刚才说的，他可以利用他的这种工具化的延长、效率的提高，实现不道德的的目的。以人类现有的法律，第一你也跟不上，第二，有的国家根本不考虑这个，只要能维持这个权力干什么都行，人工智能干什么都行，用来实行监控，实现各种目的，你看什么书、上什么网、跟谁聊天，我都给你总结下来、记录下来。  
     
    
    何光沪：
    现在已经做到了，东德可以做到。  
     
    
    王焱：
    就是一个工具理性，失去了价值理性的制衡，实际上还是韦伯说的，我们需要的不是单纯的工具理性的发达，我们需要的是目的理性的权衡，你的理性能综合的均衡考虑你的目的是不是符合道德的、有价值的，然后再来考虑你的手段的效率优化问题，如果你不考虑目的，只考虑不断增加手段的功能效率，确实有很多危险。如果落到比如说心理阴暗的个人手里，确实会给人类带来危险。以前看福柯写的“人的消灭”，觉得有点危言耸听了，现在看人类从有文明到现在大概也就六千年，现在快到了自己消灭自己的时候了，包括人工智能或者生物医学工程。前两天刚看到德国一对夫妻做实验，把两个受精卵交给医院，医院给了他们一笔奖金，他们俩就玩儿去了，那个受精卵不断的发育，最后  6  个月那个医院就说行了，不需要  10  个月就可以出来了，然后在这个过程中，根据他们俩的审美要求，鼻子要高一点，最后就成了一男一女，就是这对夫妻的孩子。  
     
    
    何光沪：
    人工子宫。  
     
    
    王焱：
    对，生物医学工程也挺可怕，这种以前被认为是上帝的密码的东西，现在掌握在人的手里，你可以把基因剪辑来、剪辑去的，现在不但植物，人自身也要用这种基因工程，不知会造出什么怪物来，你说有生命、还是没生命，或者对他可以应用人类的道德、还是不可以应用，都是没研究过的新问题，生物医学工程可能比人工智能危险还更大一些。实际上，人类自己就要杀死自己了，各种离奇古怪的办法。实际上这是一个相当恐怖的时代，需要开放性的讨论，需要各式各样的办法，不见得是那种民主的办法，但是要有方法来防止这种后果吧。这些问题，没有答案。  
     
    我就是想到这些人工智能的价值观谁给他输入，我们是给机器人输入奴隶的道德呢，还是主人的道德呢？这是尼采的说法。如果是文革时期，我们会尽量输入什么三个坚持、四个无限的价值观，如果满街都是这种机器人那也挺恐怖的。这个价值观的输入谁来掌握？比例多少？你认为哪种价值观是最合理的？我觉得现在总的趋势还是在韦伯百年前预言的框架里，确实还没有更好的解决办法，这个是需要研究者要花费力气来研究的。  
     
    另外一个后果是，人工智能的突飞猛进，使得以前人类自身拥有的一些东西大大贬值了，人具有知识、情感、意志、美学观点，像莎士比亚当年讴歌的，人是多么伟大，多么高贵，不就是因为你有这些东西吗，人工智能如果能模仿这个，那就不知会走到哪里去了！无论如何，那些缺乏创造性和创意性思维的人，将会逐渐变成贬值。  
     
     
     [  
    王焱
    政治学、社会学家，天则经济研究所特约研究员；《公共论丛》主编；曾就职于中国社会科学院政治学研究所，原《读书》、《社会学家茶座》等著名学术期刊主编。本文为作者  2018-7-11  在「人工智能与道德风险」云豹沙龙的演讲，，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
     
  

&tags=觀點" class="ssk ssk-tumblr"></a>
    <a href="https://buffer.com/add?text=王焱：透过计算，支配世界&url=http://unirule.cloud/index.php?c=article&id=4664" class="ssk ssk-buffer"></a>
</div>


    <div id="main" role="main" class="container">
      
  <!-- Html Elements for Search -->
  <ul id="results-container" class="searched" style="color: #2980B9;"></ul>

  <script src="/opinion/assets/js/simple-jekyll-search.min.js"></script>

  <!-- Configuration -->
  <script>
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/opinion/search.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a><time>{date}</time><a class="tag">{category}</a></li>',
    noResultsText: '没找到',
    limit: 100,
    fuzzy: false,
    exclude: ['Welcome']
  })

  </script>

      







  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
    


  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
    



<article class="post">
  <h1>王焱：透过计算，支配世界</h1>
  <!-- Look the author details up from the site config. -->
  

  <div>
    <span class="date">
      2018-08-11
    </span>

    <!-- Output author details if some exist. -->
    
      
        <span>
            <!-- Personal Info. -->
            <a  style="font-size:14px;">作者: 王焱</a>
        </span>
      
    


    <ul class="tag">
      <li>
        <a href="https://nodebe4.github.io/opinion/categories/#天则观点">
          天则观点
        </a>
      </li>
    </ul>

    
        <span>
            <!-- Personal Info. -->
            <a href="http://unirule.cloud/index.php?c=article&id=4664" style="font-size:14px;">原文</a>
        </span>
    

    <span style="float: right;" title="天则观点的其它文章">
      <a style="font-size: 14px;" rel="nofollow" href="#sametag" class="tags">#天则观点 的其它文章</a>
    </span>

  </div>

  <div class="entry">
    
    
    
    <div class="article">
  <div class="body-text">
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> <br />         <div style="text-align:center;">
<img alt="" src="/uploads/2018/08/111054086011.jpg" style="text-indent:21pt;" />
        </div>
 </span></p>
    <p class="MsoNormal" style="text-align:center;text-indent:0cm;">本文作者王焱 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> AI </span> 的讨论网上很多，以前也看过一些，并没有特别关注过，这次开会前看了看资料，我觉得不管人工智能如何发展，至少到目前，经典的社会理论框架里还能容纳这些问题。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> 1917 </span> 年马克斯•韦伯在慕尼黑大学有一个演讲《学术作为一种志业》，当时是德国在参加一次大战，国内又有人鼓动革命，社会动荡，乌云笼罩，大学生期望他能够指导一下，面对乱世应当采取什么样的政治立场，结果韦伯也没回答学生的问题，他讲的是在当下学术怎么样能成为一种志业。他在讲演里边已经概括了他眼中的现代社会，第一就是，我们的时代是个“透过计算，支配世界”的时代，这意味着这是个工具理性特别发达的时代，而其他的行动类型正在逐渐萎缩。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">人工智能说到底也就是一个“算法的革命”。无论哪种人工智能，现在计算上没有超出以往设想的那种能力，特别像超级计算机。据最新公布的数字，美国名为“ <span> Summit </span> ”的超级计算机的浮点运算速度可达每秒 <span> 20 </span> 亿亿次。听说中国的超级计算机，经常是在那儿闲置的，因为这个计算机造出来了，但是提不出计算的方案来，什么问题是值得你进行超级计算的？每秒 <span> 20 </span> 亿亿次。我们的创新能力差，尽管超级计算的能力有了，但是你设计不出要计算的东西来，这是中国的一个特别情况。但是按韦伯的说法，透过计算支配世界的理性是一种工具理性，它在手段上、功能效率上突飞猛进，但是价值与目的方面的理性考虑越来越萎缩，这就出现了一种极大的不平衡。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    何光沪：
   </b> 还有一个题目可以计算， <span> 14 </span> 亿人怎么想。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    王焱：
   </b> 光沪这个问题恐怕不是计算机能够计算出来的。工具理性的发达，其他的行动类型逐渐萎缩，按韦伯说的，社会行动的理想类型，有传统型的、情感型的、价值合理型的、工具合理型的。人工智能不管怎么发达，比如说 <span> AlphaGo </span> 给它输进既定的围棋游戏规则以后，你可以计算在这样的规则下，怎么样能通过算法制胜。电脑发明以前，都是一代人一代人自己发明自己的棋局，人寿几何？速度很慢。现在用电脑可能每秒钟几亿亿次，可能把以往你穷极人类的想象也做不出的那些棋局发明出来，所以棋手柯洁后来说，现在人得向电脑学习。他下棋的对手是电脑，电脑的棋局我们都想不到，甚至理解不了。但是围棋的游戏规则还是人给机器输入的，怎么叫赢，怎么叫输，电脑是在这个既定规则下演算。所以人工智能的突飞猛进，还是在工具理性的框架内，在既定规则既定的目的之下的发展。相比之下，价值理性就不行了，这样高的运算速度，目的何在？在社会政治领域内，韦伯嘲笑过那些只讲价值正确、目的正确的人，最终只是“没有结果的亢奋”。但是只讲工具效率，不去思考目的的类型蔓延世界，也许结果更可怕。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">韦伯这一派的社会理论，严格区分自然科学的方法和人文社会科学的方法是不一样的，为什么呢？因为自然科学的对象并没有反思能力，也没有价值观。你看一颗恒星在那儿，不会说自己的价值观变了，它就挪了一个位置。但是人文社会研究就不同了，它们研究的对象是人，人有价值观，有反思能力，他不但要在手段的意义上，追求功能与效率，还要价值的实现，还要追求目的和意义。所以自然科学的方法不适用于人文社会科学。人工智能也一样，我们把它看作一种拟人化的智能，但是有些东西它自己发明不出来。还要靠人类为它输入。下棋的规则是人制定的，不是机器制定的。机器人能做什么，不能做什么，也是由人指定的。即使人工智能自己有创新能力，那也是在既定的规则范围之内的创新。人工智能的发展，可以说是古代兵家思维的畸形发展，在给定的目的之下取胜就行。不可能自己发明一个规则或确定一个目的，自己能发明一个规则或目的，你就不是人工智能了，就是人类了。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">韦伯演讲正好已经过去了 <span> 100 </span> 年，人类社会的百年，他的预测并没有过时，这首先就是透过计算支配世界的能力突飞猛进，所以他定义我们的时代是一个工具理性化的时代。人类的价值理性却没有什么进展。但是人类的奇特之处就在于有各式各样的价值观，各式各样的情感，人类的行动必然依据一定的价值观进行。现在的无人驾驶汽车就遇到这种伦理难题，你在公路上行驶，左边有一个小孩，右边有一辆车、里面有一车人，你要避让就出危险，你是选救这个孩子呢、还是选择救那一车人。你要是像功利主义者那样思考，可能就会牺牲那个孩子吧，另外一边有一车人呢。但是无论人怎么选择，还是有一套想法的，有一种理据。但是你把这个交给无人驾驶汽车，它就没法选择了，所以我估计无人驾驶汽车也遇到瓶颈了，要是人还好办，要是车给它设定牺牲那孩子，就难办了。包括无人机之类的，像阿富汗发生的这种情况，你设定的程序唯一目的就是取胜，不是人道主义，不是雨果说的，要用一个孩子的无辜牺牲，哪怕给我至尊的帝王之位我也不换，但是无人机不可能这么想，它就是取胜，没有什么人道主义这类的考虑。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">人类社会拥有各式各样的价值观，现代人类社会已经进化到那种程度了，自从洛克的《宗教宽容》一书以来，我们一般人对于各式各样的价值观大都能够兼容，认为各种价值观都有其合理的方面 <span> , </span> 但是我们每个人还是有自己的价值观，对于人工智能来说，价值观你怎么给它输入呢？这就是一个难题了。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">现实中的人我们知道，某人被人定义为“脑残”，但是我们不会因为说他脑残，就要消灭他，他的脑残价值观也许也有他合理的地方，在这种意义上，我们也可将人工智能定义为“脑残”，因为他只有工具理性而没有价值理性，没有情感。 <span> 80 </span> 年代读韦伯的时候，觉得他太悲观了，工具理性的发达，也有若干正面意义，至少它导致了人的福利的提高。但是工具理性的片面发展，造成的结果是，在我们的前面，“不是鲜花灿烂的春天，而是严寒肃杀的冬夜”。在韦伯看来，这主要是由于工具理性的过度发达和价值理性相对的萎缩，加上官僚科层组织的发达，这两点使得人类的自由受到了越来越严重的桎梏。有一本美国学者米茨曼研究韦伯的书，题目就叫做《铁笼》，说人类未来发展的趋势，就是人类日益被自己打造的铁笼所囚禁，不管人工智能如何发展，在前面等待着我们的就是那个铁笼。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">现在包括人工智能，无论是掌握在大资本家手里，还是专制者手里，确实给你打造好了这样的铁笼，只不过以往的帝王都梦想不到今天控制社会能够达到这种程度。这在美国好莱坞的很多电影里都有反映，一个独裁者背后有一个超级计算机的大球，可以即时监控全世界，你是谁、你正在哪，你在干什么。过去，秦始皇时代是腹诽心谤者要杀头，但是他并没有一个监控的手段知道你怎么就腹诽心谤了。现在就行了，能监控你的一举一动，你在想什么做什么，都能知道，都能监控。你在哪儿消费了，你上午手机支付，买了一包方便面，下午到天则发表了一通人工智能给我们带来的风险的演讲。现在的人确实面临着很多风险，包括大数据，掌握在什么人手里。一个信息产业的老板能看见每个人在做什么，我们就看不到。这就是所谓的垄断信息的资本家，像中国这样的社会，大数据掌握在少数个人手里，确实存在一定的大风险。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    何光沪：
   </b> 现在的云服务。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    王焱：
   </b> 但是人之为人，不是人类设计的电脑，对于民众而言，他可能今天这么想、明天那么想，你很难给他定义成一个僵死的不变的东西，固定的东西。莎士比亚写的《尤利乌斯•凯撒》，里边的民众今天说拥护杀死凯撒的人，因为他要独裁要破坏罗马的共和体制；明天又说杀死凯撒的人才是真正的阴谋家。此亦一是非，彼亦一是非。西方大学讲政治哲学经常拿这个当作教材，用来说明民众不可靠。但是实际上人可能就是这样的。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">人工智能虽然发达，我觉得这个问题还没有解决，就是人工智能还没有自我意识，它虽有智力，但是仅仅是一种计算手段效率的智力，没有权衡目的价值的智力。如果以目的 <span> - </span> 手段来定向，对于正常人类来说，可以说 <span> AI </span> 还是个脑残。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">人的价值观从哪儿来？能否输入给电脑，那种价值观应当优先？ <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">有些电影里我们看到的，做这个机器人的工程师给机器人输入一条绝对律令：绝对不允许违抗制造你的主人，机器人还可以分析主人的音频、分析出你是不是真的主人说的话，你说的他不能违抗，绝对不能。否则他的智力超常，把你杀死了怎么办。绝对律令是造机器的人设定的。但是这个主人也不见得就是好人，可能是专制者、要当法西斯，发展到后面，很可能谁掌握的机器人多谁就是老大，就像以前说，一个奴隶主有多少奴隶，他有一千个奴隶，厉害！他有五百个，就差一些。他有五百个能够控制一个地区，有一千个机器人就能控制世界。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">人工智能的价值观没有解决，现在还是设计它、制造它的人能给它输入一些规则和戒律，按现在这种发展，实际上还是工具理性的发展，机器人执行一个命令，命令是工具理性的，就是说你可在最短的时间内计算出最优的解决办法。但是机器人并没有能力选择，认为依据我的价值观实现这个目的是不道德的，所以我就恕难从命，不行动，至少现在的人工智能还判断不了。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">在人类的控制之下的人工智能，应当说是眼耳鼻舌身的发达，这是人的器官的一个延长吧，行动能力效率的增加，但现在已经面临那种危险了，就是刚才说的，他可以利用他的这种工具化的延长、效率的提高，实现不道德的的目的。以人类现有的法律，第一你也跟不上，第二，有的国家根本不考虑这个，只要能维持这个权力干什么都行，人工智能干什么都行，用来实行监控，实现各种目的，你看什么书、上什么网、跟谁聊天，我都给你总结下来、记录下来。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    何光沪：
   </b> 现在已经做到了，东德可以做到。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    王焱：
   </b> 就是一个工具理性，失去了价值理性的制衡，实际上还是韦伯说的，我们需要的不是单纯的工具理性的发达，我们需要的是目的理性的权衡，你的理性能综合的均衡考虑你的目的是不是符合道德的、有价值的，然后再来考虑你的手段的效率优化问题，如果你不考虑目的，只考虑不断增加手段的功能效率，确实有很多危险。如果落到比如说心理阴暗的个人手里，确实会给人类带来危险。以前看福柯写的“人的消灭”，觉得有点危言耸听了，现在看人类从有文明到现在大概也就六千年，现在快到了自己消灭自己的时候了，包括人工智能或者生物医学工程。前两天刚看到德国一对夫妻做实验，把两个受精卵交给医院，医院给了他们一笔奖金，他们俩就玩儿去了，那个受精卵不断的发育，最后 <span> 6 </span> 个月那个医院就说行了，不需要 <span> 10 </span> 个月就可以出来了，然后在这个过程中，根据他们俩的审美要求，鼻子要高一点，最后就成了一男一女，就是这对夫妻的孩子。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    何光沪：
   </b> 人工子宫。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    王焱：
   </b> 对，生物医学工程也挺可怕，这种以前被认为是上帝的密码的东西，现在掌握在人的手里，你可以把基因剪辑来、剪辑去的，现在不但植物，人自身也要用这种基因工程，不知会造出什么怪物来，你说有生命、还是没生命，或者对他可以应用人类的道德、还是不可以应用，都是没研究过的新问题，生物医学工程可能比人工智能危险还更大一些。实际上，人类自己就要杀死自己了，各种离奇古怪的办法。实际上这是一个相当恐怖的时代，需要开放性的讨论，需要各式各样的办法，不见得是那种民主的办法，但是要有方法来防止这种后果吧。这些问题，没有答案。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">我就是想到这些人工智能的价值观谁给他输入，我们是给机器人输入奴隶的道德呢，还是主人的道德呢？这是尼采的说法。如果是文革时期，我们会尽量输入什么三个坚持、四个无限的价值观，如果满街都是这种机器人那也挺恐怖的。这个价值观的输入谁来掌握？比例多少？你认为哪种价值观是最合理的？我觉得现在总的趋势还是在韦伯百年前预言的框架里，确实还没有更好的解决办法，这个是需要研究者要花费力气来研究的。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">另外一个后果是，人工智能的突飞猛进，使得以前人类自身拥有的一些东西大大贬值了，人具有知识、情感、意志、美学观点，像莎士比亚当年讴歌的，人是多么伟大，多么高贵，不就是因为你有这些东西吗，人工智能如果能模仿这个，那就不知会走到哪里去了！无论如何，那些缺乏创造性和创意性思维的人，将会逐渐变成贬值。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> [ </span> <b>
    王焱
   </b> 政治学、社会学家，天则经济研究所特约研究员；《公共论丛》主编；曾就职于中国社会科学院政治学研究所，原《读书》、《社会学家茶座》等著名学术期刊主编。本文为作者 <span> 2018-7-11 </span> 在「人工智能与道德风险」云豹沙龙的演讲，，图片与说明为中评周刊编辑所加，转载请注明 <span> ] </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
  </div>
</div>


  </div>

  <hr style="border-top:1px solid #28323C;"/>

<font size=2px>
  文章版权归原作者所有。
</font>

<div style="text-align:center"><img width="1px" src="https://i.imgur.com/HSw56Ez.png" alt="二维码分享本站" style="text-align:center"/></div>

  <div id="sametag">
    <h4 style="display: inline-block;">#天则观点 的其它文章</h4>
    <span>--<a href="https://nodebe4.github.io/opinion/2019-05-29/%E7%9B%9B%E6%B4%AA-%E7%94%A8%E7%BB%93%E6%9E%84%E6%80%A7%E5%AF%B9%E7%AD%89%E5%8E%9F%E5%88%99%E6%9B%BF%E4%BB%A3%E6%80%BB%E4%BD%93%E5%85%B3%E7%A8%8E%E5%AF%B9%E6%8A%97/">最新</a>-</span>
    <span>-<a href="https://nodebe4.github.io/opinion/2009-04-24/%E6%B2%BB%E5%9B%BD-18%E4%BA%BF%E4%BA%A9%E7%BA%A2%E7%BA%BF%E7%9A%84%E5%88%B6%E5%BA%A6%E5%90%AB%E4%B9%89/">最早</a>--</span>
    
      <li>
        <time>2018-08-24</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-24/%E5%BC%A0%E6%9E%97-%E6%88%BF%E7%A7%9F%E6%9A%B4%E6%B6%A8-%E6%B6%88%E8%B4%B9%E9%99%8D%E7%BA%A7%E4%B8%8E-%E9%87%91%E8%9E%8D%E9%9A%BE%E6%B0%91/">
          张林：房租暴涨、消费降级与“金融难民”
        </a>
      </li>
    
    
      <li>
        <time>2018-08-12</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-12/%E7%9B%9B%E6%B4%AA-%E8%AE%A8%E8%AE%BA%E4%B8%80%E4%B8%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D-%E5%9C%A8-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD/">
          盛洪：讨论一下人工智能的负面影响 —— 在「人工智能与道德风险」云豹沙龙的开场白
        </a>
      </li>
    
    
      <li>
        <time>2018-08-11</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/">
          吕兆楠：人工智能与管理
        </a>
      </li>
    
    
      <li>
        <time>2018-08-11</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-11/%E4%BD%95%E5%85%89%E6%B2%AA-%E5%BB%BA%E7%AB%8B%E5%9B%BD%E9%99%85%E7%9B%91%E7%AE%A1-%E9%A2%84%E9%98%B2%E4%BA%BA%E7%B1%BB%E6%B5%A9%E5%8A%AB-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%97%AE%E9%A2%98%E7%B3%BB%E5%88%97%E6%80%9D%E8%80%83%E4%B9%8B%E4%BA%8C/">
          何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]
        </a>
      </li>
    
  </div>


  <hr>
  <div class="pagination">
    
      <span class="prev" >
          <a href="https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/">
            前一篇：吕兆楠：人工智能与管理
          </a>
      </span>
    
    
      <span class="next" >
          <a href="https://nodebe4.github.io/opinion/2018-08-12/%E7%9B%9B%E6%B4%AA-%E8%AE%A8%E8%AE%BA%E4%B8%80%E4%B8%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D-%E5%9C%A8-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD/">
            後一篇：盛洪：讨论一下人工智能的负面影响 —— 在「人工智能与道德风险」云豹沙龙的开场白
          </a>
      </span>
    

    <script>
    /* post pagination keyboard shortcuts */
    document.body.onkeyup = function(e){
      if (e.keyCode == '37') { window.location = 'https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/'; } // left arrow key
      if (e.keyCode == '39') { window.location = 'https://nodebe4.github.io/opinion/2018-08-12/%E7%9B%9B%E6%B4%AA-%E8%AE%A8%E8%AE%BA%E4%B8%80%E4%B8%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D-%E5%9C%A8-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD/'; } // right arrow key
      if (e.keyCode == '45') { window.location = 'https://nodebe4.github.io/opinion/2018-08-12/%E7%9B%9B%E6%B4%AA-%E8%AE%A8%E8%AE%BA%E4%B8%80%E4%B8%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D-%E5%9C%A8-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD/'; } // insert key
      if (e.keyCode == '46') { window.location = 'https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/'; } // delete key
    };
    </script>
    <link rel="stylesheet" type="text/css" href="/opinion/assets/css/fab.css" />

<div class="fab-wrapper">
  <div class="fab-wheel">
    
    
    
    <a class="fab-action fab-action-1" title="上一篇(热键 &#8594;)" href="https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/">
      <i>后</i>
    </a>
    
    
    <a class="fab-action fab-action-2" title="下一篇(热键 &#8592;)" href="https://nodebe4.github.io/opinion/2018-08-12/%E7%9B%9B%E6%B4%AA-%E8%AE%A8%E8%AE%BA%E4%B8%80%E4%B8%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D-%E5%9C%A8-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD/">
      <i>前</i>
    </a>
    
    
    <a class="fab-action fab-action-3" title="<天则观点>上一篇(热键 ins)" href="https://nodebe4.github.io/opinion/2018-08-12/%E7%9B%9B%E6%B4%AA-%E8%AE%A8%E8%AE%BA%E4%B8%80%E4%B8%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D-%E5%9C%A8-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99%E7%9A%84%E5%BC%80%E5%9C%BA%E7%99%BD/">
      <i>左</i>
    </a>
    
    
    <a class="fab-action fab-action-4" title="<天则观点>下一篇(热键 del)" href="https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/">
      <i>右</i>
    </a>
    
  </div>
</div>


  </div>


  

</article>

    </div>

    <div style="z-index:2;">
<script src="/opinion/assets/js/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  cornerOffset: 20, // px
  id: 'back-to-top',
  backgroundColor: '#ddd',
  textColor: 'red'
})</script>
</div>


    <div class="wrapper-footer" id="footer">
      <div class="container">
        <footer class="footer">
          <img width="200px" src="https://i.imgur.com/HSw56Ez.png" alt="二维码分享本站"/>
<font size=2px>二维码分享本站</font>

<!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

  

  

  
  <li><a href="mailto:beauti4@protonmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
  

  

  

  
  <li><a href="https://github.com/NodeBE4/opinion" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
  

  

  

  

  

  
  <li><a href="/opinion/feed.xml" class="icon-21 rss" title="RSS"><svg viewBox="0 0 512 512"><path d="M201.8 347.2c0 20.3-16.5 36.8-36.8 36.8 -20.3 0-36.8-16.5-36.8-36.8s16.5-36.8 36.8-36.8C185.3 310.4 201.8 326.8 201.8 347.2zM128.2 204.7v54.5c68.5 0.7 124 56.3 124.7 124.7h54.5C306.7 285.3 226.9 205.4 128.2 204.7zM128.2 166.6c57.9 0.3 112.3 22.9 153.2 63.9 41 41 63.7 95.5 63.9 153.5h54.5c-0.3-149.9-121.7-271.4-271.6-271.9V166.6L128.2 166.6z"/></svg><!--[if lt IE 9]><em>RSS</em><![endif]--></a></li>
  

  

  

  

  

    
</ul>





<p><span style="color:blue">内容每小时更新一次.</span> Powered by <a href="https://github.com/AWEEKJ/kiko-now">Kiko Now</a> & <a href="https://github.com/gitalk/gitalk">Gitalk</a> & <a href="https://github.com/duty-machine/news">duty-machine</a>, 站务 <a href="https://be4.herokuapp.com">NodeBE4</a>（<span style="color:red">被墙</span>）</p>





        </footer>
      </div>
    </div>

    



  </body>
</html>
