<!DOCTYPE html>
<html>
  <head>
  <title>何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二] – 觀點 – 從草根到大師 git.io/JJCxS</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="
  
    
    
    
   
     
           

      
       
 本文作者何光沪教授 
      
  
    
    
    
   
    从去年年底到今年年初，我在关于人工智能问题的一些演讲和文章中提出了一个呼吁：“学会自我规限，预防灭顶之灾”！  
    
    
    
   
    那是从宗教角度对这个问题所作的一点理论思考——从上帝赋予人以自己的形象或性质，思考人类赋予机器以自己的形象或性质；人类特有的这些性质，第一是自由，第二是心智，第三是创造性或爱心；人类创造机器人，即人工智能，就是把自己的部分形象或性质，比如，把“智能”即理性思考的能力，赋予了机器。  
    
    
    
   
    从宗教角度出发，会发现人类在这件事情上，既滥用了天赋的创造性，又受制于天生的局限性。一方面，人想要创造出具备智能甚至超级智能的机器，有几何级数增长的学习能力，有威胁人类生存的繁殖能力的机器。另一方面，人没有能力赋予机器或者人工智能以良心，同时又没有能力防止它们获得自由，即自行抉择的能力。于是人工智能就可能有智而无心；很多专家认为，超级人工智能会获得自行决策的能力，即哲学上说的自由。机器人有自由而没良心，那会极其恐怖。  
    
    
    
   
    另外，人工智能的创造要依靠物质、需要能源和原材料，这可能导致与人类争夺资源，导致冲突，因此会有极大的风险。  
    
    
    
   
    前面谈的主要是一般性的问题。现在我想提出一个比较具体的呼吁：“建立国际监管，预防人类浩劫”！  
    
    
    
   
    这是从现实角度对这个问题进一步思考，可以分成三点。  
    
    
    
   
    
    一．
    
    
   
    
    
    
   
    现代科学技术的发展及其社会结果，有很多例证都说明，对之进行监管十分必要。或者反过来说，现代科学技术发展证明，听之任之是危险的，毫无规限地任其发展，对其应用缺乏适当的监督、管理和指导，会带来极大的危害。  
    
    
    
   
    例如，避孕技术的发展，对传统性道德造成了很大的破坏；人工受精和代孕技术的发展，对亲子概念和亲子关系造成了冲击；堕胎手术的便利，已经杀死了很多婴儿；胎儿性别鉴定的技术，如果不加以监管，在很多传统文化的环境当中，比如在中国和印度传统文化的环境当中，会造成人口性别的严重失衡，以及婚姻方面巨大的社会难题。  
    
    
    
   
    又例如，如果对生物克隆技术不加以监管，在那些仅仅追求利润或名声，或是仅仅被求知或好奇所驱动的海量的研究人员或研究机构那里，难免会埋下危险的种子——有朝一日造出克隆人或半人半兽怪物，从而威胁人类社会的伦理、法律等等基本秩序。  
    
    
    
   
    还有许多例证，例如对制冷技术对地球臭氧层的破坏，热能发展和碳排放对大气层和气候的破坏等等，更是总所周知。  
    
    
    
   
    由于绝大多数科学家和技术人员很难超越自己的专业领域，去思考这些涉及到社会伦理、法律权利，甚至历史发展、文化文明、人性变化的问题，所以，伦理学家、社会学家、法学家、哲学家、宗教学家等社会科学、人文学科的专业人士，就应当而且必须对这些表面上只属于自然科学和技术科学的问题，提出意见、建言献策，甚至提出监督管理的建议。进一步说，国家的立法机构更应当考虑和讨论这类建议，并采取及时而必要的立法行动，制定措施，督促执法机构加以执行。  
    
    
    
   
    因为这些问题，关系到社会中每一个人的生活，甚至关系到全人类的命运。  
    
    
    
   
    
    二．
    
    
   
    
    
    
   
    事实上，现在我们需要的，不仅是国内的监管，而且是国际的监管。  
    
    
    
   
    由于科学的发展进步没有国界，技术的应用结果没有国界，知识和信息的传播也是跨国进行；又由于交通和通讯技术的飞跃，使得现代社会进入了国际化和全球化的时代，因此，科技发展的影响，不论是好是坏，绝不会局限于一个国家或一些国家。在这种情况下，对影响重大的科技发展，就必须进行国际监管。  
    
    
    
   
    人工智能现在正在开始发挥越来越重大的影响，而且即将对全人类的生活产生全面的、深远的、颠覆性的影响，在这个领域进行国际监管，应该立即提上国际合作的日程。  
    
    
    
   
    原子能方面的科学技术及其应用，是一个重要的例证。首先，核武器的使用会造成巨大规模的人员伤亡，会使得更大规模的人群罹患绝症，非常痛苦，生不如死，甚至会造成人类文明的灭亡，以及一切生物的灭绝和地球生态的毁灭。其次，核能的和平利用，尤其是核电的使用，也未彻底解决核废料长达  10  亿年之久的放射性危害的问题，而且未能完全杜绝核泄漏的发生（切尔诺贝利和福岛都是例证），而泄露的污染和危害范围，也是超越国界的。这里还未提到一些国家掩盖了针对别国的、为着战争目的的核能发展。  
    
    
    
   
    因此种种，从爱因斯坦这类科学家和广岛投弹美军飞行员的反思，到国际原子能委员会的成立及其国际检查活动，反映了人类对这类科技成果负面作用的共识。这种亡羊补牢式的思考当然很重要，但是有些事情，如果亡羊再来补牢，则为时晚矣！  
    
    
    
   
    因为超级人工智能极可能来得很快，并以几何级数的速度迅猛发展，所以，对人工智能研发的国际监管，事实上越早越好！  
    
    
    
   
    
    三．
    
    
   
    
    
    
   
    斯蒂芬•霍金（  S. Hawking  ）指出，人工智能的出现  ,  在人类历史上的意义不亚于核武器。这已经向我们指出了事情的严重程度。他又说，人工智能的出现，是人类历史上最好的事情，也可能是最坏的事情。我们当然应该争取最好的事情、避免最坏的事情。不仅如此，正如卡鲁姆•蔡斯（  K. Chace  ）在《人工智能革命》中所说，对付人工智能的巨大挑战，这场战争不能输，因为获胜的奖品是绚烂的未来——坐享一切，长生不老；失败的惩罚是灭顶之灾——人类灭绝，甚至更糟。  
    
    
    
   
    既然是灭顶之灾，那么即使只有万分之一的可能，也应该作出一万分的努力去避免。实际上，在当今的国际局势下，人工智能的研发，分散在不同的国家、不同的机构，包括国家机构、军队机构，以及数不清的跨国公司、私人机构甚至“车库”当中，这就大大增加了危险发生的可能性。总所周知，军事机构研发的，可能首先是破坏性的或杀人的机器人，所以负面作用或危险发生的可能性，也因此大大增加。  
    
    
    
   
    在一些人工智能研发能力很强的国家当中，民族主义的高涨当然也会大大地增加危险。例如在全世界人工智能研发能力最强的中国和美国，如果让民族主义和民粹主义占据主导地位，让“零和思维方式”、“高地思维方式”（“占领高地”是军事用语）占据主导地位，人类的命运就岌岌可危了。在这种情况下，光讲“自我规限”是远远不够的。“自律”的因素必须加上“他律”和“共治”的因素，加上“法治”和“强制”的因素。或许，要比国际原子能机构的监管更强、更严才行。或许，应该由联合国安理会这样的机构，来对各国和各机构的人工智能研发进行适当的监督、管理和指导。  
    
    
    
   
    在此我谨建议，这项工作应该从下述三件事情开始：  
    
    
    
   
    要研讨并且确认人工智能现在已经和未来将会对全人类 （不止是对一个人群、一个国家）产生的益处和害处、福利和危险。  
    
    
    
   
    要研讨如何整合全世界的研发资源  ,  共同防止“强人工智能”获得自我决策能力或脱离人类控制的能力（包括防止作战机器人滥杀平民）。这需要在科学上和技术上（不仅在哲学上）定义“自主决策”。  
    
    
    
   
    要研讨制定规程，让各国、各机构定期报告和专项报告研究项目、研究计划和研究进展，接受检查和核查，并且执行国际监管机构的决议。  
    
    
    
   
    谢谢！  
    
    
    
   
     2018  年  7  月于北京  
     
     
     [  
    何光沪
    著名宗教哲学家，曾任中国人民大学宗教学系教授、博士生导师，中国社会科学院世界宗教研究所研究员、学术委员会委员。本文为作者  2018  年  7  月  11  日在「人工智能与道德风险」云豹沙龙的演讲，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
    
    
    
   
    
    注释：
    
    
   
    
    
    
   
    现在霍金（  S. Hawking  ）、盖茨（  B. Gates  ）还有马斯克（  E. Mask  ），都联名呼吁要立法限制人工智能研发。  
    
    
    
   
  

" />
    <meta property="og:description" content="
  
    
    
    
   
     
           

      
       
 本文作者何光沪教授 
      
  
    
    
    
   
    从去年年底到今年年初，我在关于人工智能问题的一些演讲和文章中提出了一个呼吁：“学会自我规限，预防灭顶之灾”！  
    
    
    
   
    那是从宗教角度对这个问题所作的一点理论思考——从上帝赋予人以自己的形象或性质，思考人类赋予机器以自己的形象或性质；人类特有的这些性质，第一是自由，第二是心智，第三是创造性或爱心；人类创造机器人，即人工智能，就是把自己的部分形象或性质，比如，把“智能”即理性思考的能力，赋予了机器。  
    
    
    
   
    从宗教角度出发，会发现人类在这件事情上，既滥用了天赋的创造性，又受制于天生的局限性。一方面，人想要创造出具备智能甚至超级智能的机器，有几何级数增长的学习能力，有威胁人类生存的繁殖能力的机器。另一方面，人没有能力赋予机器或者人工智能以良心，同时又没有能力防止它们获得自由，即自行抉择的能力。于是人工智能就可能有智而无心；很多专家认为，超级人工智能会获得自行决策的能力，即哲学上说的自由。机器人有自由而没良心，那会极其恐怖。  
    
    
    
   
    另外，人工智能的创造要依靠物质、需要能源和原材料，这可能导致与人类争夺资源，导致冲突，因此会有极大的风险。  
    
    
    
   
    前面谈的主要是一般性的问题。现在我想提出一个比较具体的呼吁：“建立国际监管，预防人类浩劫”！  
    
    
    
   
    这是从现实角度对这个问题进一步思考，可以分成三点。  
    
    
    
   
    
    一．
    
    
   
    
    
    
   
    现代科学技术的发展及其社会结果，有很多例证都说明，对之进行监管十分必要。或者反过来说，现代科学技术发展证明，听之任之是危险的，毫无规限地任其发展，对其应用缺乏适当的监督、管理和指导，会带来极大的危害。  
    
    
    
   
    例如，避孕技术的发展，对传统性道德造成了很大的破坏；人工受精和代孕技术的发展，对亲子概念和亲子关系造成了冲击；堕胎手术的便利，已经杀死了很多婴儿；胎儿性别鉴定的技术，如果不加以监管，在很多传统文化的环境当中，比如在中国和印度传统文化的环境当中，会造成人口性别的严重失衡，以及婚姻方面巨大的社会难题。  
    
    
    
   
    又例如，如果对生物克隆技术不加以监管，在那些仅仅追求利润或名声，或是仅仅被求知或好奇所驱动的海量的研究人员或研究机构那里，难免会埋下危险的种子——有朝一日造出克隆人或半人半兽怪物，从而威胁人类社会的伦理、法律等等基本秩序。  
    
    
    
   
    还有许多例证，例如对制冷技术对地球臭氧层的破坏，热能发展和碳排放对大气层和气候的破坏等等，更是总所周知。  
    
    
    
   
    由于绝大多数科学家和技术人员很难超越自己的专业领域，去思考这些涉及到社会伦理、法律权利，甚至历史发展、文化文明、人性变化的问题，所以，伦理学家、社会学家、法学家、哲学家、宗教学家等社会科学、人文学科的专业人士，就应当而且必须对这些表面上只属于自然科学和技术科学的问题，提出意见、建言献策，甚至提出监督管理的建议。进一步说，国家的立法机构更应当考虑和讨论这类建议，并采取及时而必要的立法行动，制定措施，督促执法机构加以执行。  
    
    
    
   
    因为这些问题，关系到社会中每一个人的生活，甚至关系到全人类的命运。  
    
    
    
   
    
    二．
    
    
   
    
    
    
   
    事实上，现在我们需要的，不仅是国内的监管，而且是国际的监管。  
    
    
    
   
    由于科学的发展进步没有国界，技术的应用结果没有国界，知识和信息的传播也是跨国进行；又由于交通和通讯技术的飞跃，使得现代社会进入了国际化和全球化的时代，因此，科技发展的影响，不论是好是坏，绝不会局限于一个国家或一些国家。在这种情况下，对影响重大的科技发展，就必须进行国际监管。  
    
    
    
   
    人工智能现在正在开始发挥越来越重大的影响，而且即将对全人类的生活产生全面的、深远的、颠覆性的影响，在这个领域进行国际监管，应该立即提上国际合作的日程。  
    
    
    
   
    原子能方面的科学技术及其应用，是一个重要的例证。首先，核武器的使用会造成巨大规模的人员伤亡，会使得更大规模的人群罹患绝症，非常痛苦，生不如死，甚至会造成人类文明的灭亡，以及一切生物的灭绝和地球生态的毁灭。其次，核能的和平利用，尤其是核电的使用，也未彻底解决核废料长达  10  亿年之久的放射性危害的问题，而且未能完全杜绝核泄漏的发生（切尔诺贝利和福岛都是例证），而泄露的污染和危害范围，也是超越国界的。这里还未提到一些国家掩盖了针对别国的、为着战争目的的核能发展。  
    
    
    
   
    因此种种，从爱因斯坦这类科学家和广岛投弹美军飞行员的反思，到国际原子能委员会的成立及其国际检查活动，反映了人类对这类科技成果负面作用的共识。这种亡羊补牢式的思考当然很重要，但是有些事情，如果亡羊再来补牢，则为时晚矣！  
    
    
    
   
    因为超级人工智能极可能来得很快，并以几何级数的速度迅猛发展，所以，对人工智能研发的国际监管，事实上越早越好！  
    
    
    
   
    
    三．
    
    
   
    
    
    
   
    斯蒂芬•霍金（  S. Hawking  ）指出，人工智能的出现  ,  在人类历史上的意义不亚于核武器。这已经向我们指出了事情的严重程度。他又说，人工智能的出现，是人类历史上最好的事情，也可能是最坏的事情。我们当然应该争取最好的事情、避免最坏的事情。不仅如此，正如卡鲁姆•蔡斯（  K. Chace  ）在《人工智能革命》中所说，对付人工智能的巨大挑战，这场战争不能输，因为获胜的奖品是绚烂的未来——坐享一切，长生不老；失败的惩罚是灭顶之灾——人类灭绝，甚至更糟。  
    
    
    
   
    既然是灭顶之灾，那么即使只有万分之一的可能，也应该作出一万分的努力去避免。实际上，在当今的国际局势下，人工智能的研发，分散在不同的国家、不同的机构，包括国家机构、军队机构，以及数不清的跨国公司、私人机构甚至“车库”当中，这就大大增加了危险发生的可能性。总所周知，军事机构研发的，可能首先是破坏性的或杀人的机器人，所以负面作用或危险发生的可能性，也因此大大增加。  
    
    
    
   
    在一些人工智能研发能力很强的国家当中，民族主义的高涨当然也会大大地增加危险。例如在全世界人工智能研发能力最强的中国和美国，如果让民族主义和民粹主义占据主导地位，让“零和思维方式”、“高地思维方式”（“占领高地”是军事用语）占据主导地位，人类的命运就岌岌可危了。在这种情况下，光讲“自我规限”是远远不够的。“自律”的因素必须加上“他律”和“共治”的因素，加上“法治”和“强制”的因素。或许，要比国际原子能机构的监管更强、更严才行。或许，应该由联合国安理会这样的机构，来对各国和各机构的人工智能研发进行适当的监督、管理和指导。  
    
    
    
   
    在此我谨建议，这项工作应该从下述三件事情开始：  
    
    
    
   
    要研讨并且确认人工智能现在已经和未来将会对全人类 （不止是对一个人群、一个国家）产生的益处和害处、福利和危险。  
    
    
    
   
    要研讨如何整合全世界的研发资源  ,  共同防止“强人工智能”获得自我决策能力或脱离人类控制的能力（包括防止作战机器人滥杀平民）。这需要在科学上和技术上（不仅在哲学上）定义“自主决策”。  
    
    
    
   
    要研讨制定规程，让各国、各机构定期报告和专项报告研究项目、研究计划和研究进展，接受检查和核查，并且执行国际监管机构的决议。  
    
    
    
   
    谢谢！  
    
    
    
   
     2018  年  7  月于北京  
     
     
     [  
    何光沪
    著名宗教哲学家，曾任中国人民大学宗教学系教授、博士生导师，中国社会科学院世界宗教研究所研究员、学术委员会委员。本文为作者  2018  年  7  月  11  日在「人工智能与道德风险」云豹沙龙的演讲，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
    
    
    
   
    
    注释：
    
    
   
    
    
    
   
    现在霍金（  S. Hawking  ）、盖茨（  B. Gates  ）还有马斯克（  E. Mask  ），都联名呼吁要立法限制人工智能研发。  
    
    
    
   
  

" />
    
    <meta name="author" content="觀點" />

    
    <meta property="og:title" content="何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]" />
    <meta property="twitter:title" content="何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]" />
    

  <link rel="stylesheet" type="text/css" href="/opinion/style.css" />
  <link rel="alternate" type="application/rss+xml" title="觀點 - 從草根到大師 git.io/JJCxS" href="/opinion/feed.xml" />

  <!-- Social Share Kit CSS -->
  <link rel="stylesheet" href="/opinion/assets/css/social-share-kit.css" type="text/css">
  <link rel="stylesheet" href="/opinion/assets/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="/opinion/assets/css/bootstrap.min.css" type="text/css">
  <script type="text/javascript" src="/opinion/assets/js/jquery-3.5.1.js"></script>
  <script type="text/javascript" src="/opinion/assets/js/page.js"></script>

</head>

  <body>
    <div class="wrapper-masthead">
  <div class="container">
    <header class="masthead clearfix">
      

      <div class="site-info">
        <h1 class="site-name" style="display: inline-block;"><a href="/opinion/">觀點</a></h1>
        <i class="site-description" style="font-size: 12px;">從草根到大師 git.io/JJCxS</i>
      </div>

      <nav>
        <span id="search-container" >
          <a href="/opinion/tools"><i class="fa fa-bookmark twitter" title="百宝箱"></i></a>
        <a><i class="fa fa-search" title="限前100結果"></i></a><input type="text" id="search-input" placeholder="標題 作者 來源 日期 (17489)"
          style="margin: 10px 0px 0px 0px; height: 30px;width: auto" title="本站最正確的打開方式">
        </span>
        
        
        <a href="/opinion/categories" style="color: Tomato;"><i class="fa fa-tags" title="分类"></i></a>
        
        
        
        <a href="https://be4.herokuapp.com/" style="color: #003366;"><i class="fa fa-comments" title="论坛"></i></a>
        
        
        
        <a href="/opinion/about"><i class="fa fa-info-circle" title="关于"></i></a>
        
        
        <a title="电脑热键：&larr;上一篇(页), &rarr;下一篇(页), ins同来源新一篇，del同来源旧一篇" onclick="toggle_visibility('help')"><i class="fa fa-question-circle"></i></a>
        <a id="fa-home" href="https://nodebe4.github.io" title="BE4服务列表" onclick="//toggle_visibility('site-list')"><i class="fa fa-home" aria-hidden="true"></i></a>
      </nav>

    </header>
    <div id="site-list" class="tags" style="display: block;text-align: right;border-bottom: 1px solid lightGray;"><noscript><span style="background-color: #e8e8e8;color: #d10000;font-size: 14px;">开启浏览器JavaScript以获取搜索功能和更好的浏览体验</span></noscript></div>
    <p id="help" style="font-size: 14px;display: none;text-align: right;"><span style="color:green;">电脑热键：&larr;上一篇(页), &rarr;下一篇(页), ins同来源新一篇, del同来源旧一篇</span>; <span style="color:orange">对应触屏FAB：上下右左</span>; 轉Markdown<a href="https://euangoddard.github.io/clipboard2markdown/"><i class="fa fa-file-text-o"></i></a></p>
  </div>
</div>

<script type="text/javascript" >
  function toggle_visibility(id){
    var help = document.getElementById(id)
    if (help.style.display=='none'){
      help.style.display='block';
    }else{
      help.style.display='none';
    }
  }

  const url = "https://nodebe4.github.io/sitelist.json"

  document.addEventListener("DOMContentLoaded", function(event){
    // var homebtn = document.getElementById("fa-home")
    // homebtn.removeAttribute("href")
    var content = document.getElementById("site-list");
    content.innerHTML = ''
    var ul = document.createElement("ul")
    ul.classList.add("label")
    content.appendChild(ul)
    var cnt = 0

    $.getJSON(url, function(allsites) {

      allsites.map(item =>{
        var li = document.createElement('li')
        li.classList.add("tag")
        li.id = 'site-' + cnt
        ul.appendChild(li)
        var a0 = document.createElement('a')
        li.appendChild(a0)
        a0.href = item.url[0]
        var span = document.createElement('span')
        a0.appendChild(span)
        span.innerText = item['name']
        // span.style.backgroundColor = item['background-color']
        // span.style.color='#E4CBC3'
        span.style.color = item['background-color']
        span.style['font-size'] = '14px'
        cnt += 1
        // test_alive(li.id, a0.href)
      })
    })
  })

function test_alive(id, url){
  var divstatus = document.getElementById(id)
  const base = 'https://textance.herokuapp.com/title/'
  var fullurl = base + url
  $.ajax({
      url: fullurl,
      complete: function(data) {
        if (data.responseText.includes('502')){
          // divstatus.style.color='#FBB7B7'
          // divstatus.style.color='gray'
          // divstatus.title = "服务器无响应"
          divstatus.parentNode.removeChild(divstatus)
        }else{
          // divstatus.style.color='#B6FAC8'
          divstatus.title = data.responseText
        }
      }
  });
  return divstatus
}
</script>



    <!-- Left & centered positioning -->

<div class="ssk-sticky ssk-right ssk-center ssk-sticky-hide-xs ssk-group ssk-round">
  
    <a href="https://be4news.pythonanywhere.com/archivenow/ia/http%3A%2F%2Funirule.cloud%2Findex.php%3Fc%3Darticle%26id%3D4663" class="ssk ssk-link" title="存到互联网档案馆" target="_blank"></a>
    <a href="https://www.facebook.com/sharer.php?u=http://unirule.cloud/index.php?c=article&id=4663" class="ssk ssk-facebook"></a>
    <a href="https://twitter.com/intent/tweet?url=http://unirule.cloud/index.php?c=article&id=4663&text=何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]&hashtags=觀點" class="ssk ssk-twitter"></a>
    <a href="https://reddit.com/submit?url=http://unirule.cloud/index.php?c=article&id=4663&title=何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]" class="ssk ssk-reddit"></a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://unirule.cloud/index.php?c=article&id=4663&title=何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]" class="ssk ssk-linkedin"></a>
    <a href="mailto:{email_address}?subject=何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]&body=
  
    
    
    
   
     
           

      
       
 本文作者何光沪教授 
      
  
    
    
    
   
    从去年年底到今年年初，我在关于人工智能问题的一些演讲和文章中提出了一个呼吁：“学会自我规限，预防灭顶之灾”！  
    
    
    
   
    那是从宗教角度对这个问题所作的一点理论思考——从上帝赋予人以自己的形象或性质，思考人类赋予机器以自己的形象或性质；人类特有的这些性质，第一是自由，第二是心智，第三是创造性或爱心；人类创造机器人，即人工智能，就是把自己的部分形象或性质，比如，把“智能”即理性思考的能力，赋予了机器。  
    
    
    
   
    从宗教角度出发，会发现人类在这件事情上，既滥用了天赋的创造性，又受制于天生的局限性。一方面，人想要创造出具备智能甚至超级智能的机器，有几何级数增长的学习能力，有威胁人类生存的繁殖能力的机器。另一方面，人没有能力赋予机器或者人工智能以良心，同时又没有能力防止它们获得自由，即自行抉择的能力。于是人工智能就可能有智而无心；很多专家认为，超级人工智能会获得自行决策的能力，即哲学上说的自由。机器人有自由而没良心，那会极其恐怖。  
    
    
    
   
    另外，人工智能的创造要依靠物质、需要能源和原材料，这可能导致与人类争夺资源，导致冲突，因此会有极大的风险。  
    
    
    
   
    前面谈的主要是一般性的问题。现在我想提出一个比较具体的呼吁：“建立国际监管，预防人类浩劫”！  
    
    
    
   
    这是从现实角度对这个问题进一步思考，可以分成三点。  
    
    
    
   
    
    一．
    
    
   
    
    
    
   
    现代科学技术的发展及其社会结果，有很多例证都说明，对之进行监管十分必要。或者反过来说，现代科学技术发展证明，听之任之是危险的，毫无规限地任其发展，对其应用缺乏适当的监督、管理和指导，会带来极大的危害。  
    
    
    
   
    例如，避孕技术的发展，对传统性道德造成了很大的破坏；人工受精和代孕技术的发展，对亲子概念和亲子关系造成了冲击；堕胎手术的便利，已经杀死了很多婴儿；胎儿性别鉴定的技术，如果不加以监管，在很多传统文化的环境当中，比如在中国和印度传统文化的环境当中，会造成人口性别的严重失衡，以及婚姻方面巨大的社会难题。  
    
    
    
   
    又例如，如果对生物克隆技术不加以监管，在那些仅仅追求利润或名声，或是仅仅被求知或好奇所驱动的海量的研究人员或研究机构那里，难免会埋下危险的种子——有朝一日造出克隆人或半人半兽怪物，从而威胁人类社会的伦理、法律等等基本秩序。  
    
    
    
   
    还有许多例证，例如对制冷技术对地球臭氧层的破坏，热能发展和碳排放对大气层和气候的破坏等等，更是总所周知。  
    
    
    
   
    由于绝大多数科学家和技术人员很难超越自己的专业领域，去思考这些涉及到社会伦理、法律权利，甚至历史发展、文化文明、人性变化的问题，所以，伦理学家、社会学家、法学家、哲学家、宗教学家等社会科学、人文学科的专业人士，就应当而且必须对这些表面上只属于自然科学和技术科学的问题，提出意见、建言献策，甚至提出监督管理的建议。进一步说，国家的立法机构更应当考虑和讨论这类建议，并采取及时而必要的立法行动，制定措施，督促执法机构加以执行。  
    
    
    
   
    因为这些问题，关系到社会中每一个人的生活，甚至关系到全人类的命运。  
    
    
    
   
    
    二．
    
    
   
    
    
    
   
    事实上，现在我们需要的，不仅是国内的监管，而且是国际的监管。  
    
    
    
   
    由于科学的发展进步没有国界，技术的应用结果没有国界，知识和信息的传播也是跨国进行；又由于交通和通讯技术的飞跃，使得现代社会进入了国际化和全球化的时代，因此，科技发展的影响，不论是好是坏，绝不会局限于一个国家或一些国家。在这种情况下，对影响重大的科技发展，就必须进行国际监管。  
    
    
    
   
    人工智能现在正在开始发挥越来越重大的影响，而且即将对全人类的生活产生全面的、深远的、颠覆性的影响，在这个领域进行国际监管，应该立即提上国际合作的日程。  
    
    
    
   
    原子能方面的科学技术及其应用，是一个重要的例证。首先，核武器的使用会造成巨大规模的人员伤亡，会使得更大规模的人群罹患绝症，非常痛苦，生不如死，甚至会造成人类文明的灭亡，以及一切生物的灭绝和地球生态的毁灭。其次，核能的和平利用，尤其是核电的使用，也未彻底解决核废料长达  10  亿年之久的放射性危害的问题，而且未能完全杜绝核泄漏的发生（切尔诺贝利和福岛都是例证），而泄露的污染和危害范围，也是超越国界的。这里还未提到一些国家掩盖了针对别国的、为着战争目的的核能发展。  
    
    
    
   
    因此种种，从爱因斯坦这类科学家和广岛投弹美军飞行员的反思，到国际原子能委员会的成立及其国际检查活动，反映了人类对这类科技成果负面作用的共识。这种亡羊补牢式的思考当然很重要，但是有些事情，如果亡羊再来补牢，则为时晚矣！  
    
    
    
   
    因为超级人工智能极可能来得很快，并以几何级数的速度迅猛发展，所以，对人工智能研发的国际监管，事实上越早越好！  
    
    
    
   
    
    三．
    
    
   
    
    
    
   
    斯蒂芬•霍金（  S. Hawking  ）指出，人工智能的出现  ,  在人类历史上的意义不亚于核武器。这已经向我们指出了事情的严重程度。他又说，人工智能的出现，是人类历史上最好的事情，也可能是最坏的事情。我们当然应该争取最好的事情、避免最坏的事情。不仅如此，正如卡鲁姆•蔡斯（  K. Chace  ）在《人工智能革命》中所说，对付人工智能的巨大挑战，这场战争不能输，因为获胜的奖品是绚烂的未来——坐享一切，长生不老；失败的惩罚是灭顶之灾——人类灭绝，甚至更糟。  
    
    
    
   
    既然是灭顶之灾，那么即使只有万分之一的可能，也应该作出一万分的努力去避免。实际上，在当今的国际局势下，人工智能的研发，分散在不同的国家、不同的机构，包括国家机构、军队机构，以及数不清的跨国公司、私人机构甚至“车库”当中，这就大大增加了危险发生的可能性。总所周知，军事机构研发的，可能首先是破坏性的或杀人的机器人，所以负面作用或危险发生的可能性，也因此大大增加。  
    
    
    
   
    在一些人工智能研发能力很强的国家当中，民族主义的高涨当然也会大大地增加危险。例如在全世界人工智能研发能力最强的中国和美国，如果让民族主义和民粹主义占据主导地位，让“零和思维方式”、“高地思维方式”（“占领高地”是军事用语）占据主导地位，人类的命运就岌岌可危了。在这种情况下，光讲“自我规限”是远远不够的。“自律”的因素必须加上“他律”和“共治”的因素，加上“法治”和“强制”的因素。或许，要比国际原子能机构的监管更强、更严才行。或许，应该由联合国安理会这样的机构，来对各国和各机构的人工智能研发进行适当的监督、管理和指导。  
    
    
    
   
    在此我谨建议，这项工作应该从下述三件事情开始：  
    
    
    
   
    要研讨并且确认人工智能现在已经和未来将会对全人类 （不止是对一个人群、一个国家）产生的益处和害处、福利和危险。  
    
    
    
   
    要研讨如何整合全世界的研发资源  ,  共同防止“强人工智能”获得自我决策能力或脱离人类控制的能力（包括防止作战机器人滥杀平民）。这需要在科学上和技术上（不仅在哲学上）定义“自主决策”。  
    
    
    
   
    要研讨制定规程，让各国、各机构定期报告和专项报告研究项目、研究计划和研究进展，接受检查和核查，并且执行国际监管机构的决议。  
    
    
    
   
    谢谢！  
    
    
    
   
     2018  年  7  月于北京  
     
     
     [  
    何光沪
    著名宗教哲学家，曾任中国人民大学宗教学系教授、博士生导师，中国社会科学院世界宗教研究所研究员、学术委员会委员。本文为作者  2018  年  7  月  11  日在「人工智能与道德风险」云豹沙龙的演讲，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
    
    
    
   
    
    注释：
    
    
   
    
    
    
   
    现在霍金（  S. Hawking  ）、盖茨（  B. Gates  ）还有马斯克（  E. Mask  ），都联名呼吁要立法限制人工智能研发。  
    
    
    
   
  

" class="ssk ssk-email"></a>
    <a href="http://pinterest.com/pin/create/link/?url=http://unirule.cloud/index.php?c=article&id=4663" class="ssk ssk-pinterest"></a>
    <a href="https://www.tumblr.com/widgets/share/tool?canonicalUrl=http://unirule.cloud/index.php?c=article&id=4663&title=何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]&caption=
  
    
    
    
   
     
           

      
       
 本文作者何光沪教授 
      
  
    
    
    
   
    从去年年底到今年年初，我在关于人工智能问题的一些演讲和文章中提出了一个呼吁：“学会自我规限，预防灭顶之灾”！  
    
    
    
   
    那是从宗教角度对这个问题所作的一点理论思考——从上帝赋予人以自己的形象或性质，思考人类赋予机器以自己的形象或性质；人类特有的这些性质，第一是自由，第二是心智，第三是创造性或爱心；人类创造机器人，即人工智能，就是把自己的部分形象或性质，比如，把“智能”即理性思考的能力，赋予了机器。  
    
    
    
   
    从宗教角度出发，会发现人类在这件事情上，既滥用了天赋的创造性，又受制于天生的局限性。一方面，人想要创造出具备智能甚至超级智能的机器，有几何级数增长的学习能力，有威胁人类生存的繁殖能力的机器。另一方面，人没有能力赋予机器或者人工智能以良心，同时又没有能力防止它们获得自由，即自行抉择的能力。于是人工智能就可能有智而无心；很多专家认为，超级人工智能会获得自行决策的能力，即哲学上说的自由。机器人有自由而没良心，那会极其恐怖。  
    
    
    
   
    另外，人工智能的创造要依靠物质、需要能源和原材料，这可能导致与人类争夺资源，导致冲突，因此会有极大的风险。  
    
    
    
   
    前面谈的主要是一般性的问题。现在我想提出一个比较具体的呼吁：“建立国际监管，预防人类浩劫”！  
    
    
    
   
    这是从现实角度对这个问题进一步思考，可以分成三点。  
    
    
    
   
    
    一．
    
    
   
    
    
    
   
    现代科学技术的发展及其社会结果，有很多例证都说明，对之进行监管十分必要。或者反过来说，现代科学技术发展证明，听之任之是危险的，毫无规限地任其发展，对其应用缺乏适当的监督、管理和指导，会带来极大的危害。  
    
    
    
   
    例如，避孕技术的发展，对传统性道德造成了很大的破坏；人工受精和代孕技术的发展，对亲子概念和亲子关系造成了冲击；堕胎手术的便利，已经杀死了很多婴儿；胎儿性别鉴定的技术，如果不加以监管，在很多传统文化的环境当中，比如在中国和印度传统文化的环境当中，会造成人口性别的严重失衡，以及婚姻方面巨大的社会难题。  
    
    
    
   
    又例如，如果对生物克隆技术不加以监管，在那些仅仅追求利润或名声，或是仅仅被求知或好奇所驱动的海量的研究人员或研究机构那里，难免会埋下危险的种子——有朝一日造出克隆人或半人半兽怪物，从而威胁人类社会的伦理、法律等等基本秩序。  
    
    
    
   
    还有许多例证，例如对制冷技术对地球臭氧层的破坏，热能发展和碳排放对大气层和气候的破坏等等，更是总所周知。  
    
    
    
   
    由于绝大多数科学家和技术人员很难超越自己的专业领域，去思考这些涉及到社会伦理、法律权利，甚至历史发展、文化文明、人性变化的问题，所以，伦理学家、社会学家、法学家、哲学家、宗教学家等社会科学、人文学科的专业人士，就应当而且必须对这些表面上只属于自然科学和技术科学的问题，提出意见、建言献策，甚至提出监督管理的建议。进一步说，国家的立法机构更应当考虑和讨论这类建议，并采取及时而必要的立法行动，制定措施，督促执法机构加以执行。  
    
    
    
   
    因为这些问题，关系到社会中每一个人的生活，甚至关系到全人类的命运。  
    
    
    
   
    
    二．
    
    
   
    
    
    
   
    事实上，现在我们需要的，不仅是国内的监管，而且是国际的监管。  
    
    
    
   
    由于科学的发展进步没有国界，技术的应用结果没有国界，知识和信息的传播也是跨国进行；又由于交通和通讯技术的飞跃，使得现代社会进入了国际化和全球化的时代，因此，科技发展的影响，不论是好是坏，绝不会局限于一个国家或一些国家。在这种情况下，对影响重大的科技发展，就必须进行国际监管。  
    
    
    
   
    人工智能现在正在开始发挥越来越重大的影响，而且即将对全人类的生活产生全面的、深远的、颠覆性的影响，在这个领域进行国际监管，应该立即提上国际合作的日程。  
    
    
    
   
    原子能方面的科学技术及其应用，是一个重要的例证。首先，核武器的使用会造成巨大规模的人员伤亡，会使得更大规模的人群罹患绝症，非常痛苦，生不如死，甚至会造成人类文明的灭亡，以及一切生物的灭绝和地球生态的毁灭。其次，核能的和平利用，尤其是核电的使用，也未彻底解决核废料长达  10  亿年之久的放射性危害的问题，而且未能完全杜绝核泄漏的发生（切尔诺贝利和福岛都是例证），而泄露的污染和危害范围，也是超越国界的。这里还未提到一些国家掩盖了针对别国的、为着战争目的的核能发展。  
    
    
    
   
    因此种种，从爱因斯坦这类科学家和广岛投弹美军飞行员的反思，到国际原子能委员会的成立及其国际检查活动，反映了人类对这类科技成果负面作用的共识。这种亡羊补牢式的思考当然很重要，但是有些事情，如果亡羊再来补牢，则为时晚矣！  
    
    
    
   
    因为超级人工智能极可能来得很快，并以几何级数的速度迅猛发展，所以，对人工智能研发的国际监管，事实上越早越好！  
    
    
    
   
    
    三．
    
    
   
    
    
    
   
    斯蒂芬•霍金（  S. Hawking  ）指出，人工智能的出现  ,  在人类历史上的意义不亚于核武器。这已经向我们指出了事情的严重程度。他又说，人工智能的出现，是人类历史上最好的事情，也可能是最坏的事情。我们当然应该争取最好的事情、避免最坏的事情。不仅如此，正如卡鲁姆•蔡斯（  K. Chace  ）在《人工智能革命》中所说，对付人工智能的巨大挑战，这场战争不能输，因为获胜的奖品是绚烂的未来——坐享一切，长生不老；失败的惩罚是灭顶之灾——人类灭绝，甚至更糟。  
    
    
    
   
    既然是灭顶之灾，那么即使只有万分之一的可能，也应该作出一万分的努力去避免。实际上，在当今的国际局势下，人工智能的研发，分散在不同的国家、不同的机构，包括国家机构、军队机构，以及数不清的跨国公司、私人机构甚至“车库”当中，这就大大增加了危险发生的可能性。总所周知，军事机构研发的，可能首先是破坏性的或杀人的机器人，所以负面作用或危险发生的可能性，也因此大大增加。  
    
    
    
   
    在一些人工智能研发能力很强的国家当中，民族主义的高涨当然也会大大地增加危险。例如在全世界人工智能研发能力最强的中国和美国，如果让民族主义和民粹主义占据主导地位，让“零和思维方式”、“高地思维方式”（“占领高地”是军事用语）占据主导地位，人类的命运就岌岌可危了。在这种情况下，光讲“自我规限”是远远不够的。“自律”的因素必须加上“他律”和“共治”的因素，加上“法治”和“强制”的因素。或许，要比国际原子能机构的监管更强、更严才行。或许，应该由联合国安理会这样的机构，来对各国和各机构的人工智能研发进行适当的监督、管理和指导。  
    
    
    
   
    在此我谨建议，这项工作应该从下述三件事情开始：  
    
    
    
   
    要研讨并且确认人工智能现在已经和未来将会对全人类 （不止是对一个人群、一个国家）产生的益处和害处、福利和危险。  
    
    
    
   
    要研讨如何整合全世界的研发资源  ,  共同防止“强人工智能”获得自我决策能力或脱离人类控制的能力（包括防止作战机器人滥杀平民）。这需要在科学上和技术上（不仅在哲学上）定义“自主决策”。  
    
    
    
   
    要研讨制定规程，让各国、各机构定期报告和专项报告研究项目、研究计划和研究进展，接受检查和核查，并且执行国际监管机构的决议。  
    
    
    
   
    谢谢！  
    
    
    
   
     2018  年  7  月于北京  
     
     
     [  
    何光沪
    著名宗教哲学家，曾任中国人民大学宗教学系教授、博士生导师，中国社会科学院世界宗教研究所研究员、学术委员会委员。本文为作者  2018  年  7  月  11  日在「人工智能与道德风险」云豹沙龙的演讲，图片与说明为中评周刊编辑所加，转载请注明  ] 
     
    
    
    
   
    
    注释：
    
    
   
    
    
    
   
    现在霍金（  S. Hawking  ）、盖茨（  B. Gates  ）还有马斯克（  E. Mask  ），都联名呼吁要立法限制人工智能研发。  
    
    
    
   
  

&tags=觀點" class="ssk ssk-tumblr"></a>
    <a href="https://buffer.com/add?text=何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]&url=http://unirule.cloud/index.php?c=article&id=4663" class="ssk ssk-buffer"></a>
</div>


    <div id="main" role="main" class="container">
      
  <!-- Html Elements for Search -->
  <ul id="results-container" class="searched" style="color: #2980B9;"></ul>

  <script src="/opinion/assets/js/simple-jekyll-search.min.js"></script>

  <!-- Configuration -->
  <script>
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/opinion/search.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a><time>{date}</time><a class="tag">{category}</a></li>',
    noResultsText: '没找到',
    limit: 100,
    fuzzy: false,
    exclude: ['Welcome']
  })

  </script>

      







  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
    


  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
    



<article class="post">
  <h1>何光沪：建立国际监管，预防人类浩劫 [人工智能问题系列思考之二]</h1>
  <!-- Look the author details up from the site config. -->
  

  <div>
    <span class="date">
      2018-08-11
    </span>

    <!-- Output author details if some exist. -->
    
      
        <span>
            <!-- Personal Info. -->
            <a  style="font-size:14px;">作者: 何光沪</a>
        </span>
      
    


    <ul class="tag">
      <li>
        <a href="https://nodebe4.github.io/opinion/categories/#天则观点">
          天则观点
        </a>
      </li>
    </ul>

    
        <span>
            <!-- Personal Info. -->
            <a href="http://unirule.cloud/index.php?c=article&id=4663" style="font-size:14px;">原文</a>
        </span>
    

    <span style="float: right;" title="天则观点的其它文章">
      <a style="font-size: 14px;" rel="nofollow" href="#sametag" class="tags">#天则观点 的其它文章</a>
    </span>

  </div>

  <div class="entry">
    
    
    
    <div class="article">
  <div class="body-text">
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:9.95pt;"><br />       <div style="text-align:center;">
<img alt="" src="/uploads/2018/08/111050345028.jpg" style="text-indent:9.95pt;" />
      </div>
       <div style="text-align:center;">
<span style="text-indent:9.95pt;"> 本文作者何光沪教授 </span>
      </div>
 <span> </span></p>
    <p class="MsoNormal" style="text-align:center;text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">从去年年底到今年年初，我在关于人工智能问题的一些演讲和文章中提出了一个呼吁：“学会自我规限，预防灭顶之灾”！ <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">那是从宗教角度对这个问题所作的一点理论思考——从上帝赋予人以自己的形象或性质，思考人类赋予机器以自己的形象或性质；人类特有的这些性质，第一是自由，第二是心智，第三是创造性或爱心；人类创造机器人，即人工智能，就是把自己的部分形象或性质，比如，把“智能”即理性思考的能力，赋予了机器。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">从宗教角度出发，会发现人类在这件事情上，既滥用了天赋的创造性，又受制于天生的局限性。一方面，人想要创造出具备智能甚至超级智能的机器，有几何级数增长的学习能力，有威胁人类生存的繁殖能力的机器。另一方面，人没有能力赋予机器或者人工智能以良心，同时又没有能力防止它们获得自由，即自行抉择的能力。于是人工智能就可能有智而无心；很多专家认为，超级人工智能会获得自行决策的能力，即哲学上说的自由。机器人有自由而没良心，那会极其恐怖。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">另外，人工智能的创造要依靠物质、需要能源和原材料，这可能导致与人类争夺资源，导致冲突，因此会有极大的风险。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">前面谈的主要是一般性的问题。现在我想提出一个比较具体的呼吁：“建立国际监管，预防人类浩劫”！ <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">这是从现实角度对这个问题进一步思考，可以分成三点。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    一．
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">现代科学技术的发展及其社会结果，有很多例证都说明，对之进行监管十分必要。或者反过来说，现代科学技术发展证明，听之任之是危险的，毫无规限地任其发展，对其应用缺乏适当的监督、管理和指导，会带来极大的危害。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">例如，避孕技术的发展，对传统性道德造成了很大的破坏；人工受精和代孕技术的发展，对亲子概念和亲子关系造成了冲击；堕胎手术的便利，已经杀死了很多婴儿；胎儿性别鉴定的技术，如果不加以监管，在很多传统文化的环境当中，比如在中国和印度传统文化的环境当中，会造成人口性别的严重失衡，以及婚姻方面巨大的社会难题。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">又例如，如果对生物克隆技术不加以监管，在那些仅仅追求利润或名声，或是仅仅被求知或好奇所驱动的海量的研究人员或研究机构那里，难免会埋下危险的种子——有朝一日造出克隆人或半人半兽怪物，从而威胁人类社会的伦理、法律等等基本秩序。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">还有许多例证，例如对制冷技术对地球臭氧层的破坏，热能发展和碳排放对大气层和气候的破坏等等，更是总所周知。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">由于绝大多数科学家和技术人员很难超越自己的专业领域，去思考这些涉及到社会伦理、法律权利，甚至历史发展、文化文明、人性变化的问题，所以，伦理学家、社会学家、法学家、哲学家、宗教学家等社会科学、人文学科的专业人士，就应当而且必须对这些表面上只属于自然科学和技术科学的问题，提出意见、建言献策，甚至提出监督管理的建议。进一步说，国家的立法机构更应当考虑和讨论这类建议，并采取及时而必要的立法行动，制定措施，督促执法机构加以执行。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">因为这些问题，关系到社会中每一个人的生活，甚至关系到全人类的命运。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    二．
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">事实上，现在我们需要的，不仅是国内的监管，而且是国际的监管。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">由于科学的发展进步没有国界，技术的应用结果没有国界，知识和信息的传播也是跨国进行；又由于交通和通讯技术的飞跃，使得现代社会进入了国际化和全球化的时代，因此，科技发展的影响，不论是好是坏，绝不会局限于一个国家或一些国家。在这种情况下，对影响重大的科技发展，就必须进行国际监管。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">人工智能现在正在开始发挥越来越重大的影响，而且即将对全人类的生活产生全面的、深远的、颠覆性的影响，在这个领域进行国际监管，应该立即提上国际合作的日程。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">原子能方面的科学技术及其应用，是一个重要的例证。首先，核武器的使用会造成巨大规模的人员伤亡，会使得更大规模的人群罹患绝症，非常痛苦，生不如死，甚至会造成人类文明的灭亡，以及一切生物的灭绝和地球生态的毁灭。其次，核能的和平利用，尤其是核电的使用，也未彻底解决核废料长达 <span> 10 </span> 亿年之久的放射性危害的问题，而且未能完全杜绝核泄漏的发生（切尔诺贝利和福岛都是例证），而泄露的污染和危害范围，也是超越国界的。这里还未提到一些国家掩盖了针对别国的、为着战争目的的核能发展。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">因此种种，从爱因斯坦这类科学家和广岛投弹美军飞行员的反思，到国际原子能委员会的成立及其国际检查活动，反映了人类对这类科技成果负面作用的共识。这种亡羊补牢式的思考当然很重要，但是有些事情，如果亡羊再来补牢，则为时晚矣！ <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">因为超级人工智能极可能来得很快，并以几何级数的速度迅猛发展，所以，对人工智能研发的国际监管，事实上越早越好！ <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    三．
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">斯蒂芬•霍金（ <span> S. Hawking </span> ）指出，人工智能的出现 <span> , </span> 在人类历史上的意义不亚于核武器。这已经向我们指出了事情的严重程度。他又说，人工智能的出现，是人类历史上最好的事情，也可能是最坏的事情。我们当然应该争取最好的事情、避免最坏的事情。不仅如此，正如卡鲁姆•蔡斯（ <span> K. Chace </span> ）在《人工智能革命》中所说，对付人工智能的巨大挑战，这场战争不能输，因为获胜的奖品是绚烂的未来——坐享一切，长生不老；失败的惩罚是灭顶之灾——人类灭绝，甚至更糟。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">既然是灭顶之灾，那么即使只有万分之一的可能，也应该作出一万分的努力去避免。实际上，在当今的国际局势下，人工智能的研发，分散在不同的国家、不同的机构，包括国家机构、军队机构，以及数不清的跨国公司、私人机构甚至“车库”当中，这就大大增加了危险发生的可能性。总所周知，军事机构研发的，可能首先是破坏性的或杀人的机器人，所以负面作用或危险发生的可能性，也因此大大增加。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">在一些人工智能研发能力很强的国家当中，民族主义的高涨当然也会大大地增加危险。例如在全世界人工智能研发能力最强的中国和美国，如果让民族主义和民粹主义占据主导地位，让“零和思维方式”、“高地思维方式”（“占领高地”是军事用语）占据主导地位，人类的命运就岌岌可危了。在这种情况下，光讲“自我规限”是远远不够的。“自律”的因素必须加上“他律”和“共治”的因素，加上“法治”和“强制”的因素。或许，要比国际原子能机构的监管更强、更严才行。或许，应该由联合国安理会这样的机构，来对各国和各机构的人工智能研发进行适当的监督、管理和指导。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">在此我谨建议，这项工作应该从下述三件事情开始： <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">要研讨并且确认人工智能现在已经和未来将会对全人类 （不止是对一个人群、一个国家）产生的益处和害处、福利和危险。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">要研讨如何整合全世界的研发资源 <span> , </span> 共同防止“强人工智能”获得自我决策能力或脱离人类控制的能力（包括防止作战机器人滥杀平民）。这需要在科学上和技术上（不仅在哲学上）定义“自主决策”。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">要研讨制定规程，让各国、各机构定期报告和专项报告研究项目、研究计划和研究进展，接受检查和核查，并且执行国际监管机构的决议。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">谢谢！ <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> 2018 </span> 年 <span> 7 </span> 月于北京 <span> </span></p>
    <p class="MsoNormal" style="text-indent:0cm;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> [ </span> <b>
    何光沪
   </b> 著名宗教哲学家，曾任中国人民大学宗教学系教授、博士生导师，中国社会科学院世界宗教研究所研究员、学术委员会委员。本文为作者 <span> 2018 </span> 年 <span> 7 </span> 月 <span> 11 </span> 日在「人工智能与道德风险」云豹沙龙的演讲，图片与说明为中评周刊编辑所加，转载请注明 <span> ] </span></p>
    <p class="MsoNormal" style="text-indent:21.0pt;"><span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    注释：
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
    <p class="MsoNormal" style="text-indent:21.0pt;">现在霍金（ <span> S. Hawking </span> ）、盖茨（ <span> B. Gates </span> ）还有马斯克（ <span> E. Mask </span> ），都联名呼吁要立法限制人工智能研发。 <span> </span></p>
    <p class="MsoNormal" style="text-indent:21.1pt;"><b>
    <span>
    </span>
   </b></p>
  </div>
</div>


  </div>

  <hr style="border-top:1px solid #28323C;"/>

<font size=2px>
  文章版权归原作者所有。
</font>

<div style="text-align:center"><img width="1px" src="https://i.imgur.com/HSw56Ez.png" alt="二维码分享本站" style="text-align:center"/></div>

  <div id="sametag">
    <h4 style="display: inline-block;">#天则观点 的其它文章</h4>
    <span>--<a href="https://nodebe4.github.io/opinion/2019-05-29/%E7%9B%9B%E6%B4%AA-%E7%94%A8%E7%BB%93%E6%9E%84%E6%80%A7%E5%AF%B9%E7%AD%89%E5%8E%9F%E5%88%99%E6%9B%BF%E4%BB%A3%E6%80%BB%E4%BD%93%E5%85%B3%E7%A8%8E%E5%AF%B9%E6%8A%97/">最新</a>-</span>
    <span>-<a href="https://nodebe4.github.io/opinion/2009-04-24/%E6%B2%BB%E5%9B%BD-18%E4%BA%BF%E4%BA%A9%E7%BA%A2%E7%BA%BF%E7%9A%84%E5%88%B6%E5%BA%A6%E5%90%AB%E4%B9%89/">最早</a>--</span>
    
      <li>
        <time>2018-08-11</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-11/%E7%8E%8B%E7%84%B1-%E9%80%8F%E8%BF%87%E8%AE%A1%E7%AE%97-%E6%94%AF%E9%85%8D%E4%B8%96%E7%95%8C/">
          王焱：透过计算，支配世界
        </a>
      </li>
    
    
      <li>
        <time>2018-08-11</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/">
          吕兆楠：人工智能与管理
        </a>
      </li>
    
    
      <li>
        <time>2018-08-10</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-10/%E8%AE%A8%E8%AE%BA%E4%B8%8E%E6%8F%90%E9%97%AE-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99/">
          讨论与提问「人工智能与道德风险」云豹沙龙
        </a>
      </li>
    
    
      <li>
        <time>2018-08-10</time>
        <a href="https://nodebe4.github.io/opinion/2018-08-10/%E8%92%8B%E8%B1%AA-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%88%B1%E4%B8%8E%E5%BF%A7/">
          蒋豪：人工智能爱与忧
        </a>
      </li>
    
  </div>


  <hr>
  <div class="pagination">
    
      <span class="prev" >
          <a href="https://nodebe4.github.io/opinion/2018-08-11/%E4%BC%8A%E5%88%A9%E5%A4%8F%E6%8F%90-%E4%BC%8A%E6%96%AF%E5%85%B0%E6%95%99%E4%B8%8E%E7%BB%B4%E5%90%BE%E5%B0%94%E4%BA%BA%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8/">
            前一篇：伊利夏提：伊斯兰教与维吾尔人独立运动
          </a>
      </span>
    
    
      <span class="next" >
          <a href="https://nodebe4.github.io/opinion/2018-08-11/%E5%85%A8%E7%90%8314%E4%BD%8D%E4%B8%93%E5%AE%B6%E8%B0%88%E4%B8%AD%E7%BE%8E%E5%85%B3%E7%B3%BB/">
            後一篇：全球14位专家谈中美关系
          </a>
      </span>
    

    <script>
    /* post pagination keyboard shortcuts */
    document.body.onkeyup = function(e){
      if (e.keyCode == '37') { window.location = 'https://nodebe4.github.io/opinion/2018-08-11/%E4%BC%8A%E5%88%A9%E5%A4%8F%E6%8F%90-%E4%BC%8A%E6%96%AF%E5%85%B0%E6%95%99%E4%B8%8E%E7%BB%B4%E5%90%BE%E5%B0%94%E4%BA%BA%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8/'; } // left arrow key
      if (e.keyCode == '39') { window.location = 'https://nodebe4.github.io/opinion/2018-08-11/%E5%85%A8%E7%90%8314%E4%BD%8D%E4%B8%93%E5%AE%B6%E8%B0%88%E4%B8%AD%E7%BE%8E%E5%85%B3%E7%B3%BB/'; } // right arrow key
      if (e.keyCode == '45') { window.location = 'https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/'; } // insert key
      if (e.keyCode == '46') { window.location = 'https://nodebe4.github.io/opinion/2018-08-10/%E8%AE%A8%E8%AE%BA%E4%B8%8E%E6%8F%90%E9%97%AE-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99/'; } // delete key
    };
    </script>
    <link rel="stylesheet" type="text/css" href="/opinion/assets/css/fab.css" />

<div class="fab-wrapper">
  <div class="fab-wheel">
    
    
    
    <a class="fab-action fab-action-1" title="上一篇(热键 &#8594;)" href="https://nodebe4.github.io/opinion/2018-08-11/%E4%BC%8A%E5%88%A9%E5%A4%8F%E6%8F%90-%E4%BC%8A%E6%96%AF%E5%85%B0%E6%95%99%E4%B8%8E%E7%BB%B4%E5%90%BE%E5%B0%94%E4%BA%BA%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8/">
      <i>后</i>
    </a>
    
    
    <a class="fab-action fab-action-2" title="下一篇(热键 &#8592;)" href="https://nodebe4.github.io/opinion/2018-08-11/%E5%85%A8%E7%90%8314%E4%BD%8D%E4%B8%93%E5%AE%B6%E8%B0%88%E4%B8%AD%E7%BE%8E%E5%85%B3%E7%B3%BB/">
      <i>前</i>
    </a>
    
    
    <a class="fab-action fab-action-3" title="<天则观点>上一篇(热键 ins)" href="https://nodebe4.github.io/opinion/2018-08-11/%E5%90%95%E5%85%86%E6%A5%A0-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E7%AE%A1%E7%90%86/">
      <i>左</i>
    </a>
    
    
    <a class="fab-action fab-action-4" title="<天则观点>下一篇(热键 del)" href="https://nodebe4.github.io/opinion/2018-08-10/%E8%AE%A8%E8%AE%BA%E4%B8%8E%E6%8F%90%E9%97%AE-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E9%81%93%E5%BE%B7%E9%A3%8E%E9%99%A9-%E4%BA%91%E8%B1%B9%E6%B2%99%E9%BE%99/">
      <i>右</i>
    </a>
    
  </div>
</div>


  </div>


  

</article>

    </div>

    <div style="z-index:2;">
<script src="/opinion/assets/js/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  cornerOffset: 20, // px
  id: 'back-to-top',
  backgroundColor: '#ddd',
  textColor: 'red'
})</script>
</div>


    <div class="wrapper-footer" id="footer">
      <div class="container">
        <footer class="footer">
          <img width="200px" src="https://i.imgur.com/HSw56Ez.png" alt="二维码分享本站"/>
<font size=2px>二维码分享本站</font>

<!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

  

  

  
  <li><a href="mailto:beauti4@protonmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
  

  

  

  
  <li><a href="https://github.com/NodeBE4/opinion" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
  

  

  

  

  

  
  <li><a href="/opinion/feed.xml" class="icon-21 rss" title="RSS"><svg viewBox="0 0 512 512"><path d="M201.8 347.2c0 20.3-16.5 36.8-36.8 36.8 -20.3 0-36.8-16.5-36.8-36.8s16.5-36.8 36.8-36.8C185.3 310.4 201.8 326.8 201.8 347.2zM128.2 204.7v54.5c68.5 0.7 124 56.3 124.7 124.7h54.5C306.7 285.3 226.9 205.4 128.2 204.7zM128.2 166.6c57.9 0.3 112.3 22.9 153.2 63.9 41 41 63.7 95.5 63.9 153.5h54.5c-0.3-149.9-121.7-271.4-271.6-271.9V166.6L128.2 166.6z"/></svg><!--[if lt IE 9]><em>RSS</em><![endif]--></a></li>
  

  

  

  

  

    
</ul>





<p><span style="color:blue">内容每小时更新一次.</span> Powered by <a href="https://github.com/AWEEKJ/kiko-now">Kiko Now</a> & <a href="https://github.com/gitalk/gitalk">Gitalk</a> & <a href="https://github.com/duty-machine/news">duty-machine</a>, 站务 <a href="https://be4.herokuapp.com">NodeBE4</a>（<span style="color:red">被墙</span>）</p>





        </footer>
      </div>
    </div>

    



  </body>
</html>
