<!DOCTYPE html>
<html>
  <head>
  <title>Fake News – 觀點 – 從草根到大師 git.io/JJCxS</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="
  
     Fake News 
    
  Posted on             Wednesday, November 16, 2016 
            Friday, July 24, 2020 
     Author  by  Ben Thompson   
    
    
  
  
  
    Between 2001 and 2003, Judith Miller wrote a number of pieces in the New York Times asserting that Iraq had the capability and the ambition to produce weapons of mass destruction. It was fake news.
    Looking back, it’s impossible to say with certainty what role Miller’s stories played in the U.S.’s ill-fated decision to invade Iraq in 2003; the same sources feeding Miller were well-connected with the George W. Bush administration’s foreign policy team. Still, it meant something to have the New York Times backing them up, particularly for Democrats who may have been inclined to push back against Bush more aggressively. After all, the New York Times was not some fly-by-night operation, it was the preeminent newspaper in the country, and one generally thought to lean towards the left. Miller’s stories had a certain resonance by virtue of where they were published.
    It’s tempting to make a connection between the Miller fiasco and the current debate about Facebook’s fake news problem; the cautionary tale that “fake news is bad” writes itself. My takeaway, though, is the exact opposite: it matters less what is fake and more who decides what is news in the first place.
     Facebook’s Commoditization of Media 
    In  Aggregation Theory  I described the process by which the demise of distribution-based economic power has resulted in the rise of powerful intermediaries that own the customer experience and commoditize their suppliers.  In the case of Facebook  , the social network started with the foundation of pre-existing offline networks that were moved online. Given that humans are inherently social, users started prioritizing time on Facebook over time spent reading, say, the newspaper (or any of the effectively infinite set of alternatives for attention).
    It followed, then, that it was in the interest of media companies, businesses, and basically anyone else who wanted to get the attention of users, to be on Facebook as well. This was great for Facebook: the more compelling content it could provide to its users, the more time they would spend on Facebook; the more time they spent on Facebook, the more opportunities Facebook would have to place advertisements in front of them. And, critically, the more time users spent on Facebook, the 
    less
    time they had to read anything else, further increasing the motivation for media companies (and businesses of all types) to be on Facebook themselves, resulting in a virtuous cycle in Facebook’s favor: by having the users Facebook captured the suppliers, which deepened their hold on the users, increasing their power over suppliers.
    This process reduced Facebook’s content suppliers — media companies — into pure commodity providers. All that mattered for everyone was the level of engagement: media companies got ad views, Facebook got shares, and users got the psychic reward of having flipped a bit in a database. Of course not all content was engaging to all users; that’s what the algorithm was for: show people only what they want to see, whether it be baby pictures, engagement announcements, cat pictures, quizzes, or, yes, political news. It was, from Facebook’s perspective — and, frankly, from its users’ perspective — all the same. That includes fake news too, by the way: it’s not that there is anything particularly special about  news from Macedonia  , it’s that according to the algorithm there isn’t anything particularly special about 
    any
    content, beyond the level of engagement it drives.
     The Media and Trump 
    There has been a lot of discussion — in the media, naturally — about how the media made President-elect Donald Trump. The story is that Trump would have never amounted to anything had the media not given him  billions of dollars worth of earned media  — basically news coverage (as opposed to paid media, which is advertising) — and that the industry needed to take responsibility. It’s a lovely bit of self-reflection that lets the industry deny the far more discomforting reality: that the media couldn’t have done a damn thing about Trump if they had wanted to.
    The reason the media covered Trump so extensively is quite simple: that is what users wanted. And, in a world where media is a commodity, to act as if one has the editorial prerogative to 
    not
    cover a candidate users want to see is to face that reality square in the face, absent the clicks that make the medicine easier to take.
    Indeed, this is the same reason fake news flourishes: because users want it. These sites get traffic because users click on their articles and share them, because they confirm what they already think to be true. Confirmation bias is a hell of a drug — and, as Techcrunch reporter Kim-Mai Cutler so aptly  put it on Twitter  , it’s a hell of a business model.
     Why Facebook Should Fix Fake News 
    So now we arrive at the question of what to do about fake news. Perhaps the most common sentiment was laid out by Zeynep Tufekci in the  New York Times  : Facebook should eliminate fake news and the filter effect — the tendency to see news you already agree with — while they’re at it. Tufekci writes:
    
      Mark Zuckerberg, Facebook’s chief, believes that it is “a pretty crazy idea” that “fake news on Facebook, which is a very small amount of content, influenced the election in any way.” In holding fast to the claim that his company has little effect on how people make up their minds, Mr. Zuckerberg is doing real damage to American democracy — and to the world…
      The problem with Facebook’s influence on political discourse is not limited to the dissemination of fake news. It’s also about echo chambers. The company’s algorithm chooses which updates appear higher up in users’ newsfeeds and which are buried. Humans already tend to cluster among like-minded people and seek news that confirms their biases. Facebook’s research shows that the company’s algorithm encourages this by somewhat prioritizing updates that users find comforting…
    
    Tufekci offers up a number of recommendations for Facebook, including sharing data with outside researchers to better understand how misinformation spreads and the extent of filter bubbles,  1  acting much more aggressively to eliminate fake news like it does spam and other objectionable content, rehiring human editors, and retweaking its algorithm to favor news balance, not just engagement.
     Why Facebook Should Not 
    All seem reasonable on their face, but in fact Tufekci’s recommendations are radical in their own way.
    First, there is no incentive for Facebook to do any of this; while the company  denies  this  report in Gizmodo  that the company shelved a change to the News Feed algorithm that would have eliminated fake news stories because it disproportionately affected right-wing sites, the fact remains that the company is heavily incentivized to be perceived as neutral by all sides; anything else would drive away users, a particularly problematic outcome for a social network.  2 
    Moreover, any move away from a focus on engagement would, by definition, decrease the time spent on Facebook, and here Tufekci is wrong to claim that this is acceptable because there is “no competitor in sight.” In fact, Facebook is in its most challenging position in a long time: Snapchat is stealing attention from its most valuable demographics, even as the News Feed is approaching saturation in terms of ad load, and there is a real danger Snapchat will beat the company to the biggest prize in consumer tech: TV-centric brand advertising dollars.
    There are even more fundamental problems, though: how do you decide what is fake and what isn’t? Where is the line? And, perhaps most critically, who decides? To argue that the existence of some number of fake news items amongst an ocean of other content ought to result in active editing of Facebook content is not simply a logistical nightmare but, at least when it comes to the potential of bad outcomes, far more fraught than it appears.
    That goes double for the filter bubble problem: there is a very large leap from arguing Facebook impacts its users’ flow of information via the second-order effects of driving engagement, to insisting the platform actively influence what users see for political reasons. It doesn’t matter that the goal is a better society, as opposed to picking partisan sides; after all, partisans think their goal is a better society as well. Indeed, if the entire concern is the outsized role that Facebook plays in its users’ news consumption, then the far greater fear should be the potential of someone actively abusing that role for their own ends.
    I get why top-down solutions are tempting: fake news and filter bubbles are in front of our faces, and wouldn’t it be better if Facebook fixed them? The problem is the assumption that whoever wields that top-down power will just so happen to have the same views I do. What, though, if they don’t? Just look at our current political situation: those worried about Trump have to contend with the fact that the power of the executive branch has been dramatically expanded over the decades; we place immense responsibility and capability in the hands of one person, forgetting that said responsibility and capability is not so easily withdrawn if we don’t like the one wielding it.
    To that end I would be far more concerned about Facebook were they to begin actively editing the News Feed; as I  noted last week  I’m increasingly concerned about Zuckerberg’s utopian-esque view of the world, and it is a frighteningly small step from influencing the world to controlling the world. Just as bad would be government regulation: our most critical liberty when it comes to a check on tyranny is the freedom of speech, and it would be directly counter to that liberty to put a bureaucrat — who reports to the President — in charge of what people see.
    The key thing to remember is that the actual impact of fake news is dependent on who delivers it: sure, those Macedonian news stories aren’t great, but their effect, such as it is, comes from confirming what people already believe. Contrast that to Miller’s stories in the New York Times: because the New York Times was a trusted gatekeeper, many people fundamentally 
    changed
    their opinions, resulting in a disaster the full effects of which are still being felt. In that light, the potential downside of Facebook coming anywhere close to deciding the news can scarcely be imagined.
     Liberty and Laziness 
    There may be some middle ground here: perhaps some sources are so obviously fake that Facebook can easily exclude them, ideally with full transparency about what they are doing and why. And, to the extent Facebook can share data with outside researchers without compromising its competitive position, it should do so. The company should also provide even more options to users to control their feed if they wish to avoid filter bubbles.
    In truth, though, you and I know that few users will bother. And that, seemingly, is what bothers many of Facebook’s critics the most. If users won’t seek out the “right” news sources, well, then someone ought to make them see them. It all sounds great — and, without question, a far more convenient solution to winning elections than actually making the changes necessary to do so — until you remember that that someone you just entrusted with such awesome power could disagree with you, and that the very notion of controlling what people read is the hallmark of totalitarianism.
    Let me be clear: I am well aware of the problematic aspects of Facebook’s impact; I am particularly worried about the ease with which we sort ourselves into tribes, in part because of the filter bubble effect noted above (that’s one of the reasons  Why Twitter Must Be Saved  ). But the solution is not the reimposition of gatekeepers done in by the Internet; whatever fixes this problem must spring from the power of the Internet, and the fact that each of us, if we choose, has access to more information and sources of truth than ever before, and more ways to reach out and understand and persuade those with whom we disagree. Yes, that is more work than demanding Zuckerberg change what people see, but giving up liberty for laziness never works out well in the end.
    
    
    For more about how the Internet has fundamentally changed politics, please see this piece from March,
    
     The Voters Decide
    
    .
   
    
      Facebook has done a study about the latter, but as Tufekci and others have documented,  the study was full of problems  [  ↩  ]
      Indeed, it wasn’t that long ago that I was making this exact argument in response to those who insisted Facebook would alter the News Feed to serve their own political purposes [  ↩  ]
    
    
      
         Share 
        
          
              Facebook  
              Twitter  
              LinkedIn  
              Email  
            
          
        
      
    
    
       
     Related
     
    
  
  

" />
    <meta property="og:description" content="
  
     Fake News 
    
  Posted on             Wednesday, November 16, 2016 
            Friday, July 24, 2020 
     Author  by  Ben Thompson   
    
    
  
  
  
    Between 2001 and 2003, Judith Miller wrote a number of pieces in the New York Times asserting that Iraq had the capability and the ambition to produce weapons of mass destruction. It was fake news.
    Looking back, it’s impossible to say with certainty what role Miller’s stories played in the U.S.’s ill-fated decision to invade Iraq in 2003; the same sources feeding Miller were well-connected with the George W. Bush administration’s foreign policy team. Still, it meant something to have the New York Times backing them up, particularly for Democrats who may have been inclined to push back against Bush more aggressively. After all, the New York Times was not some fly-by-night operation, it was the preeminent newspaper in the country, and one generally thought to lean towards the left. Miller’s stories had a certain resonance by virtue of where they were published.
    It’s tempting to make a connection between the Miller fiasco and the current debate about Facebook’s fake news problem; the cautionary tale that “fake news is bad” writes itself. My takeaway, though, is the exact opposite: it matters less what is fake and more who decides what is news in the first place.
     Facebook’s Commoditization of Media 
    In  Aggregation Theory  I described the process by which the demise of distribution-based economic power has resulted in the rise of powerful intermediaries that own the customer experience and commoditize their suppliers.  In the case of Facebook  , the social network started with the foundation of pre-existing offline networks that were moved online. Given that humans are inherently social, users started prioritizing time on Facebook over time spent reading, say, the newspaper (or any of the effectively infinite set of alternatives for attention).
    It followed, then, that it was in the interest of media companies, businesses, and basically anyone else who wanted to get the attention of users, to be on Facebook as well. This was great for Facebook: the more compelling content it could provide to its users, the more time they would spend on Facebook; the more time they spent on Facebook, the more opportunities Facebook would have to place advertisements in front of them. And, critically, the more time users spent on Facebook, the 
    less
    time they had to read anything else, further increasing the motivation for media companies (and businesses of all types) to be on Facebook themselves, resulting in a virtuous cycle in Facebook’s favor: by having the users Facebook captured the suppliers, which deepened their hold on the users, increasing their power over suppliers.
    This process reduced Facebook’s content suppliers — media companies — into pure commodity providers. All that mattered for everyone was the level of engagement: media companies got ad views, Facebook got shares, and users got the psychic reward of having flipped a bit in a database. Of course not all content was engaging to all users; that’s what the algorithm was for: show people only what they want to see, whether it be baby pictures, engagement announcements, cat pictures, quizzes, or, yes, political news. It was, from Facebook’s perspective — and, frankly, from its users’ perspective — all the same. That includes fake news too, by the way: it’s not that there is anything particularly special about  news from Macedonia  , it’s that according to the algorithm there isn’t anything particularly special about 
    any
    content, beyond the level of engagement it drives.
     The Media and Trump 
    There has been a lot of discussion — in the media, naturally — about how the media made President-elect Donald Trump. The story is that Trump would have never amounted to anything had the media not given him  billions of dollars worth of earned media  — basically news coverage (as opposed to paid media, which is advertising) — and that the industry needed to take responsibility. It’s a lovely bit of self-reflection that lets the industry deny the far more discomforting reality: that the media couldn’t have done a damn thing about Trump if they had wanted to.
    The reason the media covered Trump so extensively is quite simple: that is what users wanted. And, in a world where media is a commodity, to act as if one has the editorial prerogative to 
    not
    cover a candidate users want to see is to face that reality square in the face, absent the clicks that make the medicine easier to take.
    Indeed, this is the same reason fake news flourishes: because users want it. These sites get traffic because users click on their articles and share them, because they confirm what they already think to be true. Confirmation bias is a hell of a drug — and, as Techcrunch reporter Kim-Mai Cutler so aptly  put it on Twitter  , it’s a hell of a business model.
     Why Facebook Should Fix Fake News 
    So now we arrive at the question of what to do about fake news. Perhaps the most common sentiment was laid out by Zeynep Tufekci in the  New York Times  : Facebook should eliminate fake news and the filter effect — the tendency to see news you already agree with — while they’re at it. Tufekci writes:
    
      Mark Zuckerberg, Facebook’s chief, believes that it is “a pretty crazy idea” that “fake news on Facebook, which is a very small amount of content, influenced the election in any way.” In holding fast to the claim that his company has little effect on how people make up their minds, Mr. Zuckerberg is doing real damage to American democracy — and to the world…
      The problem with Facebook’s influence on political discourse is not limited to the dissemination of fake news. It’s also about echo chambers. The company’s algorithm chooses which updates appear higher up in users’ newsfeeds and which are buried. Humans already tend to cluster among like-minded people and seek news that confirms their biases. Facebook’s research shows that the company’s algorithm encourages this by somewhat prioritizing updates that users find comforting…
    
    Tufekci offers up a number of recommendations for Facebook, including sharing data with outside researchers to better understand how misinformation spreads and the extent of filter bubbles,  1  acting much more aggressively to eliminate fake news like it does spam and other objectionable content, rehiring human editors, and retweaking its algorithm to favor news balance, not just engagement.
     Why Facebook Should Not 
    All seem reasonable on their face, but in fact Tufekci’s recommendations are radical in their own way.
    First, there is no incentive for Facebook to do any of this; while the company  denies  this  report in Gizmodo  that the company shelved a change to the News Feed algorithm that would have eliminated fake news stories because it disproportionately affected right-wing sites, the fact remains that the company is heavily incentivized to be perceived as neutral by all sides; anything else would drive away users, a particularly problematic outcome for a social network.  2 
    Moreover, any move away from a focus on engagement would, by definition, decrease the time spent on Facebook, and here Tufekci is wrong to claim that this is acceptable because there is “no competitor in sight.” In fact, Facebook is in its most challenging position in a long time: Snapchat is stealing attention from its most valuable demographics, even as the News Feed is approaching saturation in terms of ad load, and there is a real danger Snapchat will beat the company to the biggest prize in consumer tech: TV-centric brand advertising dollars.
    There are even more fundamental problems, though: how do you decide what is fake and what isn’t? Where is the line? And, perhaps most critically, who decides? To argue that the existence of some number of fake news items amongst an ocean of other content ought to result in active editing of Facebook content is not simply a logistical nightmare but, at least when it comes to the potential of bad outcomes, far more fraught than it appears.
    That goes double for the filter bubble problem: there is a very large leap from arguing Facebook impacts its users’ flow of information via the second-order effects of driving engagement, to insisting the platform actively influence what users see for political reasons. It doesn’t matter that the goal is a better society, as opposed to picking partisan sides; after all, partisans think their goal is a better society as well. Indeed, if the entire concern is the outsized role that Facebook plays in its users’ news consumption, then the far greater fear should be the potential of someone actively abusing that role for their own ends.
    I get why top-down solutions are tempting: fake news and filter bubbles are in front of our faces, and wouldn’t it be better if Facebook fixed them? The problem is the assumption that whoever wields that top-down power will just so happen to have the same views I do. What, though, if they don’t? Just look at our current political situation: those worried about Trump have to contend with the fact that the power of the executive branch has been dramatically expanded over the decades; we place immense responsibility and capability in the hands of one person, forgetting that said responsibility and capability is not so easily withdrawn if we don’t like the one wielding it.
    To that end I would be far more concerned about Facebook were they to begin actively editing the News Feed; as I  noted last week  I’m increasingly concerned about Zuckerberg’s utopian-esque view of the world, and it is a frighteningly small step from influencing the world to controlling the world. Just as bad would be government regulation: our most critical liberty when it comes to a check on tyranny is the freedom of speech, and it would be directly counter to that liberty to put a bureaucrat — who reports to the President — in charge of what people see.
    The key thing to remember is that the actual impact of fake news is dependent on who delivers it: sure, those Macedonian news stories aren’t great, but their effect, such as it is, comes from confirming what people already believe. Contrast that to Miller’s stories in the New York Times: because the New York Times was a trusted gatekeeper, many people fundamentally 
    changed
    their opinions, resulting in a disaster the full effects of which are still being felt. In that light, the potential downside of Facebook coming anywhere close to deciding the news can scarcely be imagined.
     Liberty and Laziness 
    There may be some middle ground here: perhaps some sources are so obviously fake that Facebook can easily exclude them, ideally with full transparency about what they are doing and why. And, to the extent Facebook can share data with outside researchers without compromising its competitive position, it should do so. The company should also provide even more options to users to control their feed if they wish to avoid filter bubbles.
    In truth, though, you and I know that few users will bother. And that, seemingly, is what bothers many of Facebook’s critics the most. If users won’t seek out the “right” news sources, well, then someone ought to make them see them. It all sounds great — and, without question, a far more convenient solution to winning elections than actually making the changes necessary to do so — until you remember that that someone you just entrusted with such awesome power could disagree with you, and that the very notion of controlling what people read is the hallmark of totalitarianism.
    Let me be clear: I am well aware of the problematic aspects of Facebook’s impact; I am particularly worried about the ease with which we sort ourselves into tribes, in part because of the filter bubble effect noted above (that’s one of the reasons  Why Twitter Must Be Saved  ). But the solution is not the reimposition of gatekeepers done in by the Internet; whatever fixes this problem must spring from the power of the Internet, and the fact that each of us, if we choose, has access to more information and sources of truth than ever before, and more ways to reach out and understand and persuade those with whom we disagree. Yes, that is more work than demanding Zuckerberg change what people see, but giving up liberty for laziness never works out well in the end.
    
    
    For more about how the Internet has fundamentally changed politics, please see this piece from March,
    
     The Voters Decide
    
    .
   
    
      Facebook has done a study about the latter, but as Tufekci and others have documented,  the study was full of problems  [  ↩  ]
      Indeed, it wasn’t that long ago that I was making this exact argument in response to those who insisted Facebook would alter the News Feed to serve their own political purposes [  ↩  ]
    
    
      
         Share 
        
          
              Facebook  
              Twitter  
              LinkedIn  
              Email  
            
          
        
      
    
    
       
     Related
     
    
  
  

" />
    
    <meta name="author" content="觀點" />

    
    <meta property="og:title" content="Fake News" />
    <meta property="twitter:title" content="Fake News" />
    

  <link rel="stylesheet" type="text/css" href="/opinion/style.css" />
  <link rel="alternate" type="application/rss+xml" title="觀點 - 從草根到大師 git.io/JJCxS" href="/opinion/feed.xml" />

  <!-- Social Share Kit CSS -->
  <link rel="stylesheet" href="/opinion/assets/css/social-share-kit.css" type="text/css">
  <link rel="stylesheet" href="/opinion/assets/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="/opinion/assets/css/bootstrap.min.css" type="text/css">
  <script type="text/javascript" src="/opinion/assets/js/jquery-3.5.1.js"></script>
  <script type="text/javascript" src="/opinion/assets/js/page.js"></script>

</head>

  <body>
    <div class="wrapper-masthead">
  <div class="container">
    <header class="masthead clearfix">
      

      <div class="site-info">
        <h1 class="site-name" style="display: inline-block;"><a href="/opinion/">觀點</a></h1>
        <i class="site-description" style="font-size: 12px;">從草根到大師 git.io/JJCxS</i>
      </div>

      <nav>
        <span id="search-container" >
          <a href="/opinion/tools"><i class="fa fa-bookmark twitter" title="百宝箱"></i></a>
        <a><i class="fa fa-search" title="限前100結果"></i></a><input type="text" id="search-input" placeholder="標題 作者 來源 日期 (17489)"
          style="margin: 10px 0px 0px 0px; height: 30px;width: auto" title="本站最正確的打開方式">
        </span>
        
        
        <a href="/opinion/categories" style="color: Tomato;"><i class="fa fa-tags" title="分类"></i></a>
        
        
        
        <a href="https://be4.herokuapp.com/" style="color: #003366;"><i class="fa fa-comments" title="论坛"></i></a>
        
        
        
        <a href="/opinion/about"><i class="fa fa-info-circle" title="关于"></i></a>
        
        
        <a title="电脑热键：&larr;上一篇(页), &rarr;下一篇(页), ins同来源新一篇，del同来源旧一篇" onclick="toggle_visibility('help')"><i class="fa fa-question-circle"></i></a>
        <a id="fa-home" href="https://nodebe4.github.io" title="BE4服务列表" onclick="//toggle_visibility('site-list')"><i class="fa fa-home" aria-hidden="true"></i></a>
      </nav>

    </header>
    <div id="site-list" class="tags" style="display: block;text-align: right;border-bottom: 1px solid lightGray;"><noscript><span style="background-color: #e8e8e8;color: #d10000;font-size: 14px;">开启浏览器JavaScript以获取搜索功能和更好的浏览体验</span></noscript></div>
    <p id="help" style="font-size: 14px;display: none;text-align: right;"><span style="color:green;">电脑热键：&larr;上一篇(页), &rarr;下一篇(页), ins同来源新一篇, del同来源旧一篇</span>; <span style="color:orange">对应触屏FAB：上下右左</span>; 轉Markdown<a href="https://euangoddard.github.io/clipboard2markdown/"><i class="fa fa-file-text-o"></i></a></p>
  </div>
</div>

<script type="text/javascript" >
  function toggle_visibility(id){
    var help = document.getElementById(id)
    if (help.style.display=='none'){
      help.style.display='block';
    }else{
      help.style.display='none';
    }
  }

  const url = "https://nodebe4.github.io/sitelist.json"

  document.addEventListener("DOMContentLoaded", function(event){
    // var homebtn = document.getElementById("fa-home")
    // homebtn.removeAttribute("href")
    var content = document.getElementById("site-list");
    content.innerHTML = ''
    var ul = document.createElement("ul")
    ul.classList.add("label")
    content.appendChild(ul)
    var cnt = 0

    $.getJSON(url, function(allsites) {

      allsites.map(item =>{
        var li = document.createElement('li')
        li.classList.add("tag")
        li.id = 'site-' + cnt
        ul.appendChild(li)
        var a0 = document.createElement('a')
        li.appendChild(a0)
        a0.href = item.url[0]
        var span = document.createElement('span')
        a0.appendChild(span)
        span.innerText = item['name']
        // span.style.backgroundColor = item['background-color']
        // span.style.color='#E4CBC3'
        span.style.color = item['background-color']
        span.style['font-size'] = '14px'
        cnt += 1
        // test_alive(li.id, a0.href)
      })
    })
  })

function test_alive(id, url){
  var divstatus = document.getElementById(id)
  const base = 'https://textance.herokuapp.com/title/'
  var fullurl = base + url
  $.ajax({
      url: fullurl,
      complete: function(data) {
        if (data.responseText.includes('502')){
          // divstatus.style.color='#FBB7B7'
          // divstatus.style.color='gray'
          // divstatus.title = "服务器无响应"
          divstatus.parentNode.removeChild(divstatus)
        }else{
          // divstatus.style.color='#B6FAC8'
          divstatus.title = data.responseText
        }
      }
  });
  return divstatus
}
</script>



    <!-- Left & centered positioning -->

<div class="ssk-sticky ssk-right ssk-center ssk-sticky-hide-xs ssk-group ssk-round">
  
    <a href="https://be4news.pythonanywhere.com/archivenow/ia/https%3A%2F%2Fstratechery.com%2F2016%2Ffake-news%2F" class="ssk ssk-link" title="存到互联网档案馆" target="_blank"></a>
    <a href="https://www.facebook.com/sharer.php?u=https://stratechery.com/2016/fake-news/" class="ssk ssk-facebook"></a>
    <a href="https://twitter.com/intent/tweet?url=https://stratechery.com/2016/fake-news/&text=Fake News&hashtags=觀點" class="ssk ssk-twitter"></a>
    <a href="https://reddit.com/submit?url=https://stratechery.com/2016/fake-news/&title=Fake News" class="ssk ssk-reddit"></a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://stratechery.com/2016/fake-news/&title=Fake News" class="ssk ssk-linkedin"></a>
    <a href="mailto:{email_address}?subject=Fake News&body=
  
     Fake News 
    
  Posted on             Wednesday, November 16, 2016 
            Friday, July 24, 2020 
     Author  by  Ben Thompson   
    
    
  
  
  
    Between 2001 and 2003, Judith Miller wrote a number of pieces in the New York Times asserting that Iraq had the capability and the ambition to produce weapons of mass destruction. It was fake news.
    Looking back, it’s impossible to say with certainty what role Miller’s stories played in the U.S.’s ill-fated decision to invade Iraq in 2003; the same sources feeding Miller were well-connected with the George W. Bush administration’s foreign policy team. Still, it meant something to have the New York Times backing them up, particularly for Democrats who may have been inclined to push back against Bush more aggressively. After all, the New York Times was not some fly-by-night operation, it was the preeminent newspaper in the country, and one generally thought to lean towards the left. Miller’s stories had a certain resonance by virtue of where they were published.
    It’s tempting to make a connection between the Miller fiasco and the current debate about Facebook’s fake news problem; the cautionary tale that “fake news is bad” writes itself. My takeaway, though, is the exact opposite: it matters less what is fake and more who decides what is news in the first place.
     Facebook’s Commoditization of Media 
    In  Aggregation Theory  I described the process by which the demise of distribution-based economic power has resulted in the rise of powerful intermediaries that own the customer experience and commoditize their suppliers.  In the case of Facebook  , the social network started with the foundation of pre-existing offline networks that were moved online. Given that humans are inherently social, users started prioritizing time on Facebook over time spent reading, say, the newspaper (or any of the effectively infinite set of alternatives for attention).
    It followed, then, that it was in the interest of media companies, businesses, and basically anyone else who wanted to get the attention of users, to be on Facebook as well. This was great for Facebook: the more compelling content it could provide to its users, the more time they would spend on Facebook; the more time they spent on Facebook, the more opportunities Facebook would have to place advertisements in front of them. And, critically, the more time users spent on Facebook, the 
    less
    time they had to read anything else, further increasing the motivation for media companies (and businesses of all types) to be on Facebook themselves, resulting in a virtuous cycle in Facebook’s favor: by having the users Facebook captured the suppliers, which deepened their hold on the users, increasing their power over suppliers.
    This process reduced Facebook’s content suppliers — media companies — into pure commodity providers. All that mattered for everyone was the level of engagement: media companies got ad views, Facebook got shares, and users got the psychic reward of having flipped a bit in a database. Of course not all content was engaging to all users; that’s what the algorithm was for: show people only what they want to see, whether it be baby pictures, engagement announcements, cat pictures, quizzes, or, yes, political news. It was, from Facebook’s perspective — and, frankly, from its users’ perspective — all the same. That includes fake news too, by the way: it’s not that there is anything particularly special about  news from Macedonia  , it’s that according to the algorithm there isn’t anything particularly special about 
    any
    content, beyond the level of engagement it drives.
     The Media and Trump 
    There has been a lot of discussion — in the media, naturally — about how the media made President-elect Donald Trump. The story is that Trump would have never amounted to anything had the media not given him  billions of dollars worth of earned media  — basically news coverage (as opposed to paid media, which is advertising) — and that the industry needed to take responsibility. It’s a lovely bit of self-reflection that lets the industry deny the far more discomforting reality: that the media couldn’t have done a damn thing about Trump if they had wanted to.
    The reason the media covered Trump so extensively is quite simple: that is what users wanted. And, in a world where media is a commodity, to act as if one has the editorial prerogative to 
    not
    cover a candidate users want to see is to face that reality square in the face, absent the clicks that make the medicine easier to take.
    Indeed, this is the same reason fake news flourishes: because users want it. These sites get traffic because users click on their articles and share them, because they confirm what they already think to be true. Confirmation bias is a hell of a drug — and, as Techcrunch reporter Kim-Mai Cutler so aptly  put it on Twitter  , it’s a hell of a business model.
     Why Facebook Should Fix Fake News 
    So now we arrive at the question of what to do about fake news. Perhaps the most common sentiment was laid out by Zeynep Tufekci in the  New York Times  : Facebook should eliminate fake news and the filter effect — the tendency to see news you already agree with — while they’re at it. Tufekci writes:
    
      Mark Zuckerberg, Facebook’s chief, believes that it is “a pretty crazy idea” that “fake news on Facebook, which is a very small amount of content, influenced the election in any way.” In holding fast to the claim that his company has little effect on how people make up their minds, Mr. Zuckerberg is doing real damage to American democracy — and to the world…
      The problem with Facebook’s influence on political discourse is not limited to the dissemination of fake news. It’s also about echo chambers. The company’s algorithm chooses which updates appear higher up in users’ newsfeeds and which are buried. Humans already tend to cluster among like-minded people and seek news that confirms their biases. Facebook’s research shows that the company’s algorithm encourages this by somewhat prioritizing updates that users find comforting…
    
    Tufekci offers up a number of recommendations for Facebook, including sharing data with outside researchers to better understand how misinformation spreads and the extent of filter bubbles,  1  acting much more aggressively to eliminate fake news like it does spam and other objectionable content, rehiring human editors, and retweaking its algorithm to favor news balance, not just engagement.
     Why Facebook Should Not 
    All seem reasonable on their face, but in fact Tufekci’s recommendations are radical in their own way.
    First, there is no incentive for Facebook to do any of this; while the company  denies  this  report in Gizmodo  that the company shelved a change to the News Feed algorithm that would have eliminated fake news stories because it disproportionately affected right-wing sites, the fact remains that the company is heavily incentivized to be perceived as neutral by all sides; anything else would drive away users, a particularly problematic outcome for a social network.  2 
    Moreover, any move away from a focus on engagement would, by definition, decrease the time spent on Facebook, and here Tufekci is wrong to claim that this is acceptable because there is “no competitor in sight.” In fact, Facebook is in its most challenging position in a long time: Snapchat is stealing attention from its most valuable demographics, even as the News Feed is approaching saturation in terms of ad load, and there is a real danger Snapchat will beat the company to the biggest prize in consumer tech: TV-centric brand advertising dollars.
    There are even more fundamental problems, though: how do you decide what is fake and what isn’t? Where is the line? And, perhaps most critically, who decides? To argue that the existence of some number of fake news items amongst an ocean of other content ought to result in active editing of Facebook content is not simply a logistical nightmare but, at least when it comes to the potential of bad outcomes, far more fraught than it appears.
    That goes double for the filter bubble problem: there is a very large leap from arguing Facebook impacts its users’ flow of information via the second-order effects of driving engagement, to insisting the platform actively influence what users see for political reasons. It doesn’t matter that the goal is a better society, as opposed to picking partisan sides; after all, partisans think their goal is a better society as well. Indeed, if the entire concern is the outsized role that Facebook plays in its users’ news consumption, then the far greater fear should be the potential of someone actively abusing that role for their own ends.
    I get why top-down solutions are tempting: fake news and filter bubbles are in front of our faces, and wouldn’t it be better if Facebook fixed them? The problem is the assumption that whoever wields that top-down power will just so happen to have the same views I do. What, though, if they don’t? Just look at our current political situation: those worried about Trump have to contend with the fact that the power of the executive branch has been dramatically expanded over the decades; we place immense responsibility and capability in the hands of one person, forgetting that said responsibility and capability is not so easily withdrawn if we don’t like the one wielding it.
    To that end I would be far more concerned about Facebook were they to begin actively editing the News Feed; as I  noted last week  I’m increasingly concerned about Zuckerberg’s utopian-esque view of the world, and it is a frighteningly small step from influencing the world to controlling the world. Just as bad would be government regulation: our most critical liberty when it comes to a check on tyranny is the freedom of speech, and it would be directly counter to that liberty to put a bureaucrat — who reports to the President — in charge of what people see.
    The key thing to remember is that the actual impact of fake news is dependent on who delivers it: sure, those Macedonian news stories aren’t great, but their effect, such as it is, comes from confirming what people already believe. Contrast that to Miller’s stories in the New York Times: because the New York Times was a trusted gatekeeper, many people fundamentally 
    changed
    their opinions, resulting in a disaster the full effects of which are still being felt. In that light, the potential downside of Facebook coming anywhere close to deciding the news can scarcely be imagined.
     Liberty and Laziness 
    There may be some middle ground here: perhaps some sources are so obviously fake that Facebook can easily exclude them, ideally with full transparency about what they are doing and why. And, to the extent Facebook can share data with outside researchers without compromising its competitive position, it should do so. The company should also provide even more options to users to control their feed if they wish to avoid filter bubbles.
    In truth, though, you and I know that few users will bother. And that, seemingly, is what bothers many of Facebook’s critics the most. If users won’t seek out the “right” news sources, well, then someone ought to make them see them. It all sounds great — and, without question, a far more convenient solution to winning elections than actually making the changes necessary to do so — until you remember that that someone you just entrusted with such awesome power could disagree with you, and that the very notion of controlling what people read is the hallmark of totalitarianism.
    Let me be clear: I am well aware of the problematic aspects of Facebook’s impact; I am particularly worried about the ease with which we sort ourselves into tribes, in part because of the filter bubble effect noted above (that’s one of the reasons  Why Twitter Must Be Saved  ). But the solution is not the reimposition of gatekeepers done in by the Internet; whatever fixes this problem must spring from the power of the Internet, and the fact that each of us, if we choose, has access to more information and sources of truth than ever before, and more ways to reach out and understand and persuade those with whom we disagree. Yes, that is more work than demanding Zuckerberg change what people see, but giving up liberty for laziness never works out well in the end.
    
    
    For more about how the Internet has fundamentally changed politics, please see this piece from March,
    
     The Voters Decide
    
    .
   
    
      Facebook has done a study about the latter, but as Tufekci and others have documented,  the study was full of problems  [  ↩  ]
      Indeed, it wasn’t that long ago that I was making this exact argument in response to those who insisted Facebook would alter the News Feed to serve their own political purposes [  ↩  ]
    
    
      
         Share 
        
          
              Facebook  
              Twitter  
              LinkedIn  
              Email  
            
          
        
      
    
    
       
     Related
     
    
  
  

" class="ssk ssk-email"></a>
    <a href="http://pinterest.com/pin/create/link/?url=https://stratechery.com/2016/fake-news/" class="ssk ssk-pinterest"></a>
    <a href="https://www.tumblr.com/widgets/share/tool?canonicalUrl=https://stratechery.com/2016/fake-news/&title=Fake News&caption=
  
     Fake News 
    
  Posted on             Wednesday, November 16, 2016 
            Friday, July 24, 2020 
     Author  by  Ben Thompson   
    
    
  
  
  
    Between 2001 and 2003, Judith Miller wrote a number of pieces in the New York Times asserting that Iraq had the capability and the ambition to produce weapons of mass destruction. It was fake news.
    Looking back, it’s impossible to say with certainty what role Miller’s stories played in the U.S.’s ill-fated decision to invade Iraq in 2003; the same sources feeding Miller were well-connected with the George W. Bush administration’s foreign policy team. Still, it meant something to have the New York Times backing them up, particularly for Democrats who may have been inclined to push back against Bush more aggressively. After all, the New York Times was not some fly-by-night operation, it was the preeminent newspaper in the country, and one generally thought to lean towards the left. Miller’s stories had a certain resonance by virtue of where they were published.
    It’s tempting to make a connection between the Miller fiasco and the current debate about Facebook’s fake news problem; the cautionary tale that “fake news is bad” writes itself. My takeaway, though, is the exact opposite: it matters less what is fake and more who decides what is news in the first place.
     Facebook’s Commoditization of Media 
    In  Aggregation Theory  I described the process by which the demise of distribution-based economic power has resulted in the rise of powerful intermediaries that own the customer experience and commoditize their suppliers.  In the case of Facebook  , the social network started with the foundation of pre-existing offline networks that were moved online. Given that humans are inherently social, users started prioritizing time on Facebook over time spent reading, say, the newspaper (or any of the effectively infinite set of alternatives for attention).
    It followed, then, that it was in the interest of media companies, businesses, and basically anyone else who wanted to get the attention of users, to be on Facebook as well. This was great for Facebook: the more compelling content it could provide to its users, the more time they would spend on Facebook; the more time they spent on Facebook, the more opportunities Facebook would have to place advertisements in front of them. And, critically, the more time users spent on Facebook, the 
    less
    time they had to read anything else, further increasing the motivation for media companies (and businesses of all types) to be on Facebook themselves, resulting in a virtuous cycle in Facebook’s favor: by having the users Facebook captured the suppliers, which deepened their hold on the users, increasing their power over suppliers.
    This process reduced Facebook’s content suppliers — media companies — into pure commodity providers. All that mattered for everyone was the level of engagement: media companies got ad views, Facebook got shares, and users got the psychic reward of having flipped a bit in a database. Of course not all content was engaging to all users; that’s what the algorithm was for: show people only what they want to see, whether it be baby pictures, engagement announcements, cat pictures, quizzes, or, yes, political news. It was, from Facebook’s perspective — and, frankly, from its users’ perspective — all the same. That includes fake news too, by the way: it’s not that there is anything particularly special about  news from Macedonia  , it’s that according to the algorithm there isn’t anything particularly special about 
    any
    content, beyond the level of engagement it drives.
     The Media and Trump 
    There has been a lot of discussion — in the media, naturally — about how the media made President-elect Donald Trump. The story is that Trump would have never amounted to anything had the media not given him  billions of dollars worth of earned media  — basically news coverage (as opposed to paid media, which is advertising) — and that the industry needed to take responsibility. It’s a lovely bit of self-reflection that lets the industry deny the far more discomforting reality: that the media couldn’t have done a damn thing about Trump if they had wanted to.
    The reason the media covered Trump so extensively is quite simple: that is what users wanted. And, in a world where media is a commodity, to act as if one has the editorial prerogative to 
    not
    cover a candidate users want to see is to face that reality square in the face, absent the clicks that make the medicine easier to take.
    Indeed, this is the same reason fake news flourishes: because users want it. These sites get traffic because users click on their articles and share them, because they confirm what they already think to be true. Confirmation bias is a hell of a drug — and, as Techcrunch reporter Kim-Mai Cutler so aptly  put it on Twitter  , it’s a hell of a business model.
     Why Facebook Should Fix Fake News 
    So now we arrive at the question of what to do about fake news. Perhaps the most common sentiment was laid out by Zeynep Tufekci in the  New York Times  : Facebook should eliminate fake news and the filter effect — the tendency to see news you already agree with — while they’re at it. Tufekci writes:
    
      Mark Zuckerberg, Facebook’s chief, believes that it is “a pretty crazy idea” that “fake news on Facebook, which is a very small amount of content, influenced the election in any way.” In holding fast to the claim that his company has little effect on how people make up their minds, Mr. Zuckerberg is doing real damage to American democracy — and to the world…
      The problem with Facebook’s influence on political discourse is not limited to the dissemination of fake news. It’s also about echo chambers. The company’s algorithm chooses which updates appear higher up in users’ newsfeeds and which are buried. Humans already tend to cluster among like-minded people and seek news that confirms their biases. Facebook’s research shows that the company’s algorithm encourages this by somewhat prioritizing updates that users find comforting…
    
    Tufekci offers up a number of recommendations for Facebook, including sharing data with outside researchers to better understand how misinformation spreads and the extent of filter bubbles,  1  acting much more aggressively to eliminate fake news like it does spam and other objectionable content, rehiring human editors, and retweaking its algorithm to favor news balance, not just engagement.
     Why Facebook Should Not 
    All seem reasonable on their face, but in fact Tufekci’s recommendations are radical in their own way.
    First, there is no incentive for Facebook to do any of this; while the company  denies  this  report in Gizmodo  that the company shelved a change to the News Feed algorithm that would have eliminated fake news stories because it disproportionately affected right-wing sites, the fact remains that the company is heavily incentivized to be perceived as neutral by all sides; anything else would drive away users, a particularly problematic outcome for a social network.  2 
    Moreover, any move away from a focus on engagement would, by definition, decrease the time spent on Facebook, and here Tufekci is wrong to claim that this is acceptable because there is “no competitor in sight.” In fact, Facebook is in its most challenging position in a long time: Snapchat is stealing attention from its most valuable demographics, even as the News Feed is approaching saturation in terms of ad load, and there is a real danger Snapchat will beat the company to the biggest prize in consumer tech: TV-centric brand advertising dollars.
    There are even more fundamental problems, though: how do you decide what is fake and what isn’t? Where is the line? And, perhaps most critically, who decides? To argue that the existence of some number of fake news items amongst an ocean of other content ought to result in active editing of Facebook content is not simply a logistical nightmare but, at least when it comes to the potential of bad outcomes, far more fraught than it appears.
    That goes double for the filter bubble problem: there is a very large leap from arguing Facebook impacts its users’ flow of information via the second-order effects of driving engagement, to insisting the platform actively influence what users see for political reasons. It doesn’t matter that the goal is a better society, as opposed to picking partisan sides; after all, partisans think their goal is a better society as well. Indeed, if the entire concern is the outsized role that Facebook plays in its users’ news consumption, then the far greater fear should be the potential of someone actively abusing that role for their own ends.
    I get why top-down solutions are tempting: fake news and filter bubbles are in front of our faces, and wouldn’t it be better if Facebook fixed them? The problem is the assumption that whoever wields that top-down power will just so happen to have the same views I do. What, though, if they don’t? Just look at our current political situation: those worried about Trump have to contend with the fact that the power of the executive branch has been dramatically expanded over the decades; we place immense responsibility and capability in the hands of one person, forgetting that said responsibility and capability is not so easily withdrawn if we don’t like the one wielding it.
    To that end I would be far more concerned about Facebook were they to begin actively editing the News Feed; as I  noted last week  I’m increasingly concerned about Zuckerberg’s utopian-esque view of the world, and it is a frighteningly small step from influencing the world to controlling the world. Just as bad would be government regulation: our most critical liberty when it comes to a check on tyranny is the freedom of speech, and it would be directly counter to that liberty to put a bureaucrat — who reports to the President — in charge of what people see.
    The key thing to remember is that the actual impact of fake news is dependent on who delivers it: sure, those Macedonian news stories aren’t great, but their effect, such as it is, comes from confirming what people already believe. Contrast that to Miller’s stories in the New York Times: because the New York Times was a trusted gatekeeper, many people fundamentally 
    changed
    their opinions, resulting in a disaster the full effects of which are still being felt. In that light, the potential downside of Facebook coming anywhere close to deciding the news can scarcely be imagined.
     Liberty and Laziness 
    There may be some middle ground here: perhaps some sources are so obviously fake that Facebook can easily exclude them, ideally with full transparency about what they are doing and why. And, to the extent Facebook can share data with outside researchers without compromising its competitive position, it should do so. The company should also provide even more options to users to control their feed if they wish to avoid filter bubbles.
    In truth, though, you and I know that few users will bother. And that, seemingly, is what bothers many of Facebook’s critics the most. If users won’t seek out the “right” news sources, well, then someone ought to make them see them. It all sounds great — and, without question, a far more convenient solution to winning elections than actually making the changes necessary to do so — until you remember that that someone you just entrusted with such awesome power could disagree with you, and that the very notion of controlling what people read is the hallmark of totalitarianism.
    Let me be clear: I am well aware of the problematic aspects of Facebook’s impact; I am particularly worried about the ease with which we sort ourselves into tribes, in part because of the filter bubble effect noted above (that’s one of the reasons  Why Twitter Must Be Saved  ). But the solution is not the reimposition of gatekeepers done in by the Internet; whatever fixes this problem must spring from the power of the Internet, and the fact that each of us, if we choose, has access to more information and sources of truth than ever before, and more ways to reach out and understand and persuade those with whom we disagree. Yes, that is more work than demanding Zuckerberg change what people see, but giving up liberty for laziness never works out well in the end.
    
    
    For more about how the Internet has fundamentally changed politics, please see this piece from March,
    
     The Voters Decide
    
    .
   
    
      Facebook has done a study about the latter, but as Tufekci and others have documented,  the study was full of problems  [  ↩  ]
      Indeed, it wasn’t that long ago that I was making this exact argument in response to those who insisted Facebook would alter the News Feed to serve their own political purposes [  ↩  ]
    
    
      
         Share 
        
          
              Facebook  
              Twitter  
              LinkedIn  
              Email  
            
          
        
      
    
    
       
     Related
     
    
  
  

&tags=觀點" class="ssk ssk-tumblr"></a>
    <a href="https://buffer.com/add?text=Fake News&url=https://stratechery.com/2016/fake-news/" class="ssk ssk-buffer"></a>
</div>


    <div id="main" role="main" class="container">
      
  <!-- Html Elements for Search -->
  <ul id="results-container" class="searched" style="color: #2980B9;"></ul>

  <script src="/opinion/assets/js/simple-jekyll-search.min.js"></script>

  <!-- Configuration -->
  <script>
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/opinion/search.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a><time>{date}</time><a class="tag">{category}</a></li>',
    noResultsText: '没找到',
    limit: 100,
    fuzzy: false,
    exclude: ['Welcome']
  })

  </script>

      







  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
    


  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
  
  

  
    



<article class="post">
  <h1>Fake News</h1>
  <!-- Look the author details up from the site config. -->
  

  <div>
    <span class="date">
      2016-11-16
    </span>

    <!-- Output author details if some exist. -->
    
      
        <span>
            <!-- Personal Info. -->
            <a  style="font-size:14px;">作者: Ben Thompson</a>
        </span>
      
    


    <ul class="tag">
      <li>
        <a href="https://nodebe4.github.io/opinion/categories/#Stratechery">
          Stratechery
        </a>
      </li>
    </ul>

    
        <span>
            <!-- Personal Info. -->
            <a href="https://stratechery.com/2016/fake-news/" style="font-size:14px;">原文</a>
        </span>
    

    <span style="float: right;" title="Stratechery的其它文章">
      <a style="font-size: 14px;" rel="nofollow" href="#sametag" class="tags">#Stratechery 的其它文章</a>
    </span>

  </div>

  <div class="entry">
    
    
    
    <article class="post-2357 post type-post status-publish format-standard hentry category-articles topics-content-moderation topics-networks topics-social concepts-aggregation-theory concepts-commoditizing-suppliers concepts-ethics-and-mores concepts-evolution-of-technology concepts-free concepts-incentives concepts-media concepts-media-and-society concepts-politics concepts-product-management concepts-strengths-weaknesses concepts-technology-and-society concepts-the-internet-and-media concepts-the-social-epoch concepts-understanding-users companies-facebook" id="post-2357">
  <header class="entry-header">
    <h1 class="entry-title" id="fake-news-"> Fake News </h1>
    <div class="entry-meta">
<span class="posted-on"> <span class="screen-reader-text"> Posted on </span>           <time class="entry-date published" datetime="2016-11-16T06:55:16-08:00"> Wednesday, November 16, 2016 </time>
           <time class="updated" datetime="2020-07-24T19:10:46-07:00"> Friday, July 24, 2020 </time>
 </span> <span class="byline"> <span class="author vcard"> <span class="screen-reader-text"> Author </span> by <a class="url fn n" href="https://stratechery.com/author/stratechery/"> Ben Thompson </a> </span> </span>
    </div>
    <!-- .entry-meta -->
  </header>
  <!-- .entry-header -->
  <div class="entry-content">
    <p>Between 2001 and 2003, Judith Miller wrote a number of pieces in the New York Times asserting that Iraq had the capability and the ambition to produce weapons of mass destruction. It was fake news.</p>
    <p>Looking back, it’s impossible to say with certainty what role Miller’s stories played in the U.S.’s ill-fated decision to invade Iraq in 2003; the same sources feeding Miller were well-connected with the George W. Bush administration’s foreign policy team. Still, it meant something to have the New York Times backing them up, particularly for Democrats who may have been inclined to push back against Bush more aggressively. After all, the New York Times was not some fly-by-night operation, it was the preeminent newspaper in the country, and one generally thought to lean towards the left. Miller’s stories had a certain resonance by virtue of where they were published.</p>
    <p>It’s tempting to make a connection between the Miller fiasco and the current debate about Facebook’s fake news problem; the cautionary tale that “fake news is bad” writes itself. My takeaway, though, is the exact opposite: it matters less what is fake and more who decides what is news in the first place.</p>
    <h4 id="facebooks-commoditization-of-media-"> Facebook’s Commoditization of Media </h4>
    <p>In <a href="https://stratechery.com/2015/aggregation-theory/"> Aggregation Theory </a> I described the process by which the demise of distribution-based economic power has resulted in the rise of powerful intermediaries that own the customer experience and commoditize their suppliers. <a href="https://stratechery.com/2016/the-fang-playbook/"> In the case of Facebook </a> , the social network started with the foundation of pre-existing offline networks that were moved online. Given that humans are inherently social, users started prioritizing time on Facebook over time spent reading, say, the newspaper (or any of the effectively infinite set of alternatives for attention).</p>
    <p>It followed, then, that it was in the interest of media companies, businesses, and basically anyone else who wanted to get the attention of users, to be on Facebook as well. This was great for Facebook: the more compelling content it could provide to its users, the more time they would spend on Facebook; the more time they spent on Facebook, the more opportunities Facebook would have to place advertisements in front of them. And, critically, the more time users spent on Facebook, the <em>
    less
   </em> time they had to read anything else, further increasing the motivation for media companies (and businesses of all types) to be on Facebook themselves, resulting in a virtuous cycle in Facebook’s favor: by having the users Facebook captured the suppliers, which deepened their hold on the users, increasing their power over suppliers.</p>
    <p>This process reduced Facebook’s content suppliers — media companies — into pure commodity providers. All that mattered for everyone was the level of engagement: media companies got ad views, Facebook got shares, and users got the psychic reward of having flipped a bit in a database. Of course not all content was engaging to all users; that’s what the algorithm was for: show people only what they want to see, whether it be baby pictures, engagement announcements, cat pictures, quizzes, or, yes, political news. It was, from Facebook’s perspective — and, frankly, from its users’ perspective — all the same. That includes fake news too, by the way: it’s not that there is anything particularly special about <a href="https://www.buzzfeed.com/craigsilverman/how-macedonia-became-a-global-hub-for-pro-trump-misinfo"> news from Macedonia </a> , it’s that according to the algorithm there isn’t anything particularly special about <em>
    any
   </em> content, beyond the level of engagement it drives.</p>
    <h4 id="the-media-and-trump-"> The Media and Trump </h4>
    <p>There has been a lot of discussion — in the media, naturally — about how the media made President-elect Donald Trump. The story is that Trump would have never amounted to anything had the media not given him <a href="http://www.nytimes.com/2016/03/16/upshot/measuring-donald-trumps-mammoth-advantage-in-free-media.html"> billions of dollars worth of earned media </a> — basically news coverage (as opposed to paid media, which is advertising) — and that the industry needed to take responsibility. It’s a lovely bit of self-reflection that lets the industry deny the far more discomforting reality: that the media couldn’t have done a damn thing about Trump if they had wanted to.</p>
    <p>The reason the media covered Trump so extensively is quite simple: that is what users wanted. And, in a world where media is a commodity, to act as if one has the editorial prerogative to <em>
    not
   </em> cover a candidate users want to see is to face that reality square in the face, absent the clicks that make the medicine easier to take.</p>
    <p>Indeed, this is the same reason fake news flourishes: because users want it. These sites get traffic because users click on their articles and share them, because they confirm what they already think to be true. Confirmation bias is a hell of a drug — and, as Techcrunch reporter Kim-Mai Cutler so aptly <a href="https://twitter.com/kimmaicutler/status/796560990854905857"> put it on Twitter </a> , it’s a hell of a business model.</p>
    <h4 id="why-facebook-should-fix-fake-news-"> Why Facebook Should Fix Fake News </h4>
    <p>So now we arrive at the question of what to do about fake news. Perhaps the most common sentiment was laid out by Zeynep Tufekci in the <a href="http://www.nytimes.com/2016/11/15/opinion/mark-zuckerberg-is-in-denial.html"> New York Times </a> : Facebook should eliminate fake news and the filter effect — the tendency to see news you already agree with — while they’re at it. Tufekci writes:</p>
    <blockquote>
      <p>Mark Zuckerberg, Facebook’s chief, believes that it is “a pretty crazy idea” that “fake news on Facebook, which is a very small amount of content, influenced the election in any way.” In holding fast to the claim that his company has little effect on how people make up their minds, Mr. Zuckerberg is doing real damage to American democracy — and to the world…</p>
      <p>The problem with Facebook’s influence on political discourse is not limited to the dissemination of fake news. It’s also about echo chambers. The company’s algorithm chooses which updates appear higher up in users’ newsfeeds and which are buried. Humans already tend to cluster among like-minded people and seek news that confirms their biases. Facebook’s research shows that the company’s algorithm encourages this by somewhat prioritizing updates that users find comforting…</p>
    </blockquote>
    <p>Tufekci offers up a number of recommendations for Facebook, including sharing data with outside researchers to better understand how misinformation spreads and the extent of filter bubbles, <a class="footnote-link footnote-identifier-link" href="#footnote_0_2357" id="identifier_0_2357" title="Facebook has done a study about the latter, but as Tufekci and others have documented, the study was full of problems "> 1 </a> acting much more aggressively to eliminate fake news like it does spam and other objectionable content, rehiring human editors, and retweaking its algorithm to favor news balance, not just engagement.</p>
    <h4 id="why-facebook-should-not-"> Why Facebook Should Not </h4>
    <p>All seem reasonable on their face, but in fact Tufekci’s recommendations are radical in their own way.</p>
    <p>First, there is no incentive for Facebook to do any of this; while the company <a href="http://www.slate.com/blogs/future_tense/2016/11/14/did_facebook_really_tolerate_fake_news_to_appease_conservatives.html"> denies </a> this <a href="http://gizmodo.com/facebooks-fight-against-fake-news-was-undercut-by-fear-1788808204"> report in Gizmodo </a> that the company shelved a change to the News Feed algorithm that would have eliminated fake news stories because it disproportionately affected right-wing sites, the fact remains that the company is heavily incentivized to be perceived as neutral by all sides; anything else would drive away users, a particularly problematic outcome for a social network. <a class="footnote-link footnote-identifier-link" href="#footnote_1_2357" id="identifier_1_2357" title="Indeed, it wasn’t that long ago that I was making this exact argument in response to those who insisted Facebook would alter the News Feed to serve their own political purposes"> 2 </a></p>
    <p>Moreover, any move away from a focus on engagement would, by definition, decrease the time spent on Facebook, and here Tufekci is wrong to claim that this is acceptable because there is “no competitor in sight.” In fact, Facebook is in its most challenging position in a long time: Snapchat is stealing attention from its most valuable demographics, even as the News Feed is approaching saturation in terms of ad load, and there is a real danger Snapchat will beat the company to the biggest prize in consumer tech: TV-centric brand advertising dollars.</p>
    <p>There are even more fundamental problems, though: how do you decide what is fake and what isn’t? Where is the line? And, perhaps most critically, who decides? To argue that the existence of some number of fake news items amongst an ocean of other content ought to result in active editing of Facebook content is not simply a logistical nightmare but, at least when it comes to the potential of bad outcomes, far more fraught than it appears.</p>
    <p>That goes double for the filter bubble problem: there is a very large leap from arguing Facebook impacts its users’ flow of information via the second-order effects of driving engagement, to insisting the platform actively influence what users see for political reasons. It doesn’t matter that the goal is a better society, as opposed to picking partisan sides; after all, partisans think their goal is a better society as well. Indeed, if the entire concern is the outsized role that Facebook plays in its users’ news consumption, then the far greater fear should be the potential of someone actively abusing that role for their own ends.</p>
    <p>I get why top-down solutions are tempting: fake news and filter bubbles are in front of our faces, and wouldn’t it be better if Facebook fixed them? The problem is the assumption that whoever wields that top-down power will just so happen to have the same views I do. What, though, if they don’t? Just look at our current political situation: those worried about Trump have to contend with the fact that the power of the executive branch has been dramatically expanded over the decades; we place immense responsibility and capability in the hands of one person, forgetting that said responsibility and capability is not so easily withdrawn if we don’t like the one wielding it.</p>
    <p>To that end I would be far more concerned about Facebook were they to begin actively editing the News Feed; as I <a href="https://stratechery.com/2016/why-twitter-must-be-saved/"> noted last week </a> I’m increasingly concerned about Zuckerberg’s utopian-esque view of the world, and it is a frighteningly small step from influencing the world to controlling the world. Just as bad would be government regulation: our most critical liberty when it comes to a check on tyranny is the freedom of speech, and it would be directly counter to that liberty to put a bureaucrat — who reports to the President — in charge of what people see.</p>
    <p>The key thing to remember is that the actual impact of fake news is dependent on who delivers it: sure, those Macedonian news stories aren’t great, but their effect, such as it is, comes from confirming what people already believe. Contrast that to Miller’s stories in the New York Times: because the New York Times was a trusted gatekeeper, many people fundamentally <em>
    changed
   </em> their opinions, resulting in a disaster the full effects of which are still being felt. In that light, the potential downside of Facebook coming anywhere close to deciding the news can scarcely be imagined.</p>
    <h4 id="liberty-and-laziness-"> Liberty and Laziness </h4>
    <p>There may be some middle ground here: perhaps some sources are so obviously fake that Facebook can easily exclude them, ideally with full transparency about what they are doing and why. And, to the extent Facebook can share data with outside researchers without compromising its competitive position, it should do so. The company should also provide even more options to users to control their feed if they wish to avoid filter bubbles.</p>
    <p>In truth, though, you and I know that few users will bother. And that, seemingly, is what bothers many of Facebook’s critics the most. If users won’t seek out the “right” news sources, well, then someone ought to make them see them. It all sounds great — and, without question, a far more convenient solution to winning elections than actually making the changes necessary to do so — until you remember that that someone you just entrusted with such awesome power could disagree with you, and that the very notion of controlling what people read is the hallmark of totalitarianism.</p>
    <p>Let me be clear: I am well aware of the problematic aspects of Facebook’s impact; I am particularly worried about the ease with which we sort ourselves into tribes, in part because of the filter bubble effect noted above (that’s one of the reasons <a href="https://stratechery.com/2016/why-twitter-must-be-saved/"> Why Twitter Must Be Saved </a> ). But the solution is not the reimposition of gatekeepers done in by the Internet; whatever fixes this problem must spring from the power of the Internet, and the fact that each of us, if we choose, has access to more information and sources of truth than ever before, and more ways to reach out and understand and persuade those with whom we disagree. Yes, that is more work than demanding Zuckerberg change what people see, but giving up liberty for laziness never works out well in the end.</p>
    <hr />
    <p><em>
    For more about how the Internet has fundamentally changed politics, please see this piece from March,
    <a href="https://stratechery.com/2016/the-voters-decide/">
     The Voters Decide
    </a>
    .
   </em></p>
    <ol class="footnotes">
      <li class="footnote" id="footnote_0_2357">Facebook has done a study about the latter, but as Tufekci and others have documented, <a href="https://stratechery.com/2016/google-v-oracle-round-3-ftc-re-opening-google-search-investigation-facebook-and-filter-bubbles/"> the study was full of problems </a> [ <a class="footnote-link footnote-back-link" href="#identifier_0_2357"> ↩ </a> ]</li>
      <li class="footnote" id="footnote_1_2357">Indeed, it wasn’t that long ago that I was making this exact argument in response to those who insisted Facebook would alter the News Feed to serve their own political purposes [ <a class="footnote-link footnote-back-link" href="#identifier_1_2357"> ↩ </a> ]</li>
    </ol>
    <div class="sharedaddy sd-sharing-enabled">
      <div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing">
        <h3 class="sd-title" id="share-"> Share </h3>
        <div class="sd-content">
          <ul>
            <li class="share-facebook"><a class="share-facebook sd-button share-icon" data-shared="sharing-facebook-2357" href="https://stratechery.com/2016/fake-news/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Facebook"> <span> Facebook </span> </a></li>
            <li class="share-twitter"><a class="share-twitter sd-button share-icon" data-shared="sharing-twitter-2357" href="https://stratechery.com/2016/fake-news/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Twitter"> <span> Twitter </span> </a></li>
            <li class="share-linkedin"><a class="share-linkedin sd-button share-icon" data-shared="sharing-linkedin-2357" href="https://stratechery.com/2016/fake-news/?share=linkedin" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on LinkedIn"> <span> LinkedIn </span> </a></li>
            <li class="share-email"><a class="share-email sd-button share-icon" data-shared="" href="https://stratechery.com/2016/fake-news/?share=email" rel="nofollow noopener noreferrer" target="_blank" title="Click to email this to a friend"> <span> Email </span> </a></li>
            <li class="share-end"></li>
          </ul>
        </div>
      </div>
    </div>
    <div class="jp-relatedposts" id="jp-relatedposts">
      <h3 class="jp-relatedposts-headline" id="related"> <em>
     Related
    </em> </h3>
    </div>
  </div>
  <!-- .entry-content -->
</article>


  </div>

  <hr style="border-top:1px solid #28323C;"/>

<font size=2px>
  文章版权归原作者所有。
</font>

<div style="text-align:center"><img width="1px" src="https://i.imgur.com/HSw56Ez.png" alt="二维码分享本站" style="text-align:center"/></div>

  <div id="sametag">
    <h4 style="display: inline-block;">#Stratechery 的其它文章</h4>
    <span>--<a href="https://nodebe4.github.io/opinion/2025-02-27/An-Interview-with-Benedict-Evans-About-AI-Unknowns/">最新</a>-</span>
    <span>-<a href="https://nodebe4.github.io/opinion/2009-12-09/Dropbox_and_the_Entrepreneurs_Blindspot/">最早</a>--</span>
    
      <li>
        <time>2016-12-07</time>
        <a href="https://nodebe4.github.io/opinion/2016-12-07/Opendoor_A_Startup_Worth_Emulating/">
          Opendoor: A Startup Worth Emulating
        </a>
      </li>
    
    
      <li>
        <time>2016-11-30</time>
        <a href="https://nodebe4.github.io/opinion/2016-11-30/How_Google_Is_Challenging_AWS/">
          How Google Is Challenging AWS
        </a>
      </li>
    
    
      <li>
        <time>2016-11-08</time>
        <a href="https://nodebe4.github.io/opinion/2016-11-08/Why_Twitter_Must_Be_Saved/">
          Why Twitter Must Be Saved
        </a>
      </li>
    
    
      <li>
        <time>2016-10-31</time>
        <a href="https://nodebe4.github.io/opinion/2016-10-31/Apple_Should_Buy_Netflix/">
          Apple Should Buy Netflix
        </a>
      </li>
    
  </div>


  <hr>
  <div class="pagination">
    
      <span class="prev" >
          <a href="https://nodebe4.github.io/opinion/2016-11-16/%E9%A9%AC%E4%BF%AE-%E5%82%85%E5%8B%92%E8%AE%B2%E5%BA%A7-%E9%BB%91%E6%9A%97%E5%9C%B0%E5%B8%A6%E4%B8%8E%E9%80%8F%E6%98%8E%E5%B1%82/">
            前一篇：马修·傅勒讲座－黑暗地带与透明层
          </a>
      </span>
    
    
      <span class="next" >
          <a href="https://nodebe4.github.io/opinion/2016-11-17/%E5%AA%92%E4%BD%93%E6%98%AF%E6%81%90%E6%80%96%E5%88%86%E5%AD%90%E7%9A%84-%E5%A5%BD%E6%9C%8B%E5%8F%8B/">
            後一篇：媒体是恐怖分子的“好朋友”？
          </a>
      </span>
    

    <script>
    /* post pagination keyboard shortcuts */
    document.body.onkeyup = function(e){
      if (e.keyCode == '37') { window.location = 'https://nodebe4.github.io/opinion/2016-11-16/%E9%A9%AC%E4%BF%AE-%E5%82%85%E5%8B%92%E8%AE%B2%E5%BA%A7-%E9%BB%91%E6%9A%97%E5%9C%B0%E5%B8%A6%E4%B8%8E%E9%80%8F%E6%98%8E%E5%B1%82/'; } // left arrow key
      if (e.keyCode == '39') { window.location = 'https://nodebe4.github.io/opinion/2016-11-17/%E5%AA%92%E4%BD%93%E6%98%AF%E6%81%90%E6%80%96%E5%88%86%E5%AD%90%E7%9A%84-%E5%A5%BD%E6%9C%8B%E5%8F%8B/'; } // right arrow key
      if (e.keyCode == '45') { window.location = 'https://nodebe4.github.io/opinion/2016-11-30/How_Google_Is_Challenging_AWS/'; } // insert key
      if (e.keyCode == '46') { window.location = 'https://nodebe4.github.io/opinion/2016-11-08/Why_Twitter_Must_Be_Saved/'; } // delete key
    };
    </script>
    <link rel="stylesheet" type="text/css" href="/opinion/assets/css/fab.css" />

<div class="fab-wrapper">
  <div class="fab-wheel">
    
    
    
    <a class="fab-action fab-action-1" title="上一篇(热键 &#8594;)" href="https://nodebe4.github.io/opinion/2016-11-16/%E9%A9%AC%E4%BF%AE-%E5%82%85%E5%8B%92%E8%AE%B2%E5%BA%A7-%E9%BB%91%E6%9A%97%E5%9C%B0%E5%B8%A6%E4%B8%8E%E9%80%8F%E6%98%8E%E5%B1%82/">
      <i>后</i>
    </a>
    
    
    <a class="fab-action fab-action-2" title="下一篇(热键 &#8592;)" href="https://nodebe4.github.io/opinion/2016-11-17/%E5%AA%92%E4%BD%93%E6%98%AF%E6%81%90%E6%80%96%E5%88%86%E5%AD%90%E7%9A%84-%E5%A5%BD%E6%9C%8B%E5%8F%8B/">
      <i>前</i>
    </a>
    
    
    <a class="fab-action fab-action-3" title="<Stratechery>上一篇(热键 ins)" href="https://nodebe4.github.io/opinion/2016-11-30/How_Google_Is_Challenging_AWS/">
      <i>左</i>
    </a>
    
    
    <a class="fab-action fab-action-4" title="<Stratechery>下一篇(热键 del)" href="https://nodebe4.github.io/opinion/2016-11-08/Why_Twitter_Must_Be_Saved/">
      <i>右</i>
    </a>
    
  </div>
</div>


  </div>


  

</article>

    </div>

    <div style="z-index:2;">
<script src="/opinion/assets/js/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  cornerOffset: 20, // px
  id: 'back-to-top',
  backgroundColor: '#ddd',
  textColor: 'red'
})</script>
</div>


    <div class="wrapper-footer" id="footer">
      <div class="container">
        <footer class="footer">
          <img width="200px" src="https://i.imgur.com/HSw56Ez.png" alt="二维码分享本站"/>
<font size=2px>二维码分享本站</font>

<!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

  

  

  
  <li><a href="mailto:beauti4@protonmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
  

  

  

  
  <li><a href="https://github.com/NodeBE4/opinion" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
  

  

  

  

  

  
  <li><a href="/opinion/feed.xml" class="icon-21 rss" title="RSS"><svg viewBox="0 0 512 512"><path d="M201.8 347.2c0 20.3-16.5 36.8-36.8 36.8 -20.3 0-36.8-16.5-36.8-36.8s16.5-36.8 36.8-36.8C185.3 310.4 201.8 326.8 201.8 347.2zM128.2 204.7v54.5c68.5 0.7 124 56.3 124.7 124.7h54.5C306.7 285.3 226.9 205.4 128.2 204.7zM128.2 166.6c57.9 0.3 112.3 22.9 153.2 63.9 41 41 63.7 95.5 63.9 153.5h54.5c-0.3-149.9-121.7-271.4-271.6-271.9V166.6L128.2 166.6z"/></svg><!--[if lt IE 9]><em>RSS</em><![endif]--></a></li>
  

  

  

  

  

    
</ul>





<p><span style="color:blue">内容每小时更新一次.</span> Powered by <a href="https://github.com/AWEEKJ/kiko-now">Kiko Now</a> & <a href="https://github.com/gitalk/gitalk">Gitalk</a> & <a href="https://github.com/duty-machine/news">duty-machine</a>, 站务 <a href="https://be4.herokuapp.com">NodeBE4</a>（<span style="color:red">被墙</span>）</p>





        </footer>
      </div>
    </div>

    



  </body>
</html>
