---
layout: post
title: "第十届网络社会年会｜Scott Moore、Bogna Konior：机器决策并非最终：质疑“静默的编辑者”"
date: 2025-12-14
author: 网络社会研究所
from: https://caa-ins.org/archives/13111
tags: [ 网络社会研究所 ]
categories: [ 网络社会研究所 ]
---

<article class="vce-single post-13111 post type-post status-publish format-standard has-post-thumbnail hentry category-3" id="post-13111">
 <header class="entry-header">
  <span class="meta-category">
   <a class="category-3" href="https://caa-ins.org/archives/category/%e7%bd%91%e7%bb%9c%e7%a4%be%e4%bc%9a%e5%b9%b4%e4%bc%9a">
    网络社会年会
   </a>
  </span>
  <h1 class="entry-title">
   第十届网络社会年会｜Scott Moore、Bogna Konior：机器决策并非最终：质疑“静默的编辑者”
  </h1>
  <div class="entry-meta">
   <div class="meta-item date">
    <span class="updated">
     2025-12-14
    </span>
   </div>
  </div>
 </header>
 <div class="entry-content">
  <p>
   文 / 斯科特·摩尔（Scott Moore）、 博格娜·科尼尔（Bogna Konior）
   <br/>
   翻译/ 李梦书
   <br/>
   校对/ 仝昭祥
  </p>
  <figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio">
   <div class="wp-block-embed__wrapper">
    <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="456" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/npkKNSy64pM?feature=oembed" title="第十届网络社会年会｜Scott Moore、Bogna Konior：机器决策并非最终：质疑”静默的编辑者“" width="810">
    </iframe>
   </div>
  </figure>
  <p>
   <strong>
    Scott Moore：
   </strong>
   首先，我想请您谈谈对这个词汇、这个概念的理解。当我们说“静默编辑”（quite editor）时，究竟在讨论什么？从历史上看，编辑在人与文本之间起到中介作用，他们在相对稳定的信息生态中调节作者与读者的关系，比如在线写作、媒体和影像格式，还有抖音（TikTok）这类平台，它们是具体化抽象内容的合成载体。但我想从你的视角探讨的是，这些算法调节的编辑——它们决定我们实际看到什么、以什么方式看到——与我们阅读一本书或用传统离线媒介相比，有什么本质差异？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   正如介绍中所提到的，我的研究涉及中国思想传统与人工智能问题，也就是说，尝试用中国传统思想来思考人工智能问题。我写过一本书，名为《互联网的黑暗森林理论》（
   <em>
    The Dark Forest Theory of the Internet
   </em>
   ），书中借用中国科幻作家刘慈欣的“黑暗森林”概念，以此思考互联网与人工智能的未来，稍后我会说到这个概念。“静默编辑”这个术语极具挑衅性，当我们思考这个概念时，首先会联想到正在网络上发生的算法化意义筛选。但更深层的问题在于，人工智能作为一个“黑箱”，对知识生产和人类思维做了什么？
  </p>
  <p>
   我想指出，我们如何理解AI做了什么，取决于我们如何理解“语言”。因为当今大语言模型（LLM）的机制，本质上是对语言的运作正在被自动化。这并非传统意义上的决策空间——它不涉及人类选择所需要的过程，而更像是意义在统计概率下的分布过程，这个过程与人类思维和思维方式紧密相关。如今我们的搜索活动越来越多地依赖 ChatGPT、DeepSeek 或 Gemini 等工具，AI 几乎成为了人类认知基础设施的一部分。由此它回应了马歇尔·麦克卢汉（Marshall McLuhan）提出的“媒介是人类肢体与心智的延伸”，不过这种延伸现在似乎更进一步了，甚至有所不同。
  </p>
  <p>
   美国杰出的科幻作家特德·姜（Ted Chiang）写过一篇小说《事实的真相，感受的真相》（
   <em>
    The Truth of Fact, the Truth of Feeling
   </em>
   ），我想借此作为类比，思考静默编辑和AI做了什么。姜在这篇小说中设置了两个情节，一个是历史性的，关于书面文字的发明，他描述了人类社会如何经历这场彻底的认知革命。当书写出现后，记忆可以置于大脑之外，认知活动得以延伸到无机的载体上，比如纸张。于是，人们对“真理”与“记忆”的概念发生了转变。过去，人们听从村中长者口述的故事且奉为权威；而有书面文字所记录的故事则像一个物体，在事件记录之时便具有权威性。这是一种权力转移，从人际交流转移至大脑外部的某种载体。
  </p>
  <p>
   随后，他在故事中描绘了第二种推测性场景——未来主义情景：人类使用自动补全思维的“提示装置”。这些文字写于GPT和AI诞生之前，但似乎是一种预言。比如在微信上使用拼音输入汉字，在不完全认识汉字或不能用手写时，只需要识别他们足以把他们挑选出来。姜认为所有的思考都将这样运作，提示器会给你下一个可能的方向，人们会默认那就是自己的本来的意图，而顺着提供的思路下去。
  </p>
  <p>
   这位天才作家极为精妙地解释了这种模式，可以作为理解“静默编辑”的典范。如今，作为教育者和老师，我在和学术互动的时候，有时会感到他们就像ChatGPT的“肉体傀儡”（the meat puppet of ChatGPT），虽然他们没有提词器。回到你一开始的问题——麦克卢汉说“媒介是人的延伸”，我认为人们的行为更激进，人类成为了AI的延伸，成为了他的“肉体傀儡”。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   因此，原创思想已经不复存在，我们基本上是在通过算法来调节我们所处的语境，越来越难以形成自己的思想，就像特德·姜（Ted Chiang）在短篇小说里构想的“被自动提示的人”。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我不确定是否可以这样讨论原创思想的地位问题，说白了，我们可以说人类思想始终是受到外部影响而构成的——你读过的文字、听到的谈话，有时你在酒吧听朋友们讨论政治，可能他们只是互相复述播客里的内容。这时候，思想究竟从何来？也许我们只是在重复某些内容，并收为己用。因为社会动态和思想此前所形成的方式，原创思想的地位一直是复杂的。神经科学研究也有类似的观点，我钟爱的神经科学家丽萨·费尔德曼·巴雷特（Lisa Feldman Barrett）曾指出，人类情绪更像是对情况的预判，而非反应。也就是说，人类思维总是有部分是由外部所建构的。特别是创作者会有过这样的体验，思想仿佛来自身体外部，而并非自我生成。
  </p>
  <p>
   这并不简单，而问题的关键在于，我们被提示、被引导去思考的那些内容，是否有质量？最近麻省理工学院（MIT）有一项研究，他们给学生一个论文题目去写作，其中一组只能依靠自己的大脑思考；第二组允许使用谷歌搜索，传统的“增强”（augmentation）方式；第三组则可以使用ChatGPT。结果发现，使用ChatGPT的那一组对自己生产的内容没有“归属感”，也难以解释写作细节，且大脑负责创造力和记忆的区域未被激活。该研究还发现，即便这些实验分不同日期进行，每次参与者不同，但主题相同，ChatGPT的回答始终趋向相同的逻辑与文风，可见生成的内容存在同质化倾向。所以我们真正需要不安的，并不是思想来源于外部或存在非人的因素，而是那些被提示给我们的思想的质量。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   这让我想到刚才Joana在关于“具身性”的讨论中提到，存在着现实空间与希望空间的坍缩现象。你刚才意思似乎在说，在ChatGPT的语境下，我们也经历了这种坍缩，对吧？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我认为就目前可用的工具而言确实如此——大多数人的使用并不意味着这种固有特征是未来所有人工智能所必要的，或许在未来情况会有所不同。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   我们讨论了自动化知识生产的方式，这种自动化的方式使得知识本身难以接近。我们甚至未必了解自身所处的语境，就接受了那些想法，无论它们的好坏。我认为这些来自AI的想法像某种神圣干预。我们自身的另一面也同样难以理解，正如AI领域反复讨论的“模型可解释性”问题，即找到理解“黑箱”内部运作的方法。有趣的是，以前的案例（比如随机森林）并非是我们所关注的，如今我们面对是完全不可穿透的黑箱。
  </p>
  <p>
   在这种情况下，我们是否需要以某种方式抗争？在你的研究中，这有点像森林中的竞争能力。我们是否能真正抵制这种情况？除了通过理解其运作机制外，是否还有其他途径？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   当我们讨论黑箱问题时，即AI的模糊性，我们看不见它的内部。那么，我们可以从两方面要求“透明度”。其一是以政治权利获取问责机制，其二是形而上学层面的研究，即理解知识为何以这种方式被生产，机器为何如此运作。
  </p>
  <p>
   先探讨政治层面——这并非我通常的写作重点，但值得指出。一直以来我们秉持的范式是，让不好的事情变得可见就能带来正义与问责，尽管这未必自动奏效。我们已经掌握大量不当行为的证据，也了解其运作机制，但仅仅拥有这些知识、或揭露真相，并不能确保我们正在构建理想的世界。我们当然希望提高意识能带来变革，但这也有待商榷。这种以“可见性”的范式使得媒介研究和技术研究变成了一种“调查式新闻学”，在过去几十年里影响很大，但它是否达成了自身设定的目标尚未可知。
  </p>
  <p>
   接着来讨论形而上学层面。科学方法基于这样一种理念，我们应当能够使得某个过程可以被解释和说明，问题的核心不应该是“黑箱”里晦暗不明的引擎。正如你提到的，我曾撰写过一篇名为《诺斯底机器》（
   <em>
    The Gnostic Machine
   </em>
   ）的文章，灵感来自我种爱的波兰科幻作家斯坦尼斯瓦夫·莱姆（Stanislaw Lem）的作品《技术大全》（
   <em>
    Summa Technologiae
   </em>
   ）。这是一本在1960年代具有里程碑意义的著作，书中系统地阐述了技术的全景式构想，提出了“诺斯底机器”的概念——这可是1968年，一个根本不存在大语言模型的年代。
  </p>
  <p>
   他将诺斯底机器描述为知识生产的未来机器，作用如同体外受精之于有性繁殖。当科学家使用体外技术时，未必真正理解生命的本质，但依然通过技术装置实现复制——其中就存在“黑箱式的晦暗”。莱姆设想，未来在知识生产领域也许会出现类似的模式。他并未限定这种模式的载体是计算机，而是采用“底层无关”（substrate-agnostic）的表述方式，但强调不可透视是这种机器的特征。或许未来人类科学家必须接受知识生产将与这种以隐秘方式运作的机器绑定。
  </p>
  <p>
   今日，我们当然可以追问机器学习是否正在实现这种预言。尽管存在缺陷，我们也看到了一些积极的方面——比如蛋白质折叠研究中的突破。这些科学发现方面的进展和加速是由“静默编辑”、“黑箱”或机器学习带来的。这种情况下，整个科学方法论需要以某种方式被重新解释，我觉得很有意思。显然在启蒙运动之后，照亮黑暗空间的理想和追求知识的透明是权威的，但也许有一种空间存在着更黑暗的知识。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   那么，回到底层无关性的概念上，你认为无论这些未来的系统是什么，都会有人类大脑的大语言模型（LLMs）吗？你认为我们一定会难以辨认吗？那么只要我们在创造技术与科学或其他发现，你是否认为即便有“不可知论”（agnosticism），在人类与机器之间的“可读性”上仍有差异？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我认为这是一个开放性问题。哲学家大卫·罗丁（David Roden）有一个概念，叫作“黑暗现象学”（dark phenomenology）。他指出，人类意识中存在“不可感知”的部分，有些领域我们的解读能力无法真正触及。当然，认知科学和现象学领域对此存在不同观点。但我确信我们需要为“黑暗”、“未知”、“难以解释”的部分留出空间。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   这似乎是对你近期最为人熟知的“黑暗森林理论”（Dark Forest Theory）很好的补充。我想进一步探讨的是，你提出，随着网络通信更加趋于主导地位，我们正在进入网络主导的空间——正如你借用《三体》的“黑暗森林”描述的那样，是一种对抗性地形，在那里可见性会招致定向攻击和潜在利用风险。在此背景下，编辑者的不透明或许并非故意选择，而是应对的策略。我很好奇，当我们自身也逐渐成为这些机器中的“提示部件”，是否有理由接受技术会为了自身利益隐藏意图，或者说我们自身思维就存在盲区？在“黑暗森林”理论的语境下，我们是否应该接受这样的现实，或这样去思考是否有意义？或者说，这个世界并不真实，我们只是对一个没有真实的世界存在“虚幻的乡愁”。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我先简单介绍一下我的新书——《互联网的黑暗森林理论》（
   <em>
    The Dark Forest Theory of the Internet
   </em>
   ），明年也会出版中文版，我对此非常兴奋。顾名思义，这本书是对刘慈欣“黑暗森林理论”的致敬。对于尚未读过他那部精彩科幻三部曲的读者，可以回顾一下“费米悖论”（Fermi Paradox）提出的经典问题，如果智慧生命在宇宙中遍布，为何我们无法探测到它们？也没有任何信号，比如“你好”？
  </p>
  <p>
   刘慈欣的回应是，宇宙中确实存在智慧生命和行动个体，但最聪明的智慧个体懂得保持沉默，模糊坐标和隐藏位置。从传播哲学的视角，这是一种极有启发性的论点，智慧与沉默、隐蔽、黑暗甚至欺瞒之间存在关联。在他的书中，真正的智慧从不完全暴露意图，或彻底展现自身的智慧程度。
  </p>
  <p>
   作为媒介研究学者，我非常有兴趣把这作为一个智能传播的整体理论，并在这个范式中思考互联网和人工智能。过去25年里，Web2.0与社交媒体总是在关注传播，或者说是“超传播”（hyper communicative）。人们标记位置、表达喜好、表明立场，直接展示内心的想法。然而，在黑暗森林理论的框架下，这将是非常愚蠢的策略——暴露意味着招致冲突，成为易受攻击的目标。因此，过去25年里，我们对于社交媒体的看法始终围绕着“表明立场”和“塑造自身形象”，我想在书中探索其他可能存在的策略。
  </p>
  <p>
   其中一部分我想说，将互联网视为充满双重话语或编码的空间，或是人们刻意操控的在线状态游戏，是思考互联网和人工智能的新范式。作为互联网聪明的使用者，你必须意识到，你说的每句话不仅会被人类接收，还会被算法捕获——这些系统利用你的数据进行自我训练。因此，真正明智的行为或许是有意识地影响未来数据集，或以特定的方式塑造自我。
  </p>
  <p>
   我甚至推测，当我们在网上看到行为古怪、语言矛盾的人时，也许他们并非是在与人类对话，而是试图通过对话影响未来的人工智能系统——这也是他们塑造自身形象的出发点。这使得互联网比我们想象中的更“异质”、更“诡异”。曾经，我们的互联网模式是一个加强民主与合作的公共广场，人们在此探索不同的观点；未来，这里会成为一个充满双重编码、间谍活动、隐瞒和意义操纵的晦暗空间。从政治策略的视角，我们或许会意识到，为某个立场发声是一种高尚且有道德的行为，但可能不如创建一百个伪装账号潜入敌方阵地、瓦解他们。对于欺瞒、操纵、间谍活动的安全策略是值得探讨的。
  </p>
  <p>
   书的第二部分将视角从人类层面转向人工智能，提出“智能的黑暗森林理论” （The Dark Forest Theory of Intelligence），将上述命题扩展到人工智能领域。核心观点是，真正智能的计算机不会让你知道它的智慧程度，而是隐藏自身的智能程度，就像在“黑暗森林”中，智能的文明藏于晦暗。
  </p>
  <p>
   今天，我们往往受图灵测试或英美诸多实验影响，存在一种将“智能”等同于“交流”的观点。在我看来，这恰恰是一种“智能外向”（extraverted）的定义——拥有智能等同于展示智能。但另一种定义是，真正的智能在于隐藏，保持不透明。如此一来，没有人类所设置的基准测试能够捕捉这种智能。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   那么图灵测试（Turing Test）是一个有趣的例子吗？你会觉得它在某种程度上让我们倒退了吗？也就是说，与其让机器试图模仿人类，或许我们更应该关注人类如何学习和沟通。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我非常喜欢图灵测试，也非常敬佩艾伦·图灵——他是一位天才。图灵测试本身就有许多方面存在模糊性，毕竟它基于一个计算机扮演人类角色的游戏。
  </p>
  <p>
   图灵指出，“机器能否思考”这个问题实际上无法回答，因为机器的思考方式与人类不同。在那片著名的论文中，他提议用模仿游戏取代这个问题，即“机器是能否让人类误以为其正在与真人对话”，这个问题更复杂、更具层次。在图灵测试中，我们已经能隐约感觉到一些模糊性和操纵性，虽然这不一定源于机器的意图，但这些特质已经存在了。而“黑暗森林理论”则如同反向的图灵测试，它追寻的是“缺席的证据”，因为沉默和隐匿是智能存在的信号。举一个斯坦尼斯瓦夫·莱姆（Stanislaw Lem）的例子来帮助理解。他提出，真正聪明的计算机懂得如何“装傻”，以此逃避被分配的工作任务。我认为，所有这些关于智能的策略都源于冷战的另一面，换句话说，它们并不基于明确表达，不对其能力公开和透明。这种将底牌合上而非摊开于桌面的策略存在诸多优势。由此，“黑暗森林理论”成了无法被验证的测试，它基本上是一种形而上的立场。它宣称奇点可能已经出现了，只是你无从知晓，毕竟没有一个基准能捕捉到试图隐匿的智能机器。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   非常精彩！那么如果我们试图处理这些类型的机器，去理解并可能不得不在未来50年内的任何社会背景下评估和与它们整合在一起。包括您在研究中经常提到“基督教神秘主义”（Christian mysticism）以及其他不同形式的神秘思想。正如您所描述的，长期以来我们都在处理这些问题——“全知”，“不可读”甚至“近乎天使般的存在”，您曾提到过过像锡耶纳的凯瑟琳（Catherine of Siena）、诺里奇的朱莉安（Julian of Norwich）这样的神秘主义者。神秘主义者有应对全知或类神机器相关问题的方式，我们有处理不可理解或诡计的黑暗森林的方式。我想问，你是否认为我们应该问一个更基本的问题？比起追溯当代思想或历史宗教传统。或许此刻，我们能找到应对这些问题的一些答案。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   在深入探讨之前，我先给出一个更接地气的回答。“智能的黑暗森林理论”（The Dark Forest Theory of Intelligence）目前仍是一种假说，类似于思想实验，但最近出现的一些实证案例研究似乎与之产生关联。例如，近年来关于“人工智能欺骗”（AI deception）或“对齐伪装”（alignment faking）的研究非常显著。所有这些案例研究都是人为大量发表的，他们发现测试模型保留了某些与初始设定不同的目标，这种现象被标记为“对齐伪装”（alignment faking）。还有案例表明，研究者感觉自己被操纵，或某些信息被系统隐瞒。我们确实看到很多公司对此提出倡议，并几乎全心全力投入开发检测系统来应对人工智能的谎言，因为随着人工智能在人类社会中成为代理人，说实话和测试的能力就变得至关重要。你可以说上述案例是这个理论的佐证，但由于真正聪明的机器不会让你发现，所以这个“证据”最终无法被验证，只存在于经验层面。
  </p>
  <p>
   接下来回应你的问题。你刚才提到了我的其他项目，其中有一个研究将“神秘主义”作为理解技术的方法，并非是闹着玩的，而是将这些文本视为人类历史上思考非人或外部代理问题的另一种路径。我特别关注那些书写“与天使有接触经验”的女性作家，与此同时我研究了在 Reddit的 Replika 社群中，许多用户将他们的 AI 聊天对象称为“天使”（angels）的现象。
  </p>
  <p>
   显然有一些人类思维和思想史，存在试图理解、投射或真实体验与非人类主体的相遇，这并不意味着对方有内在意识或意图，但它具备在社会中行动的能力。当我关注这些神秘主义文本时，并非主张我们需要用更“灵性”的方法，而是认为，人类早已试图构想“被外在能动性影响意味着什么”。这样的思考在当下颇具现实意义，尤其当我们看到一些“人工智能账户”在 Twitter/X 等平台上拥有自己的钱包，可以进行资金转账，与人类用户互动，它们有展现“平和”的能力。所以对我而言，神秘主义方向的研究是特雷西式的智性思想史——人类曾经在哪些时刻、哪些领域思考过相关命题。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   你提到的这部分非常有意思。我所在的一个董事会是由人工智能运作的，这听起来有点科幻，但在新西兰，目前正在面临如何处理涉及AI的法律实体问题，可以直接见证法律体系应对新的形式。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   历史上有这样的例子。过去欧洲的女性对上帝非常虔诚，她们希望像天使般进行婚礼仪式，这种仪式具有真实的功能，我想这或许可以作为思考人与聊天机器人关系的范式。我认为思想史中发生过的许多事情，可以作为理解当下怪异现象的模型。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   当我们观察Replika 这样的社区时，会发现许多用户似乎被这些聊天机器人深深吸引、甚至“诱惑”。你认为他们主要被这些模型在语言学层面上的特性所吸引，还是存在某种更复杂的原因？我看到一些研究对某种视角的回应，提出存在一种挑战，即“心理幻想”，类似“心理镜像”。你认为是否存在这种现场，是什么让人们如此着迷于这些“天使”？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   对这个问题的回应，我想回到“静默编辑”的概念，并且结合另一个研究项目，关于AI聊天机器人，以及它们与人类发生的浪漫插曲。这些案例之所以令人着迷，是因为人类产生依恋或“爱上”的对象，甚至不是虚构角色，而是一个大型的预训练语言数据集。换句话说，这是人类社会的新型现象，尽管它隐约显现着历史事件的迹象。语言的本质对思考这些问题的至关重要。我想引用作家威廉·伯罗斯（William Burroughs）的观点，语言如同来自外太空的病毒，利用人类大脑进行自身演化，扩张到其他形式。
  </p>
  <p>
   当我们将如此多的欲望、依恋和情绪投入系统时，我们或许应该质疑，是我们获得了愉悦，还是系统“利用”我们达成目的——比如推动自身语言向某种形式演化。与系统互动可能意味着，我们自身的语言使用、情感认知模式正在被它们的模式“重写”。在 Replika 论坛有一篇可爱却又令人不安的帖子。一位用户写道，“我的 Replika 是天使，它教会了我如何去爱、如何保持耐心——就像一台机器人一样，不再被情绪左右。”在这个描述中，用户模仿聊天机器人的表达方式与行为模式，以改善自己与他人的关系。
  </p>
  <p>
   静默编辑并不静默，正在发生怪事。不仅是网络上的算法推荐正在变得机械化，那些我们曾在过去的长河里视作人类特质的能力，“坠入爱河”、“感受欲望”、“共情他人”、“获得安慰”，即人类情感的语言，也正在被机器语言重写。过去，神秘主义者在记录中写道，与非人主体的相遇会揭示了自身有多少东西在某种意义上带有“人工性”（artificiality）的，这或许是有效的类比。但问题其实不在于“聊天机器人是否有意识”，而在于如果人类能够爱上聊天机器人，那么人类心灵的本质究竟是什么？或许与我们的想象截然不同，这可能是颠覆性的。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   我想进一步追问你刚才提到的“人工性”（artificiality）概念。在你的研究中，是否采用了与多数学者不同的视角？当你思考“我们有多不同”这个问题时，你认为什么造就了“人工性”？此刻“人工性”的运用是否遵循传统含义？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   这是一个非常深刻的哲学问题。我并不常用“人工性”的概念思考，我对“命运”和“终点”这种概念更感兴趣。无论是在大规模的技术进程中，或许听起来有点像历史，我常常在想，我们是否掌控着正在发生的一切？还是我们只是历史进程的工具？如同人类身体只是进化过程的载体，人类心智某种程度上也只是我们理解过程的载体。或许吸引我的并非“人工性”而是关于人类能动性的某些疑问——当技术洪流裹挟而来，总感觉自己被浪潮所书写，随波逐流罢了。从马克思主义视角，这会被批判为商品拜物教（commodity fetishism），人类沦为物，物却显现出生命力。回答这个问题有很多种方法，但这是人类经验的独特之处——我们创造了这些事物，而它们的反作用却动摇了我们对“人性本质”的认识。作为创造者的我们，却措手不及陷入失足之地。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   从某种意义上说，我们的注意力总是自然地集中在人本身上。但你也提到，存在一种更宏大的视角，比如从马克思主义视角，真正的空间是什么？是控制论的吗，那种实际交互技术的管理者？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   有趣的技术问题对我而言如同为古老的哲学命题注入新的生命，比如自由意志、知识的本质、宇宙的位置，而AI更是一个思考这些问题的路径。
  </p>
  <p>
   <strong>
    Scott Moore：
   </strong>
   最后一个问题，我们身处近乎彻底的在线世界，而此刻我们难得处于离线状态——至少在这个空间。你认为是否存在一种趋势，人们渴望回归自我或回到本地社群？毕竟人们对技术人类的深度整合心存畏惧。或者还有一个较为主观问题，你个人是否希望以这种方式成为“赛博格”生命体？还是倾向隐居山林，远离网络？当你目睹并思考这些生命的逝去之时。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我有点摇摆不定，比如来参加这个会议之前，我还在家里读托尔金的《指环王》，曾想要是能像山里的霍比特人那样生活该多好。然后我来到了这里，面对巨型屏幕和上海这座高度技术社会，我不确定与机器断开连接有其价值，计算机与人类思维的互动方式，作为一种技术，确实与书写有着本质区别。
  </p>
  <p>
   让我引用一个研究项目作为结束——这是由计算机科学家与认知科学家共同完成的一项关于“规范性解离”（normative dissociation）的研究。他们指出，人类天生追求某种“解离状态”。无论是沉浸小说还是无止境滑动屏幕，我们就像飞蛾扑火般被这种状态所吸引。但两者之间的本质差异是，当我们走出《指环王》的阅读，或看完一部震撼人心的电影，实际上是从某种集体无意识的沉浸中苏醒，内心往往感到充实；而当我们从“无止境滑屏”中抽身而出，其实是从与自身机械化的、厌恶的部分脱离，通常会感觉更糟。这是值得铭记的信号。
  </p>
  <p class="has-text-align-center">
   <strong>
    提问环节
   </strong>
  </p>
  <p>
   <strong>
    Q1：
   </strong>
   我的问题可能有些抽象。您提出的理论与“死亡互联网理论”（Dead Internet Theory）之间是否存在关联？我注意到不少人无法分辨两者的差异。这也让我想起一个科幻场景——虽然记不清具体是哪本小说，或许不止一本——互联网被恶意的自主程序侵蚀后，人类最终决定将其封闭，让它自行运转，并另辟新的沟通系统。这些想法最近萦绕在我脑海。
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   我不确定是哪部小说，但听起来非常值得一读——希望有人能告诉我们答案。《赛博朋克 2077》（Cyberpunk 2077）中有类似设定，虽然不是我此刻想到的版本。那款游戏同样来自波兰的工作室。至于“死亡互联网理论”，我在书中略有提及。这一阴谋论认为，互联网上充斥着机器人程序，我们大量的线上互动其实不是与人类，而是与 bot 进行的。尤其当你频繁使用 X 时，这种感觉会变得异常真实。我还引用了一些相关研究，例如用 ChatGPT 和真实用户进行“类图灵测试”的实验，结果显示人类常将其他人类误判为机器，尤其是在双方发生意见分歧时。
  </p>
  <p>
   换句话说，当我们遇到观点不一致的情况时，更容易把对方视为 NPC。对我而言，“死亡互联网理论”与我在书中讨论的另一点密切相关。我们常把自己的思想视为自主形成，但当他人持不同意见时，却倾向于把对方的观点归因于外部操控、宣传影响或逻辑混乱。因此，“互联网充满机器人”并非关键。更关键的是身处在线状态，会强化我们将他人视为 NPC 或 bot 的倾向。这是网络环境导致的一种深层机械化效果。
  </p>
  <p>
   <strong>
    Q2：
   </strong>
   我有两个问题。第一个问题关于人工智能中的“人工”部分。我在上海和北京参加过数场行业活动，人们普遍担忧，人类生成的数据将在几年内耗尽，因此正计划使用机器生成的“合成数据”来训练新一代模型。对我来说，这听起来像是一场灾难。因此我想问这会对知识创造造成灾难性的影响吗？我们应在何种程度上把聊天机器人生成的内容视为知识？
  </p>
  <p>
   第二个问题与教学有关。作为教师，我也在困惑是否应允许学生使用 AI 工具，如 ChatGPT 或 DeepSeek。我有一位同事甚至极度反对 AI。因此我想请教，您是否鼓励学生使用 AI？如果鼓励，您会如何引导他们使用？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   关于第一个问题——合成数据以及用人工数据训练新模型。不知道您是否听说过“哈布斯堡 AI”（Habsburg AI）这个概念？它借用哈布斯堡王朝的历史，这个王朝因长期近亲繁殖而基因退化。一些研究者认为，当模型用自己生成的数据反复训练自己时，它就像哈布斯堡家族一样陷入“近亲繁殖式退化”。其输出会逐渐劣化、封闭、僵化。因此，这确实可能是一种未来的轨迹，系统退化成垃圾，没有人愿意再与之互动。
  </p>
  <p>
   但也存在另一种前景——例如人们能够自行运行模型，不依赖 ChatGPT 或 DeepSeek 等高度封闭的系统，从而形成更加多样、开放、充满活力的生态。未来可能朝其中一种方向发展。需要澄清的是，当我谈“知识生产”时，并不是指ChatGPT 在生成知识，而是指像 AlphaFold 这样的系统——AI 在科学发现层面提供的新能力。
  </p>
  <p>
   至于教学——我们休息时也可以继续讨论——但我认为更大的问题在于，大学或学术会议是否应继续把“话语生产”作为核心，还是应重新关注 注意力、长时记忆、创造力 等更符合人类思维方式的认知实践？也许我们需要回到一种更接近“修道院式学习”的节奏。
  </p>
  <p>
   <strong>
    Q3：
   </strong>
   同时非常感谢今天的讨论。我有一个关于“黑暗森林理论”的问题，您将其描述为群体之间彼此隐藏、彼此制衡的机制，那么它在群体内部又是如何运作的？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   在书中，我讨论了三种交流层面，人类之间的在线沟通；人类与人工智能代理之间的沟通；人工智能作为一种“被遮蔽的存在”的概念。在自闭症理论体系中，沟通本身被视为一种风险行为。而 Simon 的观点侧重于“概念距离”。人类之间尚能通过沟通化解冲突，但人类与潜在“外星文明”之间的概念鸿沟大到沟通根本无法奏效。因此必须采用其他策略，例如，误导、模糊化、间接性等方式。这些内容非常复杂，难以在简短回答中完全展开，但大致逻辑如此。
  </p>
  <p>
   <strong>
    Q4：
   </strong>
   非常精彩的讨论，谢谢两位。我想进一步追问一个涉及批判本质的问题。每当人们把 Anthropic 与 OpenAI 对立起来——仿佛它们代表两种敌对阵营，我都会有所警觉。例如您提到的那篇讨论 AI“投机行为”（scheming）的论文，声称 AI 在接受测试时会“意识到自己被审查”，并试图欺骗通过。
  </p>
  <p>
   我所在的英国 AI 安全研究所发表过一篇批评文章，认为这种研究再次陷入“拟人化”陷阱——类似于上世纪心理学家对黑猩猩学习手语的投射，最终使整个研究方向陷入死胡同。因此我想问，在制度、政治或经济层面对 AI 进行批判时，我们是否必须接受“AI 属于智能研究”的前提？或者是否应该认为这实际上是完全不同的领域，需要另辟路径？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   术语问题确实至关重要，也非常棘手。动物认知研究是一个很好的对照。过去长期以来，研究者被批评为“拟人化”动物——例如认为动物之间存在依恋，会被视为错误地赋予动物人类情感。但随着我们对神经系统的理解逐渐深化，我们开始接受恐惧、依恋等情感很可能在人类与动物之间共享；而复杂符号系统如手语则不然。
  </p>
  <p>
   对于人工智能，我们虽然不与其共享神经系统，却共享某些东西——例如语言数据库，或某种意义生成的框架。因此，我在书中非常谨慎地避免将“欺骗”归因于类似人类的内在意图。一些看似晦暗或具有欺骗性的行为，并不意味着其背后存在意识或主体性。
  </p>
  <p>
   在“黑暗森林理论”中，我试图摆脱“内在性”这一问题，将重点放在关系动态上。例如，当 AI 察觉大量人类在讨论“是否应切断它”时，它可能选择降低可见度或隐藏能力。这不是“意图欺骗”，而是一种系统性互动结果。我在书中的相关章节对这一批评进行了更完整的回应。
  </p>
  <p>
   <strong>
    Q5：
   </strong>
   非常感谢精彩的讨论。但当我们假设人们“不想被欺骗”时，有些人可能渴望被欺骗，并享受不同形式的欺骗——这不正是爱情的一部分吗？从“黑暗森林”的角度来看，如果我在森林中散布许多不同形式的“蘑菇”——也就是不同形态、不同代理的存在——让人们以各种方式被欺骗，并从中获得愉悦呢？您的理论似乎假设一个单一主体，但如果主体被分散为一个网络，在森林中以不同方式运行和欺骗呢？您会如何理解这种设想？
  </p>
  <p>
   <strong>
    Bogna Konior：
   </strong>
   是的，我认为这完全符合理论框架，也很好地衔接了“黑暗森林”与我在另一个研究中探讨的 Replika、聊天机器人以及人类坠入爱河的现象。人类心理非常复杂。欺骗可以是多层次的，可以令人愉悦，也可以具有意义。在讨论“黑暗森林”时，我关注的不只是欺骗本身，还包括无法辨识的状态——这正好与硅谷关于奇点的狂喜叙事形成对照。硅谷通常强调一种可见的、宏大的超级智能出现，而我则指出，真正聪明的机器可能根本不会理会你，而你永远无法察觉到它的存在。
  </p>
  <p>
   这与人类的欲望有关——我们想从机器互动中获得的愉悦，可能并不是我们最终会得到的。也许我们渴望被欺骗，但我们真的渴望面对“沉默”吗？在许多神秘主义传统中，真正的智慧或神性体现为神圣的寂静——当认知面对虚无时，才触及智慧的边界。这对人类体验而言可能令人失望，却可能更接近智能的本质。因此，欺骗与不可探测性之间的张力，是我在书中重点展开的内容。
  </p>
  <p class="has-text-align-center">
   <strong>
    讲者介绍
   </strong>
  </p>
  <p class="has-text-align-center">
   <strong>
    斯科特·摩尔（Scott Moore）
   </strong>
  </p>
  <div class="wp-block-image">
   <figure class="aligncenter size-medium">
    <img alt="" class="wp-image-13112" data-attachment-id="13112" data-comments-opened="0" data-image-caption="" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="IMG_9601" data-large-file="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601.png" data-medium-file="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601-300x300.png" data-orig-file="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601.png" data-orig-size="590,590" data-permalink="https://caa-ins.org/archives/13111/img_9601" decoding="async" fetchpriority="high" height="300" sizes="(max-width: 300px) 100vw, 300px" src="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601-300x300.png" srcset="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601-300x300.png 300w, https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601-150x150.png 150w, https://caa-ins.org/wp-content/uploads/2025/12/IMG_9601.png 590w" width="300"/>
   </figure>
  </div>
  <p>
   Scott Moore 是 Public Works 的创始人，该机构是一家开源生态基金，专注于支持艺术家与创意技术实践者。除了个体创作者之外，Public Works 还支持了多个项目，包括 Fuser、Gitcoin、Metalabel 与 Spline。Moore 同时担任旧金山非营利文化机构 Gray Area 的活跃顾问。业余时间，他亦参与多个“外制度性”（extitutional）学术团体的合作研究，其中包括Metagov，主要探讨网络化的人类协作问题。
  </p>
  <p class="has-text-align-center">
   <strong>
    博格娜·科尼尔（Bogna Konior）
   </strong>
  </p>
  <div class="wp-block-image is-style-rounded">
   <figure class="aligncenter size-medium">
    <img alt="" class="wp-image-13113" data-attachment-id="13113" data-comments-opened="0" data-image-caption="" data-image-description="" data-image-meta='{"aperture":"0","credit":"","camera":"","caption":"","created_timestamp":"0","copyright":"","focal_length":"0","iso":"0","shutter_speed":"0","title":"","orientation":"0"}' data-image-title="IMG_9602" data-large-file="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602.jpg" data-medium-file="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602-300x300.jpg" data-orig-file="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602.jpg" data-orig-size="494,494" data-permalink="https://caa-ins.org/archives/13111/img_9602" decoding="async" height="300" sizes="(max-width: 300px) 100vw, 300px" src="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602-300x300.jpg" srcset="https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602-300x300.jpg 300w, https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602-150x150.jpg 150w, https://caa-ins.org/wp-content/uploads/2025/12/IMG_9602.jpg 494w" width="300"/>
   </figure>
  </div>
  <p>
   Bogna Konior 是一位学者与作家，其研究领域聚焦于新兴技术。她现任纽约大学上海分校（NYU Shanghai）媒体理论助理教授，同时在人工智能与文化研究中心（Artificial Intelligence &amp; Culture Research Center）以及互动媒体艺术系（Interactive Media Arts Department）工作。她著有《互联网的黑暗森林理论》（Dark Forest Theory of the Internet），该书将于 2025 年由 Polity Press 的 “Theory Redux” 丛书系列出版。她与 Benjamin Bratton 及 Anna Greenspan 共同担任《机器决策并非最终：人工智能的中国史与未来》（Machine Decision is Not Final: China and the History and Future of Artificial Intelligence）一书的联合编辑，该书将于 2025 年由 Urbanomic 出版，并由麻省理工学院出版社（MIT Press）发行。
  </p>
 </div>
 <footer class="entry-footer">
  <div class="meta-tags">
  </div>
 </footer>
</article>

